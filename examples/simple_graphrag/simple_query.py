import sys
import os
import json
import asyncio
from traceback import print_tb
from typing import List, Dict, Any, Optional, Set, Tuple
import numpy as np

# æ·»åŠ  RAG-Factory ç›®å½•åˆ° Python è·¯å¾„
rag_factory_path = os.path.join(os.path.dirname(__file__), "..", "..")
sys.path.insert(0, rag_factory_path)

from rag_factory.documents.GraphExtractor import GraphExtractor
from rag_factory.documents.Prompt import ENTITY_EXTRACT_PROMPT
from rag_factory.documents.schema import Document
from rag_factory.llms import OpenAILLM
from rag_factory.Store.GraphStore.graphrag_neo4j import Neo4jGraphStore
from rag_factory.documents.parse_fn import parse_entity_extraction_result
from rag_factory.Embed import HuggingFaceEmbeddings

class KnowledgeGraphRAG:
    def __init__(self):
        self.llm = OpenAILLM(
            model_name="gpt-5-mini",
            api_key="sk-xxxx", # è¯·æ›¿æ¢ä¸ºä½ çš„api key
            base_url="https://api.gptsapi.net/v1",
        )

        self.storage = Neo4jGraphStore(
            url="bolt://localhost:7680",
            username="neo4j",
            password="12345678",
            database="neo4j",
            embedding=HuggingFaceEmbeddings(
                model_name="/finance_ML/dataarc_syn_database/model/Qwen/qwen_embedding_0.6B",
                model_kwargs={'device': 'cuda:0'}
            )
        )

        self.extractor = GraphExtractor(
            llm=self.llm,
            extract_prompt=ENTITY_EXTRACT_PROMPT,
            parse_fn=parse_entity_extraction_result,
        )
        
        # ToG-2 å‚æ•°
        self.max_depth = 3  # D: æœ€å¤§è¿­ä»£æ·±åº¦
        self.exploration_width = 3  # W: æ¢ç´¢å®½åº¦
        self.context_k = 10  # K: ä¸Šä¸‹æ–‡æ•°é‡
        self.relation_threshold = 0.2  # å…³ç³»ç­›é€‰é˜ˆå€¼
        self.decay_alpha = 0.5  # æŒ‡æ•°è¡°å‡å‚æ•°

    async def extract_entities(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“"""
        try:
            entities = await self.extractor.acall([
                Document(content=text, metadata={"file_name": "query", "chunk_id": "1"})
            ])
            
            entities_list = []
            if entities and len(entities) > 0 and 'entities' in entities[0].metadata:
                for entity in entities[0].metadata['entities']:
                    entities_list.append(entity.name)
            
            return entities_list
        except Exception as e:
            print(f"å®ä½“æå–å‡ºé”™: {e}")
            return []

    async def topic_prune(self, query: str, extracted_entities: List[str]) -> List[str]:
        """Topic Prune: ä½¿ç”¨LLMè¯„ä¼°å¹¶é€‰æ‹©ä¸é—®é¢˜æœ€ç›¸å…³çš„å®ä½“ (ToG-2)"""
        if not extracted_entities:
            return []
        
        if len(extracted_entities) <= self.exploration_width:
            return extracted_entities
        
        try:
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±ä¸“å®¶ã€‚ç»™å®šä¸€ä¸ªé—®é¢˜å’Œä»ä¸­æå–çš„å®ä½“åˆ—è¡¨ï¼Œè¯·è¯„ä¼°æ¯ä¸ªå®ä½“ä¸é—®é¢˜çš„ç›¸å…³æ€§å¹¶é€‰æ‹©æœ€ç›¸å…³çš„{self.exploration_width}ä¸ªå®ä½“ä½œä¸ºèµ·å§‹æ¢ç´¢ç‚¹ã€‚

é—®é¢˜: {query}

æå–çš„å®ä½“: {', '.join(extracted_entities)}

è¯·æŒ‰ç…§ç›¸å…³æ€§ä»é«˜åˆ°ä½æ’åºï¼Œå¹¶é€‰æ‹©æœ€ç›¸å…³çš„{self.exploration_width}ä¸ªå®ä½“ã€‚è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š
1. å®ä½“æ˜¯å¦æ˜¯é—®é¢˜çš„æ ¸å¿ƒæ¦‚å¿µ
2. å®ä½“æ˜¯å¦å¯èƒ½åŒ…å«ç­”æ¡ˆçš„å…³é”®ä¿¡æ¯
3. å®ä½“æ˜¯å¦é€‚åˆä½œä¸ºå›¾éå†çš„èµ·ç‚¹

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
é€‰æ‹©çš„å®ä½“: [å®ä½“1, å®ä½“2, å®ä½“3]
ç†ç”±: ç®€è¦è¯´æ˜é€‰æ‹©ç†ç”±"""

            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†å›¾è°±åˆ†æä¸“å®¶ï¼Œæ“…é•¿è¯†åˆ«ä¸é—®é¢˜æœ€ç›¸å…³çš„å…³é”®å®ä½“ã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            response = await self.llm.achat(messages)
            
            # è§£æå“åº”ï¼Œæå–é€‰æ‹©çš„å®ä½“
            lines = response.strip().split('\n')
            selected_entities = []
            
            for line in lines:
                if line.startswith('é€‰æ‹©çš„å®ä½“:'):
                    # æå–æ–¹æ‹¬å·å†…çš„å†…å®¹
                    import re
                    match = re.search(r'\[(.*?)\]', line)
                    if match:
                        entities_str = match.group(1)
                        selected_entities = [e.strip() for e in entities_str.split(',')]
                        break
            
            # ç¡®ä¿é€‰æ‹©çš„å®ä½“éƒ½åœ¨åŸå§‹åˆ—è¡¨ä¸­
            valid_entities = []
            for entity in selected_entities:
                if entity in extracted_entities:
                    valid_entities.append(entity)
            
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›å‰å‡ ä¸ªå®ä½“
            if not valid_entities:
                valid_entities = extracted_entities[:self.exploration_width]
            
            print(f"Topic Prune: {len(extracted_entities)} -> {len(valid_entities)}")
            print(f"é€‰æ‹©çš„èµ·å§‹å®ä½“: {valid_entities}")
            
            return valid_entities
            
        except Exception as e:
            print(f"Topic Pruneè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            # å‡ºé”™æ—¶è¿”å›å‰å‡ ä¸ªå®ä½“
            return extracted_entities[:self.exploration_width]




    async def get_entity_chunks(self, entity: str) -> List[str]:
        """è·å–å®ä½“ç›¸å…³çš„æ‰€æœ‰chunks"""
        try:
            # æœç´¢å®ä½“ç›¸å…³çš„chunks
            results = await self.storage.search(entity, k=10, search_type="query")
            chunks = []
            for result in results:
                if 'chunk' in result:
                    chunk_content = result['chunk']
                    # ç¡®ä¿chunkæ˜¯å­—ç¬¦ä¸²
                    if isinstance(chunk_content, dict):
                        # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•æå–æ–‡æœ¬å†…å®¹
                        if 'content' in chunk_content:
                            chunk_content = chunk_content['content']
                        elif 'text' in chunk_content:
                            chunk_content = chunk_content['text']
                        else:
                            # å¦‚æœæ— æ³•æå–ï¼Œè·³è¿‡è¿™ä¸ªchunk
                            continue
                    elif not isinstance(chunk_content, str):
                        # å¦‚æœä¸æ˜¯å­—ç¬¦ä¸²ï¼Œè·³è¿‡
                        continue
                    
                    chunks.append(chunk_content)
            return chunks
        except Exception as e:
            print(f"è·å–å®ä½“chunksæ—¶å‡ºé”™: {e}")
            return []
    
    async def get_entity_neighbors(self, entity: str) -> List[Dict[str, Any]]:
        """è·å–å®ä½“çš„æ‰€æœ‰é‚»å±…ï¼ŒåŒ…å«è¾¹çš„è¯¦ç»†ä¿¡æ¯"""
        try:
            # æœç´¢å®ä½“
            results = await self.storage.search(entity, k=5, search_type="entity")
            neighbors = []
            seen_neighbors = set()
            
            for result in results:
                if 'relations' in result:
                    for relation in result['relations']:
                        neighbor_name = relation.get('neighbor', {}).get('name', '')
                        if neighbor_name and neighbor_name != entity and neighbor_name not in seen_neighbors:
                            neighbor_info = {
                                'name': neighbor_name,
                                'relation_type': relation.get('relation_type', 'æœªçŸ¥å…³ç³»'),
                                'relation_description': relation.get('relation_properties', {}).get('relationship_description', 'æ— æè¿°'),
                                'source_entity': entity
                            }
                            neighbors.append(neighbor_info)
                            seen_neighbors.add(neighbor_name)
            
            return neighbors
        except Exception as e:
            print(f"è·å–å®ä½“é‚»å±…æ—¶å‡ºé”™: {e}")
            return []

    async def relation_prune(self, query: str, entity: str, relations: List[Dict[str, Any]], 
                           current_clues: str = "") -> List[Dict[str, Any]]:
        """Relation Prune: ä½¿ç”¨LLMè¯„ä¼°å¹¶ç­›é€‰æœ€æœ‰ç”¨çš„å…³ç³» (ToG-2)"""
        if not relations:
            return []
        
        try:
            # æ„å»ºå…³ç³»æè¿°
            relation_descriptions = []
            for i, rel in enumerate(relations):
                rel_type = rel.get('relation_type', 'æœªçŸ¥å…³ç³»')
                rel_desc = rel.get('relation_description', 'æ— æè¿°')
                neighbor_name = rel.get('name', 'æœªçŸ¥å®ä½“')
                
                desc = f"{i+1}. å…³ç³»: {entity} -[{rel_type}]-> {neighbor_name}"
                if rel_desc != 'æ— æè¿°':
                    desc += f" (æè¿°: {rel_desc})"
                relation_descriptions.append(desc)
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±ä¸“å®¶ã€‚ç»™å®šä¸€ä¸ªé—®é¢˜ã€å½“å‰å®ä½“å’Œå®ƒçš„æ‰€æœ‰é‚»æ¥å…³ç³»ï¼Œè¯·è¯„ä¼°æ¯ä¸ªå…³ç³»å¯¹å›ç­”é—®é¢˜çš„æœ‰ç”¨æ€§ï¼Œå¹¶ç»™å‡º0-10çš„è¯„åˆ†ï¼ˆ10åˆ†æœ€æœ‰ç”¨ï¼‰ã€‚

é—®é¢˜: {query}
å½“å‰å®ä½“: {entity}
{"å½“å‰çº¿ç´¢: " + current_clues if current_clues else ""}

å¯é€‰å…³ç³»:
{chr(10).join(relation_descriptions)}

è¯·ä¸ºæ¯ä¸ªå…³ç³»è¯„åˆ†ï¼Œè€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š
1. å…³ç³»æ˜¯å¦å¯èƒ½é€šå‘åŒ…å«ç­”æ¡ˆçš„å®ä½“
2. å…³ç³»çš„è¯­ä¹‰æ˜¯å¦ä¸é—®é¢˜ç›¸å…³
3. é‚»å±…å®ä½“æ˜¯å¦å¯èƒ½æœ‰ç”¨

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆæ¯è¡Œä¸€ä¸ªå…³ç³»çš„è¯„åˆ†ï¼‰ï¼š
1: [è¯„åˆ†] - [ç®€è¦ç†ç”±]
2: [è¯„åˆ†] - [ç®€è¦ç†ç”±]
...

åªé€‰æ‹©è¯„åˆ† >= {int(self.relation_threshold * 10)} çš„å…³ç³»ã€‚"""

            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†å›¾è°±åˆ†æä¸“å®¶ï¼Œæ“…é•¿è¯„ä¼°å…³ç³»å¯¹é—®é¢˜çš„ç›¸å…³æ€§ã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            response = await self.llm.achat(messages)
            
            # è§£æå“åº”
            lines = response.strip().split('\n')
            selected_relations = []
            
            for line in lines:
                if ':' in line:
                    try:
                        # è§£æ "æ•°å­—: è¯„åˆ† - ç†ç”±" æ ¼å¼
                        parts = line.split(':', 1)
                        if len(parts) == 2:
                            rel_idx = int(parts[0].strip()) - 1  # è½¬æ¢ä¸º0-basedç´¢å¼•
                            content = parts[1].strip()
                            
                            # æå–è¯„åˆ†
                            if ' -' in content:
                                score_part = content.split(' -')[0].strip()
                            else:
                                score_part = content.strip()
                            
                            try:
                                score = float(score_part)
                                
                                # æ£€æŸ¥è¯„åˆ†æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
                                if score >= self.relation_threshold * 10 and 0 <= rel_idx < len(relations):
                                    rel_copy = relations[rel_idx].copy()
                                    rel_copy['llm_score'] = score
                                    selected_relations.append(rel_copy)
                            except ValueError:
                                continue
                    except (ValueError, IndexError):
                        continue
            
            # æŒ‰è¯„åˆ†æ’åº
            selected_relations.sort(key=lambda x: x.get('llm_score', 0), reverse=True)
            
            print(f"Relation Prune for '{entity}': {len(relations)} -> {len(selected_relations)}")
            for rel in selected_relations[:3]:  # åªæ‰“å°å‰3ä¸ª
                print(f"  - {rel['name']}: {rel['relation_type']} (LLMè¯„åˆ†: {rel.get('llm_score', 0):.1f})")
            
            return selected_relations
            
        except Exception as e:
            print(f"Relation Pruneè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            # å‡ºé”™æ—¶è¿”å›æ‰€æœ‰å…³ç³»
            return relations

    async def retrieve_chunks_from_topic_entities(self, query: str, topic_entities: List[str], k: int = 5) -> List[Dict[str, Any]]:
        """ä»topic entitieså…³è”çš„chunksä¸­è¿›è¡Œå‘é‡æ£€ç´¢"""
        try:
            print(f"ä»{len(topic_entities)}ä¸ªtopic entitieså…³è”çš„chunksä¸­æ£€ç´¢...")
            
            # æ”¶é›†æ‰€æœ‰topic entitieså…³è”çš„chunks
            all_entity_chunks = []
            entity_chunk_map = {}  # è®°å½•chunkå±äºå“ªä¸ªentity
            
            for entity in topic_entities:
                chunks = await self.get_entity_chunks(entity)
                print(f"  å®ä½“'{entity}': æ‰¾åˆ°{len(chunks)}ä¸ªchunks")
                
                for chunk in chunks:
                    all_entity_chunks.append(chunk)
                    entity_chunk_map[chunk] = entity
            
            if not all_entity_chunks:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½•å…³è”çš„chunks")
                return []
            
            print(f"æ€»å…±æ”¶é›†åˆ°{len(all_entity_chunks)}ä¸ªchunksï¼Œå¼€å§‹å‘é‡æ£€ç´¢...")
            
            # å¯¹queryè¿›è¡Œembedding
            query_embedding = await self.storage.embedding.aembed_query(query)
            query_vec = np.array(query_embedding)
            
            # è®¡ç®—æ¯ä¸ªchunkä¸queryçš„ç›¸ä¼¼åº¦
            chunk_scores = []
            for chunk in all_entity_chunks:
                try:
                    chunk_embedding = await self.storage.embedding.aembed_query(chunk)
                    chunk_vec = np.array(chunk_embedding)
                    
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = np.dot(query_vec, chunk_vec) / (np.linalg.norm(query_vec) * np.linalg.norm(chunk_vec))
                    
                    chunk_scores.append({
                        'chunk': chunk,
                        'entity': entity_chunk_map[chunk],
                        'similarity_score': max(0.0, similarity)
                    })
                except Exception as e:
                    print(f"è®¡ç®—chunkç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}")
                    continue
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›top-k
            chunk_scores.sort(key=lambda x: x['similarity_score'], reverse=True)
            top_chunks = chunk_scores[:k]
            
            print(f"âœ… æ£€ç´¢å®Œæˆï¼Œè¿”å›top-{len(top_chunks)}ä¸ªæœ€ç›¸å…³çš„chunks")
            for i, chunk_info in enumerate(top_chunks[:3]):  # æ˜¾ç¤ºå‰3ä¸ª
                print(f"  {i+1}. æ¥è‡ªå®ä½“'{chunk_info['entity']}' (ç›¸ä¼¼åº¦: {chunk_info['similarity_score']:.4f})")
                print(f"     {chunk_info['chunk'][:100]}...")
            
            return top_chunks
            
        except Exception as e:
            print(f"ä»topic entitiesæ£€ç´¢chunksæ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return []

    async def entity_guided_context_retrieval(self, query: str, candidate_entity: str, 
                                             triple_path: List[Tuple[str, str]], k: int = None) -> List[Dict[str, Any]]:
        """Entity-guided Context Retrieval: ç»“åˆtripleè·¯å¾„çš„ä¸Šä¸‹æ–‡æ£€ç´¢ (ToG-2)"""
        if k is None:
            k = self.context_k
        
        try:
            # æ„å»ºtripleè·¯å¾„çš„æ–‡æœ¬æè¿°
            path_description = ""
            if triple_path:
                path_parts = []
                for source_entity, relation in triple_path:
                    path_parts.append(f"{source_entity} -[{relation}]->")
                path_parts.append(candidate_entity)
                path_description = " ".join(path_parts)
            
            # è·å–å€™é€‰å®ä½“ç›¸å…³çš„æ–‡æ¡£chunks
            entity_chunks = await self.get_entity_chunks(candidate_entity)
            
            if not entity_chunks:
                return []
            
            # ä¸ºæ¯ä¸ªchunkè®¡ç®—å¢å¼ºçš„ç›¸å…³æ€§åˆ†æ•°
            enhanced_chunks = []
            for chunk in entity_chunks:
                try:
                    # å¦‚æœæœ‰è·¯å¾„æè¿°ï¼Œå°†å…¶é™„åŠ åˆ°chunkåé¢æ¥è®¡ç®—ç›¸ä¼¼åº¦
                    if path_description:
                        enhanced_text = f"{path_description}: {chunk}"
                    else:
                        enhanced_text = chunk
                    
                    # è®¡ç®—queryä¸å¢å¼ºæ–‡æœ¬çš„ç›¸ä¼¼åº¦
                    query_embedding = await self.storage.embedding.aembed_query(query)
                    enhanced_embedding = await self.storage.embedding.aembed_query(enhanced_text)
                    
                    query_vec = np.array(query_embedding)
                    enhanced_vec = np.array(enhanced_embedding)
                    
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = np.dot(query_vec, enhanced_vec) / (np.linalg.norm(query_vec) * np.linalg.norm(enhanced_vec))
                    
                    enhanced_chunks.append({
                        'chunk': chunk,
                        'entity': candidate_entity,
                        'triple_path': triple_path,
                        'path_description': path_description,
                        'similarity_score': max(0.0, similarity)
                    })
                    
                except Exception as e:
                    print(f"è®¡ç®—chunkç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}")
                    # å¦‚æœå‡ºé”™ï¼Œç»™ä¸€ä¸ªé»˜è®¤åˆ†æ•°
                    enhanced_chunks.append({
                        'chunk': chunk,
                        'entity': candidate_entity,
                        'triple_path': triple_path,
                        'path_description': path_description,
                        'similarity_score': 0.0
                    })
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›top-k
            enhanced_chunks.sort(key=lambda x: x['similarity_score'], reverse=True)
            top_chunks = enhanced_chunks[:k]
            
            print(f"Entity-guided Context Retrieval for '{candidate_entity}': {len(enhanced_chunks)} -> {len(top_chunks)}")
            if top_chunks:
                print(f"  æœ€é«˜åˆ†æ•°: {top_chunks[0]['similarity_score']:.4f}")
            
            return top_chunks
            
        except Exception as e:
            print(f"Entity-guided Context Retrievalè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return []

    async def context_based_entity_prune(self, query: str, candidate_entities: List[Dict[str, Any]], 
                                       triple_paths: Dict[str, List[Tuple[str, str]]]) -> List[Dict[str, Any]]:
        """Context-based Entity Prune: åŸºäºä¸Šä¸‹æ–‡è´¨é‡è¯„åˆ†å€™é€‰å®ä½“ (ToG-2)"""
        try:
            scored_entities = []
            
            for entity_info in candidate_entities:
                entity_name = entity_info['name']
                triple_path = triple_paths.get(entity_name, [])
                
                # è·å–å®ä½“çš„ä¸Šä¸‹æ–‡
                contexts = await self.entity_guided_context_retrieval(query, entity_name, triple_path, k=5)
                
                if not contexts:
                    entity_info['context_score'] = 0.0
                    scored_entities.append(entity_info)
                    continue
                
                # ä½¿ç”¨æŒ‡æ•°è¡°å‡åŠ æƒè®¡ç®—æ€»åˆ†
                total_score = 0.0
                for i, ctx in enumerate(contexts):
                    weight = np.exp(-self.decay_alpha * i)  # æŒ‡æ•°è¡°å‡æƒé‡
                    total_score += ctx['similarity_score'] * weight
                
                entity_info['context_score'] = total_score
                entity_info['top_contexts'] = contexts[:3]  # ä¿å­˜å‰3ä¸ªæœ€ä½³ä¸Šä¸‹æ–‡
                scored_entities.append(entity_info)
            
            # æŒ‰ä¸Šä¸‹æ–‡åˆ†æ•°æ’åº
            scored_entities.sort(key=lambda x: x['context_score'], reverse=True)
            
            # é€‰æ‹©top-Wä¸ªå®ä½“
            top_entities = scored_entities[:self.exploration_width]
            
            print(f"Context-based Entity Prune: {len(candidate_entities)} -> {len(top_entities)}")
            for entity in top_entities:
                print(f"  - {entity['name']}: ä¸Šä¸‹æ–‡åˆ†æ•° {entity['context_score']:.4f}")
            
            return top_entities
            
        except Exception as e:
            print(f"Context-based Entity Pruneè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            # å‡ºé”™æ—¶è¿”å›åŸå§‹åˆ—è¡¨çš„å‰Wä¸ª
            return candidate_entities[:self.exploration_width]

    async def hybrid_knowledge_reasoning(self, query: str, previous_clues: str, triple_paths: List[List[Tuple[str, str]]], 
                                       top_entities: List[Dict[str, Any]], contexts: List[Dict[str, Any]]) -> Tuple[bool, str]:
        """æ··åˆçŸ¥è¯†æ¨ç†: åŸºäºå›¾è°±è·¯å¾„å’Œä¸Šä¸‹æ–‡è¿›è¡Œæ¨ç†åˆ¤æ–­ (ToG-2)"""
        try:
            # æ„å»ºçŸ¥è¯†æè¿°
            knowledge_parts = []
            
            # 1. ä¸Šä¸€è½®çº¿ç´¢
            if previous_clues:
                knowledge_parts.append(f"å‰è½®çº¿ç´¢: {previous_clues}")
            
            # 2. Tripleè·¯å¾„ä¿¡æ¯
            if triple_paths:
                path_descriptions = []
                for i, path in enumerate(triple_paths[:3]):  # åªæ˜¾ç¤ºå‰3æ¡è·¯å¾„
                    if path:
                        path_str = ""
                        for source, relation in path:
                            path_str += f"{source} -[{relation}]-> "
                        path_str = path_str.rstrip(" -> ")
                        path_descriptions.append(f"è·¯å¾„{i+1}: {path_str}")
                
                if path_descriptions:
                    knowledge_parts.append("å›¾è°±è·¯å¾„ä¿¡æ¯:\n" + "\n".join(path_descriptions))
            
            # 3. Topå®ä½“åŠå…¶ä¸Šä¸‹æ–‡
            if top_entities:
                entity_contexts = []
                for entity in top_entities:
                    entity_name = entity['name']
                    entity_contexts.append(f"å®ä½“: {entity_name} (ä¸Šä¸‹æ–‡åˆ†æ•°: {entity.get('context_score', 0):.3f})")
                    
                    # æ·»åŠ å®ä½“çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
                    if 'top_contexts' in entity:
                        for ctx in entity['top_contexts'][:2]:  # æ¯ä¸ªå®ä½“æœ€å¤š2ä¸ªä¸Šä¸‹æ–‡
                            chunk = ctx['chunk'][:200] + "..." if len(ctx['chunk']) > 200 else ctx['chunk']
                            entity_contexts.append(f"  - {chunk}")
                
                if entity_contexts:
                    knowledge_parts.append("ç›¸å…³å®ä½“åŠä¸Šä¸‹æ–‡:\n" + "\n".join(entity_contexts))
            
            # 4. é¢å¤–çš„ä¸Šä¸‹æ–‡
            if contexts:
                context_descriptions = []
                for ctx in contexts[:5]:  # æœ€å¤š5ä¸ªä¸Šä¸‹æ–‡
                    chunk = ctx['chunk'] if isinstance(ctx, dict) and 'chunk' in ctx else str(ctx)
                    chunk_preview = chunk[:150] + "..." if len(chunk) > 150 else chunk
                    score = ctx.get('similarity_score', 0) if isinstance(ctx, dict) else 0
                    context_descriptions.append(f"- {chunk_preview} (åˆ†æ•°: {score:.3f})")
                
                if context_descriptions:
                    knowledge_parts.append("é¢å¤–ç›¸å…³ä¸Šä¸‹æ–‡:\n" + "\n".join(context_descriptions))
            
            # æ„å»ºæ¨ç†æç¤º
            knowledge_summary = "\n\n".join(knowledge_parts) if knowledge_parts else "æš‚æ— ç›¸å…³çŸ¥è¯†"
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†æ¨ç†ä¸“å®¶ã€‚åŸºäºæä¾›çš„çŸ¥è¯†ä¿¡æ¯ï¼Œåˆ¤æ–­æ˜¯å¦è¶³å¤Ÿå›ç­”é—®é¢˜ï¼Œå¹¶æŒ‰è¦æ±‚è¾“å‡ºã€‚

é—®é¢˜: {query}

å½“å‰çŸ¥è¯†ä¿¡æ¯:
{knowledge_summary}

è¯·ä»”ç»†åˆ†æä»¥ä¸ŠçŸ¥è¯†ï¼Œç„¶åæŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

åˆ¤æ–­: [Yes/No]

å¦‚æœåˆ¤æ–­ä¸ºYesï¼Œè¯·è¾“å‡º:
ç­”æ¡ˆ: [ä½ çš„å®Œæ•´ç­”æ¡ˆï¼Œå°†å…³é”®å®ä½“ç”¨{{å®ä½“å}}åŒ…å›´]

å¦‚æœåˆ¤æ–­ä¸ºNoï¼Œè¯·è¾“å‡º:
çº¿ç´¢: {{æ€»ç»“å½“å‰æœ‰ç”¨çš„çº¿ç´¢å’Œå‘ç°ï¼Œç”¨äºæŒ‡å¯¼ä¸‹ä¸€è½®æœç´¢}}

æ³¨æ„ï¼š
1. åªæœ‰åœ¨çŸ¥è¯†å……åˆ†ä¸”èƒ½ç»™å‡ºå‡†ç¡®ç­”æ¡ˆæ—¶æ‰åˆ¤æ–­ä¸ºYes
2. çº¿ç´¢åº”è¯¥ç®€æ´æ˜äº†ï¼Œçªå‡ºå¯¹ä¸‹ä¸€è½®æœç´¢æœ‰å¸®åŠ©çš„ä¿¡æ¯
3. å®ä½“åå¿…é¡»ç”¨åŒå¤§æ‹¬å·åŒ…å›´
"""

            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†æ¨ç†å’Œåˆ†æä¸“å®¶ï¼Œæ“…é•¿åŸºäºå¤šæºçŸ¥è¯†è¿›è¡Œç»¼åˆåˆ¤æ–­ã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            response = await self.llm.achat(messages)
            
            # è§£æå“åº”
            lines = response.strip().split('\n')
            is_sufficient = False
            result_content = ""
            
            for i, line in enumerate(lines):
                if line.startswith('åˆ¤æ–­:'):
                    is_sufficient = 'Yes' in line or 'yes' in line or 'æ˜¯' in line
                elif line.startswith('ç­”æ¡ˆ:') and is_sufficient:
                    result_content = line.split(':', 1)[1].strip()
                    # å¦‚æœç­”æ¡ˆè·¨å¤šè¡Œï¼Œç»§ç»­æ”¶é›†
                    for j in range(i+1, len(lines)):
                        if lines[j].strip() and not lines[j].startswith(('çº¿ç´¢:', 'åˆ¤æ–­:')):
                            result_content += " " + lines[j].strip()
                        else:
                            break
                    break
                elif line.startswith('çº¿ç´¢:') and not is_sufficient:
                    result_content = line.split(':', 1)[1].strip()
                    # å¦‚æœçº¿ç´¢è·¨å¤šè¡Œï¼Œç»§ç»­æ”¶é›†
                    for j in range(i+1, len(lines)):
                        if lines[j].strip() and not lines[j].startswith(('ç­”æ¡ˆ:', 'åˆ¤æ–­:')):
                            result_content += " " + lines[j].strip()
                        else:
                            break
                    break
            
            print(f"æ··åˆçŸ¥è¯†æ¨ç†ç»“æœ: {'è¶³å¤Ÿ' if is_sufficient else 'ä¸è¶³å¤Ÿ'}")
            if is_sufficient:
                print(f"ç­”æ¡ˆ: {result_content[:100]}...")
            else:
                print(f"çº¿ç´¢: {result_content[:100]}...")
            
            return is_sufficient, result_content
            
        except Exception as e:
            print(f"æ··åˆçŸ¥è¯†æ¨ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return False, "æ¨ç†è¿‡ç¨‹å‡ºé”™"


    async def tog2_multi_hop_reasoning(self, query: str) -> Dict[str, Any]: 
        try:
            # === 1. åˆå§‹åŒ–é˜¶æ®µ ===
            print("=" * 50)
            print("ğŸ“ åˆå§‹åŒ–é˜¶æ®µ")
            print("=" * 50)
            
            # 1.1 å®ä½“æå–
            print("1.1 æå–å®ä½“...")
            extracted_entities = await self.extract_entities(query)
            print(f"æå–åˆ°çš„å®ä½“: {extracted_entities}")
            
            if not extracted_entities:
                print("âŒ æœªèƒ½æå–åˆ°å®ä½“ï¼Œé€€å‡º")
                return {"query": query, "entities": [], "response": "æ— æ³•ä»é—®é¢˜ä¸­æå–åˆ°æœ‰æ•ˆå®ä½“", "hops": 0}
            
            # 1.2 Topic Prune
            print("\n1.2 Topic Prune...")
            topic_entities = await self.topic_prune(query, extracted_entities)
            
            # 1.3 ä»topic entitieså…³è”çš„chunksä¸­æ£€ç´¢
            print("\n1.3 ä»topic entitieså…³è”çš„chunksä¸­æ£€ç´¢...")
            initial_contexts = await self.retrieve_chunks_from_topic_entities(
                query, topic_entities, k=self.context_k
            )
            
            # 1.4 åˆå§‹æ¨ç†
            print("\n1.4 åˆå§‹æ¨ç†...")
            is_sufficient, result = await self.hybrid_knowledge_reasoning(
                query, "", [], 
                [{"name": e, "context_score": 1.0} for e in topic_entities], 
                initial_contexts
            )
            
            if is_sufficient:
                print("âœ… åˆå§‹é˜¶æ®µå·²æ‰¾åˆ°ç­”æ¡ˆ!")
                return {
                    "query": query,
                    "entities": extracted_entities,
                    "topic_entities": topic_entities,
                    "response": result,
                    "hops": 0,
                    "found_answer": True,
                    "reasoning_type": "initial"
                }
            
            print(f"âŒ åˆå§‹ä¿¡æ¯ä¸è¶³ï¼Œè¿›å…¥å¤šè·³æ¢ç´¢")
            current_clues = result
            
            # === 2. å¤šè·³æ¢ç´¢é˜¶æ®µ ===
            print("\n" + "=" * 50)
            print("ğŸ” å¤šè·³æ¢ç´¢é˜¶æ®µ")
            print("=" * 50)
            
            current_topic_entities = topic_entities
            all_triple_paths = []
            search_history = []
            
            for hop in range(1, self.max_depth + 1):
                print(f"\nğŸƒâ€â™‚ï¸ ç¬¬ {hop} è·³æ¢ç´¢")
                print("-" * 30)
                
                print(f"å½“å‰å®ä½“: {current_topic_entities}")
                print(f"å½“å‰çº¿ç´¢: {current_clues[:100]}...")
                
                # 2.1 Relation Discovery & Prune
                print(f"\n2.1 å…³ç³»å‘ç°ä¸ç­›é€‰...")
                all_candidate_entities = []
                hop_triple_paths = {}
                
                for entity in current_topic_entities:
                    # è·å–å®ä½“çš„æ‰€æœ‰å…³ç³»
                    neighbors = await self.get_entity_neighbors(entity)
                    
                    if not neighbors:
                        continue
                    
                    # Relation Prune
                    selected_relations = await self.relation_prune(query, entity, neighbors, current_clues)
                    
                    # Entity Discovery
                    for rel in selected_relations:
                        candidate_entity = rel['name']
                        relation_type = rel['relation_type']
                        
                        # æ„å»ºtripleè·¯å¾„
                        base_path = hop_triple_paths.get(entity, [])
                        new_path = base_path + [(entity, relation_type)]
                        hop_triple_paths[candidate_entity] = new_path
                        
                        # æ·»åŠ åˆ°å€™é€‰å®ä½“
                        rel_copy = rel.copy()
                        rel_copy['source_entity'] = entity
                        all_candidate_entities.append(rel_copy)
                
                if not all_candidate_entities:
                    print(f"âŒ ç¬¬ {hop} è·³æœªæ‰¾åˆ°å€™é€‰å®ä½“ï¼Œåœæ­¢æ¢ç´¢")
                    break
                
                print(f"æ‰¾åˆ° {len(all_candidate_entities)} ä¸ªå€™é€‰å®ä½“")
                
                # 2.2 Context-based Entity Prune
                print(f"\n2.2 åŸºäºä¸Šä¸‹æ–‡çš„å®ä½“ç­›é€‰...")
                top_entities = await self.context_based_entity_prune(
                    query, all_candidate_entities, hop_triple_paths
                )
                
                # æ”¶é›†å½“å‰è·³çš„tripleè·¯å¾„
                current_paths = []
                for entity in top_entities:
                    path = hop_triple_paths.get(entity['name'], [])
                    if path:
                        current_paths.append(path)
                
                all_triple_paths.extend(current_paths)
                
                # 2.3 æ··åˆçŸ¥è¯†æ¨ç†
                print(f"\n2.3 æ··åˆçŸ¥è¯†æ¨ç†...")
                
                # æ”¶é›†æ‰€æœ‰ç›¸å…³ä¸Šä¸‹æ–‡
                all_contexts = []
                for entity in top_entities:
                    if 'top_contexts' in entity:
                        all_contexts.extend(entity['top_contexts'])
                
                is_sufficient, result = await self.hybrid_knowledge_reasoning(
                    query, current_clues, current_paths, top_entities, all_contexts
                )
                
                # è®°å½•æœç´¢å†å²
                search_history.append({
                    "hop": hop,
                    "topic_entities": current_topic_entities.copy(),
                    "candidate_entities": len(all_candidate_entities),
                    "selected_entities": [e['name'] for e in top_entities],
                    "triple_paths": current_paths,
                    "is_sufficient": is_sufficient,
                    "clues": current_clues
                })
                
                if is_sufficient:
                    print(f"âœ… ç¬¬ {hop} è·³æ‰¾åˆ°ç­”æ¡ˆ!")
                    return {
                        "query": query,
                        "entities": extracted_entities,
                        "topic_entities": topic_entities,
                        "response": result,
                        "hops": hop,
                        "found_answer": True,
                        "reasoning_type": "multi_hop",
                        "search_history": search_history,
                        "final_entities": top_entities,
                        "triple_paths": all_triple_paths
                    }
                
                # æ›´æ–°ä¸‹ä¸€è½®çš„topic entitieså’Œclues
                current_topic_entities = [e['name'] for e in top_entities]
                current_clues = result
                
                print(f"âŒ ç¬¬ {hop} è·³ä¿¡æ¯ä»ä¸è¶³å¤Ÿï¼Œç»§ç»­ä¸‹ä¸€è·³")
            
            # === 3. è¾¾åˆ°æœ€å¤§æ·±åº¦ ===
            print(f"\nâ° è¾¾åˆ°æœ€å¤§æ·±åº¦ {self.max_depth}ï¼Œç”Ÿæˆæœ€ä½³ç­”æ¡ˆ")
            
            # ä½¿ç”¨æ‰€æœ‰æ”¶é›†çš„ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            final_contexts = []
            final_entities = []
            
            for history in search_history:
                for entity_name in history['selected_entities']:
                    contexts = await self.entity_guided_context_retrieval(
                        query, entity_name, [], k=2
                    )
                    final_contexts.extend(contexts)
                    final_entities.append({"name": entity_name, "context_score": 1.0})
            
            # æœ€ç»ˆæ¨ç†
            _, final_result = await self.hybrid_knowledge_reasoning(
                query, current_clues, all_triple_paths, final_entities, final_contexts
            )
            
            return {
                "query": query,
                "entities": extracted_entities,
                "topic_entities": topic_entities,
                "response": final_result,
                "hops": self.max_depth,
                "found_answer": False,
                "reasoning_type": "exhaustive",
                "search_history": search_history,
                "final_entities": final_entities,
                "triple_paths": all_triple_paths
            }
            
        except Exception as e:
            print(f"âŒ ToG-2æ¨ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return {
                "query": query,
                "entities": [],
                "response": f"æ¨ç†è¿‡ç¨‹å‡ºé”™: {str(e)}",
                "hops": 0,
                "found_answer": False,
                "error": str(e)
            }
    

async def demo_tog2():
    rag = KnowledgeGraphRAG()
    # æµ‹è¯•æŸ¥è¯¢
    query = "2012â€”2021å¹´å…¨å›½ç¾Šè‚‰äº§é‡å¹´åº¦å˜åŒ–æƒ…å†µï¼ˆä¸‡å¨ï¼‰2021å¹´ï¼Œå…¨å›½ç¾Šè‚‰äº§é‡åŒæ¯”å¢é•¿ç‡çº¦ä¸ºï¼ˆ ï¼‰ã€‚A. 2.4%\nB. 3.4%\nC. 4.4%\nD. 5.4%"
    
    
    try:
        # ä½¿ç”¨ToG-2æ¨ç†
        result = await rag.tog2_multi_hop_reasoning(query)

        print(f"result: {result}")
        

    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()




if __name__ == "__main__":

    asyncio.run(demo_tog2())
