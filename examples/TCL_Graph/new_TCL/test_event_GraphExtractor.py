
import os
import sys

rag_factory_path = os.path.join(os.path.dirname(__file__), "..", "..")
sys.path.insert(0, rag_factory_path)
import asyncio
from rag_factory.llms.openai_llm import OpenAILLM
from rag_factory.documents.event_GraphExtractor import HyperRAGGraphExtractor
from rag_factory.documents.schema import Document
from rag_factory.documents.prompt_test import TCL_PROMPT, KnowledgeStructure, TCL_CLEAN_PROMPT
from pathlib import Path
import json
from typing import List


# ------------------------------ é…ç½®å‚æ•° ------------------------------
INPUT_FILE = Path("/data/FinAi_Mapping_Knowledge/chenmingzhen/RAG-Factory/examples/TCL_Graph/dataarc_parse/data/parsed_data_by_dataarc.json")
OUTPUT_DIR = Path("/data/FinAi_Mapping_Knowledge/chenmingzhen/RAG-Factory/examples/TCL_Graph/new_TCL")
FINAL_OUTPUT_FILE = OUTPUT_DIR / "extracted_documents.json"
BATCH_SIZE = 10


# ------------------------------ å·¥å…·å‡½æ•° ------------------------------
def serialize_documents(docs: List[Document]) -> List[dict]:
    return [
        {
            "content": doc.content,
            "metadata": doc.metadata
        } for doc in docs
    ]


def save_to_file(file_path: Path, data: List[dict]):
    with file_path.open("w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


# ------------------------------ ä¸»å¤„ç†é€»è¾‘ ------------------------------
async def main():
    print(f"[ä¿¡æ¯] æ­£åœ¨è¯»å–æ–‡æ¡£æ•°æ®: {INPUT_FILE}")
    docs = []
    try:
        with INPUT_FILE.open("r", encoding="utf-8") as f:
            document_data = json.load(f)
            for item in document_data:
                content = item.get('chunk', '')
                chunk_id = "chunk_" + str(hash(content))
                source = item.get('file_name', '')
                docs.append(Document(content=content, metadata={"source": source,"chunk_id": chunk_id}))
    except Exception as e:
        print(f"[é”™è¯¯] æ— æ³•è¯»å–è¾“å…¥æ–‡ä»¶: {e}")
        return

    print(f"[ä¿¡æ¯] æˆåŠŸè¯»å– {len(docs)} ä¸ªæ–‡æ¡£")

    # åˆå§‹åŒ– LLM å’ŒæŠ½å–å™¨
    llm = OpenAILLM(
        model_name="gpt-4.1-mini",
        api_key="sk-xxxxx",
        base_url="https://api.gptsapi.net/v1"
    )

    extractor = HyperRAGGraphExtractor(
        llm=llm,
        extract_prompt=TCL_PROMPT,
        response_format=KnowledgeStructure,
        enable_cleaning=True,
        clean_prompt=TCL_CLEAN_PROMPT
    )

    all_result_docs = []
    total_batches = (len(docs) + BATCH_SIZE - 1) // BATCH_SIZE

    for batch_num, i in enumerate(range(0, len(docs), BATCH_SIZE), start=1):
        batch_docs = docs[i:i + BATCH_SIZE]
        print(f"\n[æ‰¹æ¬¡ {batch_num}/{total_batches}] å¼€å§‹å¤„ç† {len(batch_docs)} ä¸ªæ–‡æ¡£...")

        try:
            result_docs = await extractor.acall(batch_docs)
        except Exception as e:
            print(f"[é”™è¯¯] æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥: {e}")
            continue

        all_result_docs.extend(result_docs)
        print(f"[æ‰¹æ¬¡ {batch_num}] å®Œæˆï¼Œå·²ç´¯è®¡å¤„ç† {len(all_result_docs)} ä¸ªæ–‡æ¡£")

        if batch_num % 1 == 0 or batch_num == total_batches:
            temp_file = OUTPUT_DIR / f"temp_extracted_batch_{batch_num}.json"
            save_to_file(temp_file, serialize_documents(all_result_docs))
            print(f"[ä¿å­˜] æ‰¹æ¬¡ {batch_num} ä¸´æ—¶ç»“æœå·²ä¿å­˜: {temp_file}")

    # ä¿å­˜æœ€ç»ˆç»“æœ
    print("\n[ä¿¡æ¯] æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œå¼€å§‹ä¿å­˜æœ€ç»ˆç»“æœ...")
    save_to_file(FINAL_OUTPUT_FILE, serialize_documents(all_result_docs))
    print(f"[å®Œæˆ] æ‰€æœ‰æ–‡æ¡£ä¿å­˜æˆåŠŸï¼Œå…±å¤„ç† {len(all_result_docs)} ä¸ªæ–‡æ¡£")
    print(f"[è·¯å¾„] {FINAL_OUTPUT_FILE}")

    # ç»Ÿè®¡ä¿¡æ¯
    total_events = sum(len(doc.metadata.get("events", [])) for doc in all_result_docs)
    total_mentions = sum(len(doc.metadata.get("mentions", [])) for doc in all_result_docs)
    total_entity_relations = sum(len(doc.metadata.get("entity_relations", [])) for doc in all_result_docs)
    total_event_relations = sum(len(doc.metadata.get("event_relations", [])) for doc in all_result_docs)

    print("\nğŸ“Š å¤„ç†ç»Ÿè®¡ï¼š")
    print(f"- æ€»æ–‡æ¡£æ•°: {len(all_result_docs)}")
    print(f"- æ€»äº‹ä»¶æ•°: {total_events}")
    print(f"- æ€»å®ä½“æåŠæ•°: {total_mentions}")
    print(f"- æ€»å®ä½“å…³ç³»æ•°: {total_entity_relations}")
    print(f"- æ€»äº‹ä»¶å…³ç³»æ•°: {total_event_relations}")


# ------------------------------ å¯åŠ¨å…¥å£ ------------------------------
if __name__ == "__main__":
    asyncio.run(main())