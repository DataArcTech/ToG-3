from rag_flow_example_commentary import TCL_RAG
from examples.simple_graphrag.simple_query import KnowledgeGraphRAG
from examples.event_graphrag.test_online_retrieval import OnlineRetrievalTester, TestConfig
import yaml
import json
import os
import copy
from dataclasses import dataclass
import time
import asyncio
from rag_factory.Retrieval import Document
import re
import itertools

def extract_json_from_markdown(text):
    """ä»Markdownæ–‡æœ¬ä¸­æå–JSONå†…å®¹"""
    json_match = re.search(r'["\']*(?:json)?\s*(\{.*\})\s*["\']*', text, re.DOTALL)
    if json_match:
        return json.loads(json_match.group(1))
    
    json_match = re.search(r'\{.*\}', text, re.DOTALL)
    if json_match:
        return json.loads(json_match.group(0))
    
    return None

@dataclass
class Example_query:
    question_content: str
    materials: str
    answer: str

@dataclass
class Example_result:
    query_id: str
    graph_spend_time: float
    seed_nodes: list
    graph_retrieval: list
    graph_answer: str

class MultiGPUConfigManager:
    """å¤šGPUé…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, base_config_path: str):
        self.base_config_path = base_config_path
        self.base_config = self._load_base_config()
    
    def _load_base_config(self) -> dict:
        """åŠ è½½åŸºç¡€é…ç½®æ–‡ä»¶"""
        with open(self.base_config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def create_gpu_config(self, gpu_id: int, test_round: int) -> dict:
        """ä¸ºæŒ‡å®šGPUå’Œæµ‹è¯•è½®æ¬¡åˆ›å»ºé…ç½®"""
        config = copy.deepcopy(self.base_config)
        
        # æ›´æ–°embeddingè®¾å¤‡
        config['embedding']['model_kwargs']['device'] = f"cuda:{gpu_id}"
        
        # æ›´æ–°rerankerè®¾å¤‡
        if 'reranker' in config:
            config['reranker']['device_id'] = f"cuda:{gpu_id}"
        
        return config
    
    def create_file_path(self, gpu_id: int, test_round: int, base_path: str) -> str:
        """åˆ›å»ºå¯¹åº”çš„æ–‡ä»¶è·¯å¾„"""
        dir_path = os.path.dirname(base_path)
        file_name = os.path.basename(base_path)
        
        name_without_ext = os.path.splitext(file_name)[0]
        ext = os.path.splitext(file_name)[1]
        
        new_file_name = f"{name_without_ext}_gpu{gpu_id}_round{test_round}{ext}"
        return os.path.join(dir_path, new_file_name)

async def write_result(rag, example_data, graph_path, rag_path, llm_path, file_path, gpu_id, test_round, graph_scores: dict, rag_scores: dict, llm_scores: dict, graph_win: dict, rag_win: dict, llm_win: dict):
    """å†™å…¥ç»“æœåˆ°æŒ‡å®šæ–‡ä»¶"""
    rag_single_scores = []
    graph_single_scores = []
    llm_single_scores = []
    rag_single_win = []
    graph_single_win = []
    llm_single_win = []
    rag_time = []
    llm_time = []
    graph_time = []
    rag_score_threshold = 0.3
    graph_score_threshold = 0.1
    graph_result = []
    rag_result = []
    llm_result = []
    graph_path = graph_path.replace("answer_by_graph_commentary_multi_gpu.json", f"answer_by_graph_commentary_multi_gpu_gpu{gpu_id}_round{test_round}.json")
    rag_path = rag_path.replace("answer_by_rag_commentary_multi_gpu.json", f"answer_by_rag_commentary_multi_gpu_gpu{gpu_id}_round{test_round}.json")
    llm_path = llm_path.replace("answer_by_llm_commentary_multi_gpu.json", f"answer_by_llm_commentary_multi_gpu_gpu{gpu_id}_round{test_round}.json")

    # è¯»å–å„GPUçš„ç»“æœæ–‡ä»¶
    with open(graph_path, 'r', encoding='utf-8') as f:
        graph_result = json.load(f)
        graph_result = {result["query_id"]: result for result in graph_result}
    with open(rag_path, 'r', encoding='utf-8') as f:
        rag_result = json.load(f)
        rag_result = {result["query_id"]: result for result in rag_result}
    with open(llm_path, 'r', encoding='utf-8') as f:
        llm_result = json.load(f)
        llm_result = {result["query_id"]: result for result in llm_result}

    print(f"ğŸš€ GPU {gpu_id} ç¬¬ {test_round} è½®ç”³è®ºå¯¹æ¯”æµ‹è¯•å¼€å§‹ï¼Œç»“æœä¿å­˜åˆ°: {file_path}")
    
    if os.path.exists(file_path):
        os.remove(file_path)
    
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(f"GPU {gpu_id} ç¬¬ {test_round} è½®ç”³è®ºå¯¹æ¯”æµ‹è¯•ç»“æœ" + "\n")
        f.write("="*80 + "\n")
    
    for idx, example in enumerate(example_data):
        query_id = idx
        query = example.question_content
        answer = example.answer
        materials = example.materials
        
        graph_answer = graph_result[query_id]['graph_answer']
        rag_answer = rag_result[query_id]['rag_answer']
        llm_answer = llm_result[query_id]['llm_answer']

        # ç­”æ¡ˆè´¨é‡è¯„ä¼°
        judge = await asyncio.to_thread(rag.judge_answer, query, materials, graph_answer, rag_answer, llm_answer, answer)
        judge_json = extract_json_from_markdown(judge)

        # ç»Ÿè®¡å¾—åˆ†
        rag_single_scores.append(judge_json['rag_score'])
        graph_single_scores.append(judge_json['graph_score'])
        llm_single_scores.append(judge_json['llm_score'])
        graph_scores[query_id].append(judge_json['graph_score'])
        rag_scores[query_id].append(judge_json['rag_score'])
        llm_scores[query_id].append(judge_json['llm_score'])
        
        # ç»Ÿè®¡è·èƒœæƒ…å†µ
        if judge_json['recommend'] == 'graph':
            graph_single_win.append(query)
            graph_win[query_id] += 1
        elif judge_json['recommend'] == 'rag':
            rag_single_win.append(query)
            rag_win[query_id] += 1
        else:
            llm_single_win.append(query)
            llm_win[query_id] += 1
        
        # ç»Ÿè®¡æ—¶é—´
        graph_time.append(graph_result[query_id]['graph_spend_time'])
        rag_time.append(rag_result[query_id]['rag_spend_time'])
        llm_time.append(llm_result[query_id]['llm_spend_time'])

        # å†™å…¥è¯¦ç»†ç»“æœ
        with open(file_path, 'a', encoding='utf-8') as f:
            f.write("query_id: " + str(query_id) + "\n")
            f.write("query: \n" + query + "\n\n")
            f.write("materials: \n" + str(materials) + "\n\n")
            f.write("-"*50 + "\n\n")
            f.write("å‚è€ƒç­”æ¡ˆ: \n" + answer + "\n\n")
            f.write("-"*50 + "\n\n")
            f.write("graph_spend_time: " + str(graph_result[query_id]['graph_spend_time']) + "\n\n")
            f.write("answer by graph: \n" + json.dumps(graph_answer, ensure_ascii=False, indent=2) + "\n\n")
            f.write(f"\næå–çš„å®ä½“: {graph_result[query_id]['seed_nodes']}" + "\n\n")
            f.write("\n=== å®ä½“æ£€ç´¢è¯¦æƒ… ===" + "\n")
            f.write("graph+ragæ£€ç´¢ç»“æœï¼š\n")
            f.write(json.dumps(graph_result[query_id]['graph_retrieval'], ensure_ascii=False, indent=2) + "\n\n")
            f.write("-"*50 + "\n\n")
            f.write("rag_spend_time: " + str(rag_result[query_id]['rag_spend_time']) + "\n\n")
            f.write("answer by rag: \n" + json.dumps(rag_answer, ensure_ascii=False, indent=2) + "\n\n")
            f.write("retrieved materials: \n")
            f.write(json.dumps(rag_result[query_id]['rag_retrieval'], ensure_ascii=False, indent=2) + "\n\n")
            f.write("-"*50 + "\n\n")
            f.write("llm_spend_time: " + str(llm_result[query_id]['llm_spend_time']) + "\n\n")
            f.write("answer by llm: \n" + json.dumps(llm_answer, ensure_ascii=False, indent=2) + "\n\n")
            f.write("-"*50 + "\n\n")
            f.write("judge: \n" + judge + "\n\n")
            f.write("-"*100 + "\n\n")

    # å†™å…¥ç»Ÿè®¡ç»“æœ
    with open(file_path, 'a', encoding='utf-8') as f:
        f.write("é¢˜ç›®æ€»æ•°: " + str(len(rag_scores)) + "\n")
        f.write("------------------------------------------------------------------------------------------------" + "\n")
        f.write("graphå¹³å‡å¾—åˆ†: " + str(round(sum(graph_single_scores)/len(graph_single_scores), 2)) + "\n")
        f.write("graphè·èƒœæ•°é‡: " + str(len(graph_single_win)) + "\n")
        f.write("------------------------------------------------------------------------------------------------" + "\n")
        f.write("ragå¹³å‡å¾—åˆ†: " + str(round(sum(rag_single_scores)/len(rag_single_scores), 2)) + "\n")
        f.write("ragè·èƒœæ•°é‡: " + str(len(rag_single_win)) + "\n")
        f.write("------------------------------------------------------------------------------------------------" + "\n")
        f.write("llmå¹³å‡å¾—åˆ†: " + str(round(sum(llm_single_scores)/len(llm_single_scores), 2)) + "\n")
        f.write("llmè·èƒœæ•°é‡: " + str(len(llm_single_win)) + "\n")
        f.write("------------------------------------------------------------------------------------------------" + "\n")
        f.write("graphæ—¶é—´: " + str(round(sum(graph_time)/len(graph_time), 2)) + "ç§’" + "\n")
        f.write("ragæ—¶é—´: " + str(round(sum(rag_time)/len(rag_time), 2)) + "ç§’" + "\n")
        f.write("llmæ—¶é—´: " + str(round(sum(llm_time)/len(llm_time), 2)) + "ç§’" + "\n")
        f.write("------------------------------------------------------------------------------------------------" + "\n")
    
    print(f"âœ… GPU {gpu_id} ç¬¬ {test_round} è½®ç”³è®ºå¯¹æ¯”æµ‹è¯•å®Œæˆï¼Œç»“æœå·²ä¿å­˜")

async def run_single_gpu_test(gpu_id: int, test_round: int, config_manager: MultiGPUConfigManager, 
                             example_data: list, graph_path: str, rag_path: str, llm_path: str, base_file_path: str,
                             rag: TCL_RAG, graph_scores: dict, rag_scores: dict, llm_scores: dict, graph_win: dict, rag_win: dict, llm_win: dict):
    """åœ¨å•ä¸ªGPUä¸Šè¿è¡Œä¸€è½®æµ‹è¯•"""
    try:
        # åˆ›å»ºæ–‡ä»¶è·¯å¾„
        file_path = config_manager.create_file_path(gpu_id, test_round, base_file_path)
        
        # è¿è¡Œæµ‹è¯•ï¼ˆä½¿ç”¨å·²åˆå§‹åŒ–çš„ragï¼‰
        await write_result(rag, example_data, graph_path, rag_path, llm_path, file_path, gpu_id, test_round, graph_scores, rag_scores, llm_scores, graph_win, rag_win, llm_win)
        
        return True
        
    except Exception as e:
        print(f"âŒ GPU {gpu_id} ç¬¬ {test_round} è½®ç”³è®ºå¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def run_multi_gpu_tests(gpu_ids: list, test_rounds: int, config_path: str, 
                             example_data_path: str, graph_path: str, rag_path: str, llm_path: str, base_result_path: str, query_id_count: int, result_path: str):
    """è¿è¡Œå¤šGPUå¤šè½®æµ‹è¯•"""
    print(f"ğŸš€ å¼€å§‹ç”³è®ºå¯¹æ¯”å¤šGPUå¤šè½®æµ‹è¯•")
    print(f"GPUè®¾å¤‡: {gpu_ids}")
    print(f"æµ‹è¯•è½®æ¬¡: {test_rounds}")
    print(f"é…ç½®æ–‡ä»¶: {config_path}")
    print(f"ç¤ºä¾‹æ•°æ®: {example_data_path}")
    print(f"å›¾æ£€ç´¢ç»“æœ: {graph_path}")
    print(f"RAGç»“æœ: {rag_path}")
    print(f"LLMç»“æœ: {llm_path}")
    print(f"ç»“æœåŸºç¡€è·¯å¾„: {base_result_path}")
    print("="*80)
    
    # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
    config_manager = MultiGPUConfigManager(config_path)
    
    # åŠ è½½ç¤ºä¾‹æ•°æ®
    example_data = []
    with open(example_data_path, 'r', encoding='utf-8') as f:
        examples = json.load(f)
        for example in examples:
            example_data.append(
                Example_query(
                    question_content=example['question_content'],
                    materials=example['materials'],
                    answer=example['answer']
                )
            )
    
    print(f"ğŸ“Š åŠ è½½äº† {len(example_data)} ä¸ªç”³è®ºæµ‹è¯•ç¤ºä¾‹")
    
    # ä¸ºæ¯ä¸ªGPUåˆå§‹åŒ–ä¸€æ¬¡RAGç³»ç»Ÿ
    gpu_rag_systems = {}
    
    print("ğŸš€ ä¸ºæ¯ä¸ªGPUåˆå§‹åŒ–RAGç³»ç»Ÿ...")
    for gpu_id in gpu_ids:
        try:
            # åˆ›å»ºGPUç‰¹å®šé…ç½®
            gpu_config = config_manager.create_gpu_config(gpu_id, 1)  # ä½¿ç”¨ç¬¬ä¸€è½®é…ç½®
            
            # åˆå§‹åŒ–RAGç³»ç»Ÿ
            rag = TCL_RAG(
                llm_config=gpu_config['llm'],
                embedding_config=gpu_config['embedding'],
                reranker_config=gpu_config['reranker'],
                retriever_config=gpu_config['retriever'],
                vector_store_config=gpu_config['store'],
                bm25_retriever_config=gpu_config['bm25']
            )
            
            gpu_rag_systems[gpu_id] = rag
            
            print(f"âœ… GPU {gpu_id} åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ GPU {gpu_id} åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ç»Ÿè®¡æ¯ä¸€é“é¢˜ç›®ä¸åŒæ¨¡å‹çš„å¹³å‡å¾—åˆ†å’Œè·èƒœæ¬¡æ•°
    graph_scores = {query_id: [] for query_id in range(query_id_count)}
    rag_scores = {query_id: [] for query_id in range(query_id_count)}
    llm_scores = {query_id: [] for query_id in range(query_id_count)}
    graph_win = {query_id: 0 for query_id in range(query_id_count)}
    rag_win = {query_id: 0 for query_id in range(query_id_count)}
    llm_win = {query_id: 0 for query_id in range(query_id_count)}

    # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
    tasks = []
    for gpu_id in gpu_ids:
        if gpu_id not in gpu_rag_systems:
            continue
        for round_num in range(1, test_rounds + 1):
            task = run_single_gpu_test(
                gpu_id, round_num, config_manager, example_data[:query_id_count], graph_path, rag_path, llm_path, base_result_path,
                gpu_rag_systems[gpu_id], graph_scores, rag_scores, llm_scores, graph_win, rag_win, llm_win
            )
            tasks.append(task)
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œ {len(tasks)} ä¸ªæµ‹è¯•ä»»åŠ¡...")
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for r in results if r is True)
    failed_count = len(results) - success_count

    # è®°å½•graph/rag/llmçš„å¹³å‡å¾—åˆ†å’Œè·èƒœæ¬¡æ•°ä»¥åŠæ¯é“é¢˜å„ä¸ªæ¨¡å‹çš„å¾—åˆ†å’Œè·èƒœæ¬¡æ•°
    with open(result_path, 'a', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write(f"æ€»è½®æ¬¡: {test_rounds}" + "\n")
        f.write(f"æ€»é¢˜ç›®æ•°: {query_id_count}" + "\n")
        f.write(f"æ€»GPUæ•°: {len(gpu_ids)}" + "\n")
        f.write(f"graphå¹³å‡å¾—åˆ†: {round(sum(itertools.chain(*graph_scores.values()))/sum(len(lst) for lst in graph_scores.values()), 2)}   æ€»åˆ†: {sum(itertools.chain(*graph_scores.values()))}  æ¬¡æ•°: {sum(len(lst) for lst in graph_scores.values())}" + "\n")
        f.write(f"ragå¹³å‡å¾—åˆ†: {round(sum(itertools.chain(*rag_scores.values()))/sum(len(lst) for lst in rag_scores.values()), 2)}   æ€»åˆ†: {sum(itertools.chain(*rag_scores.values()))}  æ¬¡æ•°: {sum(len(lst) for lst in rag_scores.values())}" + "\n")
        f.write(f"llmå¹³å‡å¾—åˆ†: {round(sum(itertools.chain(*llm_scores.values()))/sum(len(lst) for lst in llm_scores.values()), 2)}   æ€»åˆ†: {sum(itertools.chain(*llm_scores.values()))}  æ¬¡æ•°: {sum(len(lst) for lst in llm_scores.values())}" + "\n")
        f.write(f"graphè·èƒœæ¬¡æ•°: {sum(graph_win.values())}" + "\n")
        f.write(f"ragè·èƒœæ¬¡æ•°: {sum(rag_win.values())}" + "\n")
        f.write(f"llmè·èƒœæ¬¡æ•°: {sum(llm_win.values())}" + "\n")
        f.write("="*80 + "\n")
        f.write("æ¯é“é¢˜å„ä¸ªæ¨¡å‹çš„å¾—åˆ†å’Œè·èƒœæ¬¡æ•°:" + "\n")
        for query_id in range(query_id_count):
            f.write(f"é¢˜ç›®{query_id}çš„å¾—åˆ†å’Œè·èƒœæ¬¡æ•°:" + "\n")
            f.write(f"graphå¾—åˆ†: {round(sum(graph_scores[query_id])/len(graph_scores[query_id]), 2)}" + "\n")
            f.write(f"graphè·èƒœæ¬¡æ•°: {graph_win[query_id]}" + "\n")
            f.write(f"ragå¾—åˆ†: {round(sum(rag_scores[query_id])/len(rag_scores[query_id]), 2)}" + "\n")
            f.write(f"ragè·èƒœæ¬¡æ•°: {rag_win[query_id]}" + "\n")
            f.write(f"llmå¾—åˆ†: {round(sum(llm_scores[query_id])/len(llm_scores[query_id]), 2)}" + "\n")
            f.write(f"llmè·èƒœæ¬¡æ•°: {llm_win[query_id]}" + "\n")
            f.write("-"*40 + "\n")
        f.write("="*80 + "\n")


    print("\n" + "="*80)
    print(f"ğŸ“Š ç”³è®ºå¯¹æ¯”æµ‹è¯•å®Œæˆç»Ÿè®¡:")
    print(f"æ€»ä»»åŠ¡æ•°: {len(tasks)}")
    print(f"æˆåŠŸä»»åŠ¡: {success_count}")
    print(f"å¤±è´¥ä»»åŠ¡: {failed_count}")
    print(f"ä»»åŠ¡æˆåŠŸç‡: {success_count/len(tasks)*100:.1f}%")
    print("\n" + "="*80)
    if failed_count > 0:
        print(f"\nâŒ å¤±è´¥çš„ä»»åŠ¡:")
        for i, result in enumerate(results):
            if result is not True:
                print(f"  ä»»åŠ¡ {i+1}: {result}")
    
    return results

async def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‚æ•°
    gpu_ids = [0, 1, 2, 3, 4, 6, 7]  # è¦ä½¿ç”¨çš„GPUè®¾å¤‡ID
    test_rounds = 1  # æ¯ä¸ªGPUçš„æµ‹è¯•è½®æ¬¡
    query_id_count = 17 # è¦æµ‹è¯•å‰å¤šå°‘é“é¢˜ç›®
    config_path = "/finance_ML/liuyingqi/RAG-Factory/examples/TCL_rag/config_commentary_llm.yaml"
    example_data_path = "/data/FinAi_Mapping_Knowledge/liuyingqi/example/ç”³è®ºçœŸé¢˜2020-2024.json"
    graph_path = "/data/FinAi_Mapping_Knowledge/liuyingqi/commentary_test/graph_answer/answer_by_graph_commentary_multi_gpu.json"
    rag_path = "/data/FinAi_Mapping_Knowledge/liuyingqi/commentary_test/rag_answer/answer_by_rag_commentary_multi_gpu.json"
    llm_path = "/data/FinAi_Mapping_Knowledge/liuyingqi/commentary_test/llm_answer/answer_by_llm_commentary_multi_gpu.json"
    base_result_path = "/data/FinAi_Mapping_Knowledge/liuyingqi/commentary_test/judge_result/answer_by_judge_commentary_multi_gpu.txt"
    result_path = "/data/FinAi_Mapping_Knowledge/liuyingqi/commentary_test/judge_result/4.1_result.txt"
    
    # è¿è¡Œå¤šGPUå¤šè½®æµ‹è¯•
    await run_multi_gpu_tests(gpu_ids, test_rounds, config_path, example_data_path, graph_path, rag_path, llm_path, base_result_path, query_id_count, result_path)

if __name__ == "__main__":
    asyncio.run(main())


