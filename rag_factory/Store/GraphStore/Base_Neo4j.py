"""
Neo4jå›¾å­˜å‚¨åŸºç±»
æä¾›Neo4jå›¾æ•°æ®åº“çš„é€šç”¨æ“ä½œåŠŸèƒ½
"""

import asyncio
import hashlib
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from datetime import datetime
import neo4j

from rag_factory.Embed import Embeddings
from rag_factory.documents.schema import Document

from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)

neo4j_retry_errors = (
    neo4j.exceptions.ServiceUnavailable,
    neo4j.exceptions.TransientError,
    neo4j.exceptions.WriteServiceUnavailable,
    neo4j.exceptions.ClientError,
)


class GraphStoreBaseNeo4j(ABC):
    """
    Neo4jå›¾å­˜å‚¨åŸºç±»
    æä¾›Neo4jæ•°æ®åº“è¿æ¥ã€æŸ¥è¯¢æ‰§è¡Œã€çº¦æŸç®¡ç†ç­‰é€šç”¨åŠŸèƒ½
    """
    
    def __init__(self, url: str, username: str, password: str, database: str, embedding: Optional[Embeddings] = None):
        """
        åˆå§‹åŒ–Neo4jå›¾å­˜å‚¨åŸºç±»
        
        Args:
            url: Neo4jæ•°æ®åº“URL
            username: ç”¨æˆ·å
            password: å¯†ç 
            database: æ•°æ®åº“å
            embedding: åµŒå…¥æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        """
        self._driver = None
        self._driver_lock = asyncio.Lock()
        self.database = database
        self.embedding = embedding
        
        try:
            self._driver: neo4j.AsyncDriver = neo4j.AsyncGraphDatabase.driver(
                url, auth=(username, password)
            )
            print(f"âœ… æˆåŠŸè¿æ¥åˆ°Neo4jæ•°æ®åº“: {url}")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–Neo4jè¿æ¥å¤±è´¥: {e}")
            raise

    async def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self._driver:
            await self._driver.close()
            self._driver = None

    async def __aexit__(self, exc_type, exc, tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡ºæ–¹æ³•"""
        if self._driver:
            await self._driver.close()

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10), retry=retry_if_exception_type(neo4j_retry_errors))
    async def _execute_query(self, query: str, parameters: Dict[str, Any] = None):
        """
        æ‰§è¡ŒNeo4jæŸ¥è¯¢çš„é€šç”¨æ–¹æ³•ï¼Œå¸¦é‡è¯•æœºåˆ¶
        
        Args:
            query: CypheræŸ¥è¯¢è¯­å¥
            parameters: æŸ¥è¯¢å‚æ•°
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        if parameters is None:
            parameters = {}
            
        async with self._driver.session(database=self.database) as session:
            return await session.run(query, **parameters)

    def _generate_unique_id(self, prefix: str, content: str) -> str:
        """
        ç”Ÿæˆå”¯ä¸€ID
        
        Args:
            prefix: IDå‰ç¼€ (å¦‚ "chunk_", "event_", "entity_")
            content: ç”¨äºç”Ÿæˆhashçš„å†…å®¹
            
        Returns:
            å”¯ä¸€IDå­—ç¬¦ä¸²
        """
        hash_value = hashlib.md5(content.encode('utf-8')).hexdigest()[:8]
        return f"{prefix}{hash_value}"

    async def filter_existing_chunks(self, documents: List[Document]) -> List[Document]:
        """
        è¿‡æ»¤å·²å­˜åœ¨çš„chunkï¼Œè¿”å›æœªå¤„ç†è¿‡çš„chunk
        
        Args:
            documents: å¾…æ£€æŸ¥çš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            æœªåœ¨Neo4jä¸­å­˜åœ¨çš„æ–‡æ¡£åˆ—è¡¨
        """
        print(f"ğŸ” æ­£åœ¨æ£€æŸ¥ {len(documents)} ä¸ªchunkæ˜¯å¦å·²å­˜åœ¨...")
        
        # ä¸ºæ–‡æ¡£ç”Ÿæˆchunk ID
        chunk_ids = []
        doc_to_chunk_id = {}

        # å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰chunk_idï¼Œåˆ™ç”Ÿæˆchunk_id
        for doc in documents:
            if "chunk_id" not in doc.metadata:
                chunk_content = doc.content.strip()
                chunk_id = self._generate_unique_id("chunk_", chunk_content)
                doc.metadata["chunk_id"] = chunk_id
                chunk_ids.append(chunk_id)
            else:
                chunk_id = doc.metadata["chunk_id"]
            doc_to_chunk_id[chunk_id] = doc
        
        # æŸ¥è¯¢Neo4jä¸­å·²å­˜åœ¨çš„chunk ID
        existing_chunks = set()
        if chunk_ids:
            query = """
            MATCH (c:Chunk)
            WHERE c.id_ IN $chunk_ids
            RETURN c.id_ as chunk_id
            """
            async with self._driver.session(database=self.database) as session:
                result = await session.run(query, chunk_ids=chunk_ids)
                async for record in result:
                    existing_chunks.add(record["chunk_id"])
        
        # ç­›é€‰å‡ºä¸å­˜åœ¨çš„æ–‡æ¡£
        new_documents = []
        for chunk_id, doc in doc_to_chunk_id.items():
            if chunk_id not in existing_chunks:
                new_documents.append(doc)
            else:
                print(f"  âš ï¸ è·³è¿‡å·²å­˜åœ¨çš„chunk: {chunk_id}")
        
        print(f"  âœ… å‘ç° {len(new_documents)} ä¸ªæ–°chunkï¼Œå·²è·³è¿‡ {len(existing_chunks)} ä¸ªé‡å¤chunk")
        return new_documents

# TODO åˆ†æ‰¹å¤„ç†ï¼Œæœ‰bugï¼Œä¸èƒ½è‡ªåŠ¨å¯¹æ‰€æœ‰èŠ‚ç‚¹ç”ŸæˆåµŒå…¥å‘é‡ï¼Œè€Œæ˜¯ç”Ÿæˆä¸€éƒ¨åˆ†ï¼Œç„¶åå°±åœæ­¢äº†
    async def _generate_embeddings(self):
        """è‡ªåŠ¨ä¸ºæ²¡æœ‰embeddingçš„èŠ‚ç‚¹ç”ŸæˆåµŒå…¥å‘é‡"""
        if not self.embedding:
            print("âš ï¸ æœªæä¾›åµŒå…¥æ¨¡å‹ï¼Œè·³è¿‡å‘é‡ç”Ÿæˆ")
            return
            
        print("ğŸ§  æ­£åœ¨è‡ªåŠ¨ç”Ÿæˆç¼ºå¤±çš„åµŒå…¥å‘é‡...")
        
        # å…ˆè·å–æ€»æ•°ç”¨äºè¿›åº¦æ˜¾ç¤º
        async def get_total_count(node_type, condition="embedding IS NULL"):
            count_query = f"MATCH (n:{node_type}) WHERE n.{condition} RETURN count(n) as total"
            async with self._driver.session(database=self.database) as session:
                result = await session.run(count_query)
                record = await result.single()
                return record["total"] if record else 0
        
        # å¤„ç†Chunks
        total_chunks = await get_total_count("Chunk")
        if total_chunks > 0:
            print(f"  ğŸ“Š å‘ç° {total_chunks} ä¸ªchunkéœ€è¦ç”ŸæˆåµŒå…¥å‘é‡")
            await self._process_chunk_embeddings(total_chunks)
        else:
            print("  âœ… æ‰€æœ‰chunkå·²æœ‰åµŒå…¥å‘é‡")
        
        # å¤„ç†Entities  
        total_entities = await get_total_count("Entity")
        if total_entities > 0:
            print(f"  ğŸ“Š å‘ç° {total_entities} ä¸ªå®ä½“éœ€è¦ç”ŸæˆåµŒå…¥å‘é‡")
            await self._process_entity_embeddings(total_entities)
        else:
            print("  âœ… æ‰€æœ‰å®ä½“å·²æœ‰åµŒå…¥å‘é‡")
        
        # å¤„ç†Events
        total_events = await get_total_count("Event")
        if total_events > 0:
            print(f"  ğŸ“Š å‘ç° {total_events} ä¸ªäº‹ä»¶éœ€è¦ç”ŸæˆåµŒå…¥å‘é‡")
            await self._process_event_embeddings(total_events)
        else:
            print("  âœ… æ‰€æœ‰äº‹ä»¶å·²æœ‰åµŒå…¥å‘é‡")

    async def _process_chunk_embeddings(self, total_count):
        """å¤„ç†chunkåµŒå…¥å‘é‡ç”Ÿæˆ"""
        batch_size = 100
        processed = 0
        
        while processed < total_count:
            # æ¯æ¬¡é‡æ–°æŸ¥è¯¢ç¡®ä¿è·å–æœ€æ–°çš„æœªå¤„ç†æ•°æ®
            query = """
            MATCH (c:Chunk)
            WHERE c.embedding IS NULL
            RETURN c.id_ as id_, c.content as content
            LIMIT $limit
            """
            
            async with self._driver.session(database=self.database) as session:
                result = await session.run(query, {"limit": batch_size})
                records = await result.data()  # ä½¿ç”¨data()æ–¹æ³•è·å–æ‰€æœ‰è®°å½•
                
                if not records:
                    break  # æ²¡æœ‰æ›´å¤šéœ€è¦å¤„ç†çš„æ•°æ®
                
                chunks_to_embed = []
                chunk_texts = []
                
                for record in records:
                    chunk_id = record["id_"]
                    content = record["content"] or ""
                    
                    if content.strip():  # è·³è¿‡ç©ºå†…å®¹
                        chunk_texts.append(content)
                        chunks_to_embed.append(chunk_id)
                
                if chunks_to_embed:
                    print(f"    ğŸ§  å¤„ç†chunk {processed + 1}-{processed + len(chunks_to_embed)}/{total_count}")
                    embeddings = self.embedding.embed_documents(chunk_texts)
                    
                    # æ‰¹é‡æ›´æ–°
                    update_query = """
                    UNWIND $updates as update
                    MATCH (c:Chunk {id_: update.id_})
                    SET c.embedding = update.embedding
                    """
                    
                    updates = [
                        {"id_": chunk_id, "embedding": embedding}
                        for chunk_id, embedding in zip(chunks_to_embed, embeddings)
                    ]
                    
                    await self._execute_query(update_query, {"updates": updates})
                    processed += len(chunks_to_embed)
                else:
                    # å¦‚æœè¿™æ‰¹è®°å½•éƒ½æ˜¯ç©ºå†…å®¹ï¼Œæ ‡è®°ä¸ºå·²å¤„ç†ä»¥é¿å…æ— é™å¾ªç¯
                    empty_updates = []
                    for record in records:
                        chunk_id = record["id_"]
                        content = record["content"] or ""
                        if not content.strip():
                            empty_updates.append(chunk_id)
                    
                    if empty_updates:
                        # ä¸ºç©ºå†…å®¹çš„chunkè®¾ç½®ç©ºçš„embeddingæˆ–æ ‡è®°
                        empty_query = """
                        UNWIND $ids as id_
                        MATCH (c:Chunk {id_: id_})
                        SET c.embedding = []
                        """
                        await self._execute_query(empty_query, {"ids": empty_updates})
                        processed += len(empty_updates)

    async def _process_entity_embeddings(self, total_count):
        """å¤„ç†å®ä½“åµŒå…¥å‘é‡ç”Ÿæˆ"""
        batch_size = 100
        processed = 0
        
        while processed < total_count:
            query = """
            MATCH (e:Entity)
            WHERE e.embedding IS NULL
            RETURN e.id_ as id_, e.entity_name as name, e.entity_descriptions as descriptions
            LIMIT $limit
            """
            
            async with self._driver.session(database=self.database) as session:
                result = await session.run(query, {"limit": batch_size})
                records = await result.data()
                
                if not records:
                    break
                
                entities_to_embed = []
                entity_texts = []
                
                for record in records:
                    entity_id = record["id_"]
                    entity_name = record["name"] or ""
                    descriptions = record["descriptions"] or []
                    
                    text = f"{entity_name}: {' '.join(descriptions)}"
                    entity_texts.append(text)
                    entities_to_embed.append(entity_id)
                
                if entities_to_embed:
                    print(f"    ğŸ§  å¤„ç†å®ä½“ {processed + 1}-{processed + len(entities_to_embed)}/{total_count}")
                    embeddings = self.embedding.embed_documents(entity_texts)
                    
                    update_query = """
                    UNWIND $updates as update
                    MATCH (e:Entity {id_: update.id_})
                    SET e.embedding = update.embedding
                    """
                    
                    updates = [
                        {"id_": entity_id, "embedding": embedding}
                        for entity_id, embedding in zip(entities_to_embed, embeddings)
                    ]
                    
                    await self._execute_query(update_query, {"updates": updates})
                    processed += len(entities_to_embed)

    async def _process_event_embeddings(self, total_count):
        """å¤„ç†äº‹ä»¶åµŒå…¥å‘é‡ç”Ÿæˆ"""
        batch_size = 100
        processed = 0
        
        while processed < total_count:
            query = """
            MATCH (e:Event)
            WHERE e.embedding IS NULL
            RETURN e.id_ as id_, e.content as content
            LIMIT $limit
            """
            
            async with self._driver.session(database=self.database) as session:
                result = await session.run(query, {"limit": batch_size})
                records = await result.data()
                
                if not records:
                    break
                
                events_to_embed = []
                event_texts = []
                
                for record in records:
                    event_id = record["id_"]
                    content = record["content"] or ""
                    
                    if content.strip():  # è·³è¿‡ç©ºå†…å®¹
                        event_texts.append(content)
                        events_to_embed.append(event_id)
                
                if events_to_embed:
                    print(f"    ğŸ§  å¤„ç†äº‹ä»¶ {processed + 1}-{processed + len(events_to_embed)}/{total_count}")
                    embeddings = self.embedding.embed_documents(event_texts)
                    
                    update_query = """
                    UNWIND $updates as update
                    MATCH (e:Event {id_: update.id_})
                    SET e.embedding = update.embedding
                    """
                    
                    updates = [
                        {"id_": event_id, "embedding": embedding}
                        for event_id, embedding in zip(events_to_embed, embeddings)
                    ]
                    
                    await self._execute_query(update_query, {"updates": updates})
                    processed += len(events_to_embed)
                else:
                    # å¤„ç†ç©ºå†…å®¹çš„äº‹ä»¶
                    empty_updates = []
                    for record in records:
                        event_id = record["id_"]
                        content = record["content"] or ""
                        if not content.strip():
                            empty_updates.append(event_id)
                    
                    if empty_updates:
                        empty_query = """
                        UNWIND $ids as id_
                        MATCH (e:Event {id_: id_})
                        SET e.embedding = []
                        """
                        await self._execute_query(empty_query, {"ids": empty_updates})
                        processed += len(empty_updates)

    async def _merge_duplicate_entities(self):
        """ä½¿ç”¨APOCåˆå¹¶å¯èƒ½é‡å¤çš„å®ä½“èŠ‚ç‚¹ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰"""
        print("ğŸ”„ æ­£åœ¨ä½¿ç”¨APOCåˆå¹¶é‡å¤å®ä½“...")
        
        try:
            # æ£€æŸ¥APOCæ˜¯å¦å¯ç”¨
            apoc_check_query = "RETURN apoc.version() as version"
            await self._execute_query(apoc_check_query)
            print("  âœ… APOCæ’ä»¶å¯ç”¨ï¼Œå¼€å§‹åˆå¹¶é‡å¤å®ä½“")
            
            # æŸ¥æ‰¾åŒåå®ä½“å¹¶åˆå¹¶
            merge_query = """
            CALL apoc.periodic.iterate(
                "MATCH (e1:Entity), (e2:Entity) 
                 WHERE e1.entity_name = e2.entity_name AND id(e1) > id(e2) 
                 RETURN e1, e2",
                "CALL apoc.refactor.mergeNodes([e1, e2], {
                    properties: {
                        entity_descriptions: 'combine',
                        mention_texts: 'combine',
                        source_chunks: 'combine',
                        update_time: 'overwrite'
                    }
                }) YIELD node RETURN node",
                {batchSize: 10, parallel: false}
            )
            """
            
            await self._execute_query(merge_query)
            print("  âœ… å®Œæˆå®ä½“åˆå¹¶")
            
        except Exception as e:
            print(f"  âš ï¸ APOCåˆå¹¶åŠŸèƒ½ä¸å¯ç”¨æˆ–å¤±è´¥: {e}")
            print("  ğŸ’¡ å»ºè®®å®‰è£…APOCæ’ä»¶ä»¥è·å¾—æ›´å¥½çš„å®ä½“åˆå¹¶åŠŸèƒ½")

    async def get_graph_statistics(self) -> Dict[str, int]:
        """è·å–å›¾ç»Ÿè®¡ä¿¡æ¯"""
        queries = self._get_statistics_queries()
        
        statistics = {}
        for stat_name, query in queries.items():
            try:
                async with self._driver.session(database=self.database) as session:
                    result = await session.run(query)
                    records = await result.data()
                    if records:
                        statistics[stat_name] = records[0]["count"]
                    else:
                        statistics[stat_name] = 0
            except Exception as e:
                print(f"âš ï¸ è·å–ç»Ÿè®¡ä¿¡æ¯ {stat_name} æ—¶å‡ºé”™: {e}")
                statistics[stat_name] = 0
        
        return statistics

    async def delete_graph_data(self, delete_type: str = "all"):
        """
        åˆ é™¤å›¾æ•°æ®
        
        Args:
            delete_type: åˆ é™¤ç±»å‹ ("all", "entities", "events", "relations")
        """
        print(f"ğŸ—‘ï¸ æ­£åœ¨åˆ é™¤å›¾æ•°æ®: {delete_type}")
        
        delete_queries = self._get_delete_queries()
        
        if delete_type not in delete_queries:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ é™¤ç±»å‹: {delete_type}")
        
        queries = delete_queries[delete_type]
        for query in queries:
            try:
                await self._execute_query(query)
                print(f"  âœ“ æ‰§è¡Œåˆ é™¤æŸ¥è¯¢: {query}")
            except Exception as e:
                print(f"  âŒ åˆ é™¤æŸ¥è¯¢å¤±è´¥: {e}")

    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æµ‹è¯•æ•°æ®åº“è¿æ¥
            async with self._driver.session(database=self.database) as session:
                result = await session.run("RETURN 1 as test")
                records = await result.data()
                if not records or records[0]["test"] != 1:
                    raise Exception("æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥")
            
            # è·å–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            stats = await self.get_graph_statistics()
            
            return {
                "status": "healthy",
                "database": self.database,
                "statistics": stats,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    # =============================================================================
    # æŠ½è±¡æ–¹æ³• - å­ç±»å¿…é¡»å®ç°
    # =============================================================================
    
    @abstractmethod
    async def store_graph(self, documents: List[Document]) -> bool:
        """
        å­˜å‚¨å›¾ç»“æ„åˆ°Neo4jï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            bool: å­˜å‚¨æ˜¯å¦æˆåŠŸ
        """
        pass

    @abstractmethod
    async def _create_constraints_and_indexes(self):
        """åˆ›å»ºæ•°æ®åº“çº¦æŸå’Œç´¢å¼•ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰"""
        pass

    @abstractmethod
    def _get_statistics_queries(self) -> Dict[str, str]:
        """è·å–ç»Ÿè®¡æŸ¥è¯¢è¯­å¥ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰"""
        pass

    @abstractmethod
    def _get_delete_queries(self) -> Dict[str, List[str]]:
        """è·å–åˆ é™¤æŸ¥è¯¢è¯­å¥ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰"""
        pass
