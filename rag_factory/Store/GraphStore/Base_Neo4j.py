"""
Neo4jå›¾å­˜å‚¨åŸºç±»
æä¾›Neo4jå›¾æ•°æ®åº“çš„é€šç”¨æ“ä½œåŠŸèƒ½
"""

import asyncio
import hashlib
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from datetime import datetime
import neo4j

from rag_factory.Embed import Embeddings
from rag_factory.documents.schema import Document

from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)

import logging

logger = logging.getLogger(__name__)

neo4j_retry_errors = (
    neo4j.exceptions.ServiceUnavailable,
    neo4j.exceptions.TransientError,
    neo4j.exceptions.WriteServiceUnavailable,
    neo4j.exceptions.ClientError,
)


class GraphStoreBaseNeo4j(ABC):
    """
    Neo4jå›¾å­˜å‚¨åŸºç±»
    æä¾›Neo4jæ•°æ®åº“è¿æ¥ã€æŸ¥è¯¢æ‰§è¡Œã€çº¦æŸç®¡ç†ç­‰é€šç”¨åŠŸèƒ½
    """
    
    def __init__(self, url: str, username: str, password: str, database: str, embedding: Optional[Embeddings] = None):
        """
        åˆå§‹åŒ–Neo4jå›¾å­˜å‚¨åŸºç±»
        
        Args:
            url: Neo4jæ•°æ®åº“URL
            username: ç”¨æˆ·å
            password: å¯†ç 
            database: æ•°æ®åº“å
            embedding: åµŒå…¥æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        """
        self._driver = None
        self._driver_lock = asyncio.Lock()
        self.database = database
        self.embedding = embedding
        
        try:
            self._driver: neo4j.AsyncDriver = neo4j.AsyncGraphDatabase.driver(
                url, auth=(username, password)
            )
            logger.info(f"âœ… æˆåŠŸè¿æ¥åˆ°Neo4jæ•°æ®åº“: {url}")
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–Neo4jè¿æ¥å¤±è´¥: {e}")
            raise

    async def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self._driver:
            await self._driver.close()
            self._driver = None

    async def __aexit__(self, exc_type, exc, tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡ºæ–¹æ³•"""
        if self._driver:
            await self._driver.close()

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10), retry=retry_if_exception_type(neo4j_retry_errors))
    async def _execute_query(self, query: str, parameters: Dict[str, Any] = None):
        """
        æ‰§è¡ŒNeo4jæŸ¥è¯¢çš„é€šç”¨æ–¹æ³•ï¼Œå¸¦é‡è¯•æœºåˆ¶
        
        Args:
            query: CypheræŸ¥è¯¢è¯­å¥
            parameters: æŸ¥è¯¢å‚æ•°
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        if parameters is None:
            parameters = {}
            
        async with self._driver.session(database=self.database) as session:
            return await session.run(query, **parameters)

    def _generate_unique_id(self, prefix: str, content: str) -> str:
        """
        ç”Ÿæˆå”¯ä¸€ID
        
        Args:
            prefix: IDå‰ç¼€ (å¦‚ "chunk_", "event_", "entity_")
            content: ç”¨äºç”Ÿæˆhashçš„å†…å®¹
            
        Returns:
            å”¯ä¸€IDå­—ç¬¦ä¸²
        """
        hash_value = hashlib.md5(content.encode('utf-8')).hexdigest()[:8]
        return f"{prefix}{hash_value}"

    async def filter_existing_chunks(self, documents: List[Document]) -> List[Document]:
        """
        è¿‡æ»¤å·²å­˜åœ¨çš„chunkï¼Œè¿”å›æœªå¤„ç†è¿‡çš„chunk
        
        Args:
            documents: å¾…æ£€æŸ¥çš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            æœªåœ¨Neo4jä¸­å­˜åœ¨çš„æ–‡æ¡£åˆ—è¡¨
        """
        logger.info(f"ğŸ” æ­£åœ¨æ£€æŸ¥ {len(documents)} ä¸ªchunkæ˜¯å¦å·²å­˜åœ¨...")
        
        # ä¸ºæ–‡æ¡£ç”Ÿæˆchunk ID
        chunk_ids = []
        doc_to_chunk_id = {}

        # å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰chunk_idï¼Œåˆ™ç”Ÿæˆchunk_id
        for doc in documents:
            if "chunk_id" not in doc.metadata:
                chunk_content = doc.content.strip()
                chunk_id = self._generate_unique_id("chunk_", chunk_content)
                doc.metadata["chunk_id"] = chunk_id
                chunk_ids.append(chunk_id)
            else:
                chunk_id = doc.metadata["chunk_id"]
            doc_to_chunk_id[chunk_id] = doc
        
        # æŸ¥è¯¢Neo4jä¸­å·²å­˜åœ¨çš„chunk ID
        existing_chunks = set()
        if chunk_ids:
            query = """
            MATCH (c:Chunk)
            WHERE c.id_ IN $chunk_ids
            RETURN c.id_ as chunk_id
            """
            async with self._driver.session(database=self.database) as session:
                result = await session.run(query, chunk_ids=chunk_ids)
                async for record in result:
                    existing_chunks.add(record["chunk_id"])
        
        # ç­›é€‰å‡ºä¸å­˜åœ¨çš„æ–‡æ¡£
        new_documents = []
        for chunk_id, doc in doc_to_chunk_id.items():
            if chunk_id not in existing_chunks:
                new_documents.append(doc)
            else:
                logger.info(f"  âš ï¸ è·³è¿‡å·²å­˜åœ¨çš„chunk: {chunk_id}")
        
        logger.info(f"  âœ… å‘ç° {len(new_documents)} ä¸ªæ–°chunkï¼Œå·²è·³è¿‡ {len(existing_chunks)} ä¸ªé‡å¤chunk")
        return new_documents


    async def _generate_embeddings(self):
        """è‡ªåŠ¨ä¸ºæ²¡æœ‰embeddingçš„èŠ‚ç‚¹ç”ŸæˆåµŒå…¥å‘é‡"""
        if not self.embedding:
            logger.error("âš ï¸ æœªæä¾›åµŒå…¥æ¨¡å‹ï¼Œè·³è¿‡å‘é‡ç”Ÿæˆ")
            return
            
        logger.info("ğŸ§  æ­£åœ¨è‡ªåŠ¨ç”Ÿæˆç¼ºå¤±çš„åµŒå…¥å‘é‡...")
        
        # å…ˆè·å–æ€»æ•°ç”¨äºè¿›åº¦æ˜¾ç¤º
        async def get_total_count(node_type, condition="embedding IS NULL"):
            count_query = f"MATCH (n:{node_type}) WHERE n.{condition} RETURN count(n) as total"
            async with self._driver.session(database=self.database) as session:
                result = await session.run(count_query)
                record = await result.single()
                return record["total"] if record else 0
        
        # å¤„ç†Chunks
        total_chunks = await get_total_count("Chunk")
        if total_chunks > 0:
            logger.info(f"  ğŸ“Š å‘ç° {total_chunks} ä¸ªchunkéœ€è¦ç”ŸæˆåµŒå…¥å‘é‡")
            await self._process_chunk_embeddings(total_chunks)
        else:
            logger.info("  âœ… æ‰€æœ‰chunkå·²æœ‰åµŒå…¥å‘é‡")
        
        # å¤„ç†Entities  
        total_entities = await get_total_count("Entity")
        if total_entities > 0:
            logger.info(f"  ğŸ“Š å‘ç° {total_entities} ä¸ªå®ä½“éœ€è¦ç”ŸæˆåµŒå…¥å‘é‡")
            await self._process_entity_embeddings(total_entities)
        else:
            logger.info("  âœ… æ‰€æœ‰å®ä½“å·²æœ‰åµŒå…¥å‘é‡")
        
        # å¤„ç†Events
        total_events = await get_total_count("Event")
        if total_events > 0:
            logger.info(f"  ğŸ“Š å‘ç° {total_events} ä¸ªäº‹ä»¶éœ€è¦ç”ŸæˆåµŒå…¥å‘é‡")
            await self._process_event_embeddings(total_events)
        else:
            logger.info("  âœ… æ‰€æœ‰äº‹ä»¶å·²æœ‰åµŒå…¥å‘é‡")

    async def _process_chunk_embeddings(self, total_count):
        """å¤„ç†chunkåµŒå…¥å‘é‡ç”Ÿæˆ"""
        batch_size = 100
        processed = 0
        
        while processed < total_count:
            # æ¯æ¬¡é‡æ–°æŸ¥è¯¢ç¡®ä¿è·å–æœ€æ–°çš„æœªå¤„ç†æ•°æ®
            query = """
            MATCH (c:Chunk)
            WHERE c.embedding IS NULL
            RETURN c.id_ as id_, c.content as content
            LIMIT $limit
            """
            
            async with self._driver.session(database=self.database) as session:
                result = await session.run(query, {"limit": batch_size})
                records = await result.data()  # ä½¿ç”¨data()æ–¹æ³•è·å–æ‰€æœ‰è®°å½•
                
                if not records:
                    break  # æ²¡æœ‰æ›´å¤šéœ€è¦å¤„ç†çš„æ•°æ®
                
                chunks_to_embed = []
                chunk_texts = []
                
                for record in records:
                    chunk_id = record["id_"]
                    content = record["content"] or ""
                    
                    if content.strip():  # è·³è¿‡ç©ºå†…å®¹
                        chunk_texts.append(content)
                        chunks_to_embed.append(chunk_id)
                
                if chunks_to_embed:
                    logger.info(f"    ğŸ§  å¤„ç†chunk {processed + 1}-{processed + len(chunks_to_embed)}/{total_count}")
                    embeddings = self.embedding.embed_documents(chunk_texts)
                    
                    # æ‰¹é‡æ›´æ–°
                    update_query = """
                    UNWIND $updates as update
                    MATCH (c:Chunk {id_: update.id_})
                    SET c.embedding = update.embedding
                    """
                    
                    updates = [
                        {"id_": chunk_id, "embedding": embedding}
                        for chunk_id, embedding in zip(chunks_to_embed, embeddings)
                    ]
                    
                    await self._execute_query(update_query, {"updates": updates})
                    processed += len(chunks_to_embed)
                else:
                    # å¦‚æœè¿™æ‰¹è®°å½•éƒ½æ˜¯ç©ºå†…å®¹ï¼Œæ ‡è®°ä¸ºå·²å¤„ç†ä»¥é¿å…æ— é™å¾ªç¯
                    empty_updates = []
                    for record in records:
                        chunk_id = record["id_"]
                        content = record["content"] or ""
                        if not content.strip():
                            empty_updates.append(chunk_id)
                    
                    if empty_updates:
                        # ä¸ºç©ºå†…å®¹çš„chunkè®¾ç½®ç©ºçš„embeddingæˆ–æ ‡è®°
                        empty_query = """
                        UNWIND $ids as id_
                        MATCH (c:Chunk {id_: id_})
                        SET c.embedding = []
                        """
                        await self._execute_query(empty_query, {"ids": empty_updates})
                        processed += len(empty_updates)

    async def _process_entity_embeddings(self, total_count):
        """å¤„ç†å®ä½“åµŒå…¥å‘é‡ç”Ÿæˆ"""
        batch_size = 100
        processed = 0
        
        while processed < total_count:
            query = """
            MATCH (e:Entity)
            WHERE e.embedding IS NULL
            RETURN e.id_ as id_, e.entity_name as name, e.entity_descriptions as descriptions
            LIMIT $limit
            """
            
            async with self._driver.session(database=self.database) as session:
                result = await session.run(query, {"limit": batch_size})
                records = await result.data()
                
                if not records:
                    break
                
                entities_to_embed = []
                entity_texts = []
                
                for record in records:
                    entity_id = record["id_"]
                    entity_name = record["name"] or ""
                    descriptions = record["descriptions"] or []
                    
                    text = f"{entity_name}: {' '.join(descriptions)}"
                    entity_texts.append(text)
                    entities_to_embed.append(entity_id)
                
                if entities_to_embed:
                    logger.info(f"    ğŸ§  å¤„ç†å®ä½“ {processed + 1}-{processed + len(entities_to_embed)}/{total_count}")
                    embeddings = self.embedding.embed_documents(entity_texts)
                    
                    update_query = """
                    UNWIND $updates as update
                    MATCH (e:Entity {id_: update.id_})
                    SET e.embedding = update.embedding
                    """
                    
                    updates = [
                        {"id_": entity_id, "embedding": embedding}
                        for entity_id, embedding in zip(entities_to_embed, embeddings)
                    ]
                    
                    await self._execute_query(update_query, {"updates": updates})
                    processed += len(entities_to_embed)

    async def _process_event_embeddings(self, total_count):
        """å¤„ç†äº‹ä»¶åµŒå…¥å‘é‡ç”Ÿæˆ"""
        batch_size = 100
        processed = 0
        
        while processed < total_count:
            query = """
            MATCH (e:Event)
            WHERE e.embedding IS NULL
            RETURN e.id_ as id_, e.content as content
            LIMIT $limit
            """
            
            async with self._driver.session(database=self.database) as session:
                result = await session.run(query, {"limit": batch_size})
                records = await result.data()
                
                if not records:
                    break
                
                events_to_embed = []
                event_texts = []
                
                for record in records:
                    event_id = record["id_"]
                    content = record["content"] or ""
                    
                    if content.strip():  # è·³è¿‡ç©ºå†…å®¹
                        event_texts.append(content)
                        events_to_embed.append(event_id)
                
                if events_to_embed:
                    logger.info(f"    ğŸ§  å¤„ç†äº‹ä»¶ {processed + 1}-{processed + len(events_to_embed)}/{total_count}")
                    embeddings = self.embedding.embed_documents(event_texts)
                    
                    update_query = """
                    UNWIND $updates as update
                    MATCH (e:Event {id_: update.id_})
                    SET e.embedding = update.embedding
                    """
                    
                    updates = [
                        {"id_": event_id, "embedding": embedding}
                        for event_id, embedding in zip(events_to_embed, embeddings)
                    ]
                    
                    await self._execute_query(update_query, {"updates": updates})
                    processed += len(events_to_embed)
                else:
                    # å¤„ç†ç©ºå†…å®¹çš„äº‹ä»¶
                    empty_updates = []
                    for record in records:
                        event_id = record["id_"]
                        content = record["content"] or ""
                        if not content.strip():
                            empty_updates.append(event_id)
                    
                    if empty_updates:
                        empty_query = """
                        UNWIND $ids as id_
                        MATCH (e:Event {id_: id_})
                        SET e.embedding = []
                        """
                        await self._execute_query(empty_query, {"ids": empty_updates})
                        processed += len(empty_updates)

    async def _merge_duplicate_entities(self):
        """ä½¿ç”¨Louvainç®—æ³•åŸºäºå®ä½“åç§°ç›¸ä¼¼åº¦è¿›è¡Œç¤¾åŒºæ£€æµ‹å’Œåˆå¹¶"""
        logger.info("ğŸ”„ æ­£åœ¨ä½¿ç”¨Louvainç®—æ³•è¿›è¡Œå®ä½“èšç±»åˆå¹¶...")
        
        try:
            # 1. æ£€æŸ¥GDSåº“æ˜¯å¦å¯ç”¨
            if not await self._check_gds_availability():
                logger.warning("  âš ï¸ GDSåº“ä¸å¯ç”¨ï¼Œå›é€€åˆ°åŸºç¡€åˆå¹¶æ–¹å¼")
                await self._fallback_name_based_merge()
                return
            
            # 2. åˆ›å»ºåŸºäºå®ä½“åç§°ç›¸ä¼¼åº¦çš„å›¾æŠ•å½±
            graph_name, index_to_node_id = await self._create_similarity_graph()
            if not graph_name:
                logger.warning("  âš ï¸ å›¾æŠ•å½±åˆ›å»ºå¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€åˆå¹¶æ–¹å¼")
                await self._fallback_name_based_merge()
                return
            
            # 3. ä½¿ç”¨Louvainç®—æ³•æ£€æµ‹ç¤¾åŒº
            clusters = await self._detect_entity_clusters(graph_name, index_to_node_id)
            if not clusters:
                logger.warning("  âš ï¸ èšç±»æ£€æµ‹å¤±è´¥æˆ–æ— èšç±»ç»“æœï¼Œå›é€€åˆ°åŸºç¡€åˆå¹¶æ–¹å¼")
                await self._cleanup_resources(graph_name)
                await self._fallback_name_based_merge()
                return
            
            # 4. åˆå¹¶åŒç¤¾åŒºå®ä½“
            merged_count = await self._merge_clusters(clusters)
            
            # 5. æ¸…ç†èµ„æº
            await self._cleanup_resources(graph_name)
            
            logger.info(f"  âœ… Louvainèšç±»åˆå¹¶å®Œæˆï¼Œå…±åˆå¹¶ {merged_count} ä¸ªé‡å¤å®ä½“")
            
        except Exception as e:
            logger.error(f"  âš ï¸ Louvainèšç±»åˆå¹¶å¤±è´¥: {e}")
            # æ·»åŠ å¼‚å¸¸è¯¦ç»†ä¿¡æ¯
            import traceback
            logger.error(f"  è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            
            # æ¸…ç†å¯èƒ½æ®‹ç•™çš„èµ„æº
            try:
                await self._cleanup_resources("entity_similarity_graph")
                await self._cleanup_similarity_relationships()
            except:
                pass
            
            # å›é€€åˆ°åŸºç¡€åˆå¹¶æ–¹å¼
            await self._fallback_name_based_merge()

    async def _check_gds_availability(self) -> bool:
        """æ£€æŸ¥Graph Data Scienceåº“æ˜¯å¦å¯ç”¨"""
        try:
            # é¦–å…ˆå°è¯•ç›´æ¥æ£€æŸ¥GDSç‰ˆæœ¬
            check_query = "RETURN gds.version() as version"
            async with self._driver.session(database=self.database) as session:
                result = await session.run(check_query)
                record = await result.single()
                if record:
                    version = record['version']
                    logger.info(f"  âœ… GDSåº“å¯ç”¨ï¼Œç‰ˆæœ¬: {version}")
                    
                    # å°è¯•æ£€æŸ¥Neo4jç‰ˆæœ¬å…¼å®¹æ€§
                    try:
                        # å°è¯•ä¸åŒçš„ç³»ç»Ÿè¿‡ç¨‹åç§°
                        neo4j_version_queries = [
                            "CALL dbms.components() YIELD versions, name WHERE name = 'Neo4j Kernel' RETURN versions[0] as version",
                            "CALL dbms.components() YIELD versions, name WHERE name = 'Neo4j Kernel' RETURN versions as version",
                            "RETURN '5.0.0' as version"  # é»˜è®¤ç‰ˆæœ¬
                        ]
                        
                        neo4j_version = None
                        for query in neo4j_version_queries:
                            try:
                                neo4j_result = await session.run(query)
                                neo4j_record = await neo4j_result.single()
                                if neo4j_record:
                                    neo4j_version = neo4j_record['version']
                                    if isinstance(neo4j_version, list):
                                        neo4j_version = neo4j_version[0]
                                    break
                            except:
                                continue
                        
                        if neo4j_version:
                            logger.info(f"  â„¹ï¸ Neo4jç‰ˆæœ¬: {neo4j_version}")
                            
                            # æ£€æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§
                            if not self._check_version_compatibility(neo4j_version, version):
                                logger.warning(f"  âš ï¸ Neo4jç‰ˆæœ¬ {neo4j_version} ä¸GDSç‰ˆæœ¬ {version} å¯èƒ½ä¸å…¼å®¹")
                                return False
                    except Exception as e:
                        logger.warning(f"  âš ï¸ Neo4jç‰ˆæœ¬æ£€æŸ¥å¤±è´¥: {e}")
                    
                    return True
                else:
                    logger.warning("  âš ï¸ GDSåº“æœªå®‰è£…æˆ–ä¸å¯ç”¨")
                    return False
                    
        except Exception as e:
            logger.warning(f"  âš ï¸ GDSåº“æ£€æŸ¥å¤±è´¥: {e}")
        
        return False
    
    def _check_version_compatibility(self, neo4j_version: str, gds_version: str) -> bool:
        """æ£€æŸ¥Neo4jå’ŒGDSç‰ˆæœ¬å…¼å®¹æ€§"""
        try:
            # æå–ä¸»ç‰ˆæœ¬å·
            neo4j_major = int(neo4j_version.split('.')[0])
            gds_major = int(gds_version.split('.')[0])
            
            # åŸºæœ¬å…¼å®¹æ€§æ£€æŸ¥
            if neo4j_major >= 5 and gds_major >= 2:
                return True
            elif neo4j_major >= 4 and gds_major >= 1:
                return True
            
            return False
        except:
            # å¦‚æœç‰ˆæœ¬è§£æå¤±è´¥ï¼Œä¿å®ˆåœ°è¿”å›False
            return False

    async def _create_similarity_graph(self) -> str:
        """åˆ›å»ºåŸºäºå®ä½“åç§°ç›¸ä¼¼åº¦çš„å›¾æŠ•å½±"""
        logger.info("  ğŸ”§ åˆ›å»ºå®ä½“ç›¸ä¼¼åº¦å›¾...")
        
        graph_name = "entity_similarity_graph"
        
        # æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§å›¾
        await self._cleanup_resources(graph_name)
        
        # è·å–æ‰€æœ‰å®ä½“çš„embedding
        entities_query = """
        MATCH (e:Entity)
        WHERE e.embedding IS NOT NULL
        RETURN elementId(e) as node_id,
               e.entity_name as name,
               e.embedding as embedding
        """
        
        # åˆ›å»ºèŠ‚ç‚¹IDåˆ°ç´¢å¼•çš„æ˜ å°„
        node_id_to_index = {}
        index_to_node_id = {}
        
        async with self._driver.session(database=self.database) as session:
            result = await session.run(entities_query)
            entities = await result.data()
        
        if len(entities) < 2:
            return None
        
        # è®¡ç®—å®ä½“é—´çš„embeddingä½™å¼¦ç›¸ä¼¼åº¦å¹¶åˆ›å»ºSIMILARå…³ç³»
        similarity_threshold = 0.95  # ç›¸ä¼¼åº¦é˜ˆå€¼
        relationships_created = 0
        
        # å¯¼å…¥å‘é‡è®¡ç®—åº“
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np
        
        # å‡†å¤‡embeddingæ•°æ®
        embeddings = []
        entity_map = {}
        
        for i, entity in enumerate(entities):
            embedding = entity['embedding']
            if embedding:
                embeddings.append(embedding)
                entity_map[i] = entity['node_id']
                node_id_to_index[entity['node_id']] = i
                index_to_node_id[i] = entity['node_id']
        
        if len(embeddings) < 2:
            logger.info("  â„¹ï¸ æœ‰æ•ˆembeddingæ•°é‡ä¸è¶³ï¼Œè·³è¿‡ç›¸ä¼¼åº¦è®¡ç®—")
            return None
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
        try:
            embeddings_array = np.array(embeddings)
            similarity_matrix = cosine_similarity(embeddings_array)
            
            # åˆ›å»ºç›¸ä¼¼åº¦å…³ç³»
            async with self._driver.session(database=self.database) as session:
                for i in range(len(embeddings)):
                    for j in range(i + 1, len(embeddings)):
                        similarity_score = similarity_matrix[i][j]
                        
                        if similarity_score >= similarity_threshold:
                            create_relation_query = """
                            MATCH (e1:Entity), (e2:Entity)
                            WHERE elementId(e1) = $node_id1 AND elementId(e2) = $node_id2
                            CREATE (e1)-[:SIMILAR {similarity: $similarity}]->(e2)
                            """
                            
                            await session.run(create_relation_query, {
                                'node_id1': entity_map[i],
                                'node_id2': entity_map[j],
                                'similarity': float(similarity_score)
                            })
                            relationships_created += 1
                            
                            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                            if relationships_created <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªå…³ç³»
                                logger.info(f"  ğŸ”— åˆ›å»ºç›¸ä¼¼åº¦å…³ç³»: {entities[i]['name']} -> {entities[j]['name']} (ç›¸ä¼¼åº¦: {similarity_score:.3f})")
            
            logger.info(f"  âœ… åˆ›å»ºäº† {relationships_created} ä¸ªç›¸ä¼¼åº¦å…³ç³»")
            
        except Exception as e:
            logger.warning(f"  âš ï¸ embeddingç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            relationships_created = 0
        
        # å¦‚æœæ²¡æœ‰åˆ›å»ºä»»ä½•å…³ç³»ï¼Œè¯´æ˜æ²¡æœ‰ç›¸ä¼¼å®ä½“
        if relationships_created == 0:
            logger.info("  â„¹ï¸ æ²¡æœ‰å‘ç°ç›¸ä¼¼å®ä½“ï¼Œè·³è¿‡å›¾æŠ•å½±åˆ›å»º")
            return None
        
        # åˆ›å»ºå›¾æŠ•å½± - ä½¿ç”¨æ—§ç‰ˆæœ¬è¯­æ³•
        try:
            create_projection_query = f"""
            CALL gds.graph.project(
                '{graph_name}',
                'Entity',
                'SIMILAR'
            )
            YIELD graphName, nodeCount, relationshipCount
            RETURN graphName, nodeCount, relationshipCount
            """
            
            async with self._driver.session(database=self.database) as session:
                result = await session.run(create_projection_query)
                records = await result.data()
                if records:
                    logger.info(f"  âœ… æ—§ç‰ˆæœ¬å›¾æŠ•å½±åˆ›å»ºæˆåŠŸ: {records[0]['nodeCount']} ä¸ªèŠ‚ç‚¹, {records[0]['relationshipCount']} ä¸ªå…³ç³»")
                    return graph_name, index_to_node_id
                    
        except Exception as e:
            logger.error(f"  âŒ å›¾æŠ•å½±åˆ›å»ºå¤±è´¥: {e}")
            # æ¸…ç†å·²åˆ›å»ºçš„å…³ç³»
            await self._cleanup_similarity_relationships()
            return None, {}
        
        return None, {}

    async def _cleanup_similarity_relationships(self):
        """æ¸…ç†SIMILARå…³ç³»"""
        try:
            cleanup_query = "MATCH ()-[r:SIMILAR]->() DELETE r"
            async with self._driver.session(database=self.database) as session:
                await session.run(cleanup_query)
        except Exception as e:
            logger.warning(f"  âš ï¸ æ¸…ç†SIMILARå…³ç³»å¤±è´¥: {e}")

    async def _detect_entity_clusters(self, graph_name: str, index_to_node_id: dict = None) -> dict:
        """ä½¿ç”¨Louvainç®—æ³•æ£€æµ‹å®ä½“èšç±»"""
        if not graph_name:
            logger.info("  â„¹ï¸ æ²¡æœ‰å›¾æŠ•å½±ï¼Œè·³è¿‡èšç±»æ£€æµ‹")
            return {}
            
        # ä½¿ç”¨Louvainç®—æ³•æ£€æµ‹å®ä½“èšç±»
        
        try:
            louvain_query = f"""
            CALL gds.louvain.stream(
                '{graph_name}',
                {{
                    maxIterations: 10
                }}
            )
            YIELD nodeId, communityId
            WITH communityId, collect(nodeId) as nodeIds
            WHERE size(nodeIds) > 1
            RETURN communityId, nodeIds
            ORDER BY size(nodeIds) DESC
            """
            
            async with self._driver.session(database=self.database) as session:
                result = await session.run(louvain_query)
                louvain_records = await result.data()
                
        except Exception as e:
            logger.error(f"  âŒ Louvainç®—æ³•å¤±è´¥: {e}")
            return {}
        
        # è·å–èšç±»è¯¦æƒ…
        clusters = {}
        for record in louvain_records:
            community_id = record['communityId']
            node_ids = record['nodeIds']
            
            # å°†å›¾æŠ•å½±çš„ç´¢å¼•IDè½¬æ¢ä¸ºå®é™…çš„èŠ‚ç‚¹ID
            actual_node_ids = []
            if index_to_node_id:
                for node_id in node_ids:
                    if node_id in index_to_node_id:
                        actual_node_ids.append(index_to_node_id[node_id])
            else:
                # å¦‚æœæ²¡æœ‰æ˜ å°„ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ID
                actual_node_ids = [str(nid) for nid in node_ids]
            
            if not actual_node_ids:
                continue
            
            # è·å–å®ä½“è¯¦ç»†ä¿¡æ¯
            entities_query = """
            MATCH (e:Entity)
            WHERE elementId(e) IN $node_ids
            RETURN elementId(e) as node_id,
                   e.entity_name as name,
                   coalesce(e.entity_descriptions, []) as descriptions,
                   coalesce(e.mention_texts, []) as mentions,
                   coalesce(e.source_chunks, []) as sources
            """
            
            async with self._driver.session(database=self.database) as session:
                entities_result = await session.run(entities_query, {'node_ids': actual_node_ids})
                entities_records = await entities_result.data()
                entities = [dict(record) for record in entities_records]
            
            clusters[community_id] = entities
        
        logger.info(f"  âœ… æ£€æµ‹åˆ° {len(clusters)} ä¸ªéœ€è¦åˆå¹¶çš„èšç±»")
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        for cluster_id, entities in clusters.items():
            entity_names = [e.get('name', '') for e in entities]
            logger.info(f"  ğŸ” èšç±» {cluster_id}: {entity_names}")
        
        return clusters

    async def _merge_clusters(self, clusters: dict) -> int:
        """åˆå¹¶åŒç¤¾åŒºçš„å®ä½“"""
        if not clusters:
            logger.info("  â„¹ï¸ æ²¡æœ‰éœ€è¦åˆå¹¶çš„èšç±»")
            return 0
            
        logger.info("  ğŸ”„ å¼€å§‹åˆå¹¶å®ä½“èšç±»...")
        
        total_merged = 0
        
        for cluster_id, entities in clusters.items():
            if len(entities) < 2:
                continue
            
            # é€‰æ‹©ä¿¡æ¯æœ€ä¸°å¯Œçš„å®ä½“ä½œä¸ºä¸»å®ä½“
            primary_entity = max(entities, key=lambda e: (
                len(e.get('descriptions', [])) +
                len(e.get('mentions', [])) +
                len(e.get('sources', []))
            ))
            
            other_entities = [e for e in entities if e['node_id'] != primary_entity['node_id']]
            
            # æ”¶é›†æ‰€æœ‰å±æ€§
            all_descriptions = set(primary_entity.get('descriptions', []))
            all_mentions = set(primary_entity.get('mentions', []))
            all_sources = set(primary_entity.get('sources', []))
            
            for entity in other_entities:
                all_descriptions.update(entity.get('descriptions', []))
                all_mentions.update(entity.get('mentions', []))
                all_sources.update(entity.get('sources', []))
            
            # æ‰§è¡Œåˆå¹¶ï¼Œä½¿ç”¨elementId()æ›¿ä»£id()ï¼Œå¹¶è¿ç§»å…³ç³»
            other_node_ids = [entity['node_id'] for entity in other_entities]
            
            merge_query = """
            // æ‰¾åˆ°ä¸»å®ä½“å’Œå…¶ä»–å®ä½“
            MATCH (primary:Entity) WHERE elementId(primary) = $primary_id
            MATCH (other:Entity) WHERE elementId(other) IN $other_ids
            
            // è¿ç§»å…¶ä»–å®ä½“çš„æ‰€æœ‰å…³ç³»åˆ°ä¸»å®ä½“
            WITH primary, collect(other) as others
            UNWIND others as other
            
            // å¤„ç†å‡ºå‘å…³ç³»
            OPTIONAL MATCH (other)-[r]->(target)
            WHERE NOT target:Entity OR elementId(target) <> elementId(primary)
            WITH primary, other, collect({rel: r, target: target}) as outRels
            
            // å¤„ç†å…¥å‘å…³ç³»  
            OPTIONAL MATCH (source)-[r]->(other)
            WHERE NOT source:Entity OR elementId(source) <> elementId(primary)
            WITH primary, other, outRels, collect({rel: r, source: source}) as inRels
            
            // åˆ›å»ºå‡ºå‘å…³ç³»
            UNWIND outRels as outRel
            WITH primary, other, inRels, outRel
            WHERE outRel.rel IS NOT NULL
            CALL apoc.create.relationship(
                primary, 
                type(outRel.rel), 
                properties(outRel.rel), 
                outRel.target
            ) YIELD rel as newOutRel
            
            // åˆ›å»ºå…¥å‘å…³ç³»
            WITH primary, other, inRels
            UNWIND inRels as inRel
            WITH primary, other, inRel
            WHERE inRel.rel IS NOT NULL
            CALL apoc.create.relationship(
                inRel.source, 
                type(inRel.rel), 
                properties(inRel.rel), 
                primary
            ) YIELD rel as newInRel
            
            // æ›´æ–°ä¸»å®ä½“å±æ€§
            WITH primary, other
            SET primary.entity_descriptions = $descriptions,
                primary.mention_texts = $mentions,
                primary.source_chunks = $sources,
                primary.update_time = datetime()
            
            // åˆ é™¤å…¶ä»–å®ä½“
            DETACH DELETE other
            
            RETURN 1 as merged_count
            """
            
            # ç”±äºä¸Šè¿°æŸ¥è¯¢è¾ƒå¤æ‚ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
            simplified_merge_query = """
            // 1. æ‰¾åˆ°ä¸»å®ä½“å’Œå…¶ä»–å®ä½“
            MATCH (primary:Entity) WHERE elementId(primary) = $primary_id
            WITH primary
            MATCH (other:Entity) WHERE elementId(other) IN $other_ids
            
            // 2. è¿ç§»å…¶ä»–å®ä½“çš„å‡ºå‘å…³ç³»
            OPTIONAL MATCH (other)-[r]->(target)
            WHERE NOT (primary)-[]->(target) OR NOT target:Entity
            WITH primary, other, r, target
            WHERE r IS NOT NULL AND target IS NOT NULL
            CREATE (primary)-[newR]->(target)
            SET newR = properties(r)
            WITH primary, other, count(r) as out_count
            
            // 3. è¿ç§»å…¶ä»–å®ä½“çš„å…¥å‘å…³ç³»  
            OPTIONAL MATCH (source)-[r]->(other)
            WHERE NOT (source)-[]->(primary) OR NOT source:Entity
            WITH primary, other, out_count, r, source
            WHERE r IS NOT NULL AND source IS NOT NULL
            CREATE (source)-[newR]->(primary)
            SET newR = properties(r)
            WITH primary, other, out_count, count(r) as in_count
            
            // 4. æ›´æ–°ä¸»å®ä½“å±æ€§å¹¶åˆ é™¤å…¶ä»–å®ä½“
            SET primary.entity_descriptions = $descriptions,
                primary.mention_texts = $mentions,
                primary.source_chunks = $sources,
                primary.update_time = datetime()
            
            DETACH DELETE other
            
            RETURN 1 as merged_count
            """
            
            async with self._driver.session(database=self.database) as session:
                try:
                    result = await session.run(merge_query, {
                        'primary_id': primary_entity['node_id'],
                        'other_ids': other_node_ids,
                        'descriptions': list(all_descriptions),
                        'mentions': list(all_mentions),
                        'sources': list(all_sources)
                    })
                    merge_records = await result.data()
                    merged_count = len(merge_records)
                    total_merged += merged_count
                    
                    entity_names = [e.get('name', '') for e in entities]
                    logger.info(f"  ğŸ”„ åˆå¹¶èšç±» {cluster_id} ({len(entities)}ä¸ª): {entity_names} -> åˆå¹¶äº† {merged_count} ä¸ªå®ä½“")
                    
                except Exception as e:
                    logger.error(f"  âŒ åˆå¹¶èšç±» {cluster_id} å¤±è´¥: {e}")
                    # å›é€€åˆ°ç®€å•åˆå¹¶æ–¹å¼
                    simple_merge_query = """
                    // æ›´æ–°ä¸»å®ä½“
                    MATCH (primary:Entity) WHERE elementId(primary) = $primary_id
                    SET primary.entity_descriptions = $descriptions,
                        primary.mention_texts = $mentions,
                        primary.source_chunks = $sources,
                        primary.update_time = datetime()
                    
                    // åˆ é™¤å…¶ä»–å®ä½“
                    WITH primary
                    MATCH (other:Entity) WHERE elementId(other) IN $other_ids
                    DETACH DELETE other
                    
                    RETURN size($other_ids) as merged_count
                    """
                    
                    result = await session.run(simple_merge_query, {
                        'primary_id': primary_entity['node_id'],
                        'other_ids': other_node_ids,
                        'descriptions': list(all_descriptions),
                        'mentions': list(all_mentions),
                        'sources': list(all_sources)
                    })
                    merge_records = await result.data()
                    merged_count = merge_records[0]['merged_count'] if merge_records else 0
                    total_merged += merged_count
                    
                    entity_names = [e.get('name', '') for e in entities]
                    logger.info(f"  ğŸ”„ ç®€å•åˆå¹¶èšç±» {cluster_id} ({len(entities)}ä¸ª): {entity_names} -> åˆå¹¶äº† {merged_count} ä¸ªå®ä½“")
        
        return total_merged

    async def _cleanup_resources(self, graph_name: str):
        """æ¸…ç†ä¸´æ—¶èµ„æº"""
        try:
            # åˆ é™¤GDSå›¾æŠ•å½±
            if graph_name:
                drop_query = f"CALL gds.graph.drop('{graph_name}', false)"
                async with self._driver.session(database=self.database) as session:
                    try:
                        result = await session.run(drop_query)
                        await result.consume()
                    except Exception as e:
                        # å¿½ç•¥å›¾ä¸å­˜åœ¨çš„é”™è¯¯
                        if "Graph with name" not in str(e) and "does not exist" not in str(e):
                            raise e
            
            # åˆ é™¤ä¸´æ—¶ç›¸ä¼¼åº¦å…³ç³»
            cleanup_query = """
            MATCH ()-[r:SIMILAR]-()
            DELETE r
            """
            
            async with self._driver.session(database=self.database) as session:
                result = await session.run(cleanup_query)
                await result.consume()
            
            logger.info(f"  ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"  âš ï¸ èµ„æºæ¸…ç†å¤±è´¥: {e}")

    async def _fallback_name_based_merge(self):
        """å›é€€åˆ°åŸºç¡€çš„åç§°åŒ¹é…åˆå¹¶æ–¹å¼"""
        logger.info("  ğŸ”„ ä½¿ç”¨åŸºç¡€åç§°åŒ¹é…è¿›è¡Œå®ä½“åˆå¹¶...")
        
        merge_query = """
        MATCH (e1:Entity), (e2:Entity) 
        WHERE toLower(e1.entity_name) = toLower(e2.entity_name)
        AND elementId(e1) > elementId(e2)
        WITH e1, e2, 
             coalesce(e1.entity_descriptions, []) + coalesce(e2.entity_descriptions, []) as merged_desc,
             coalesce(e1.mention_texts, []) + coalesce(e2.mention_texts, []) as merged_mentions,
             coalesce(e1.source_chunks, []) + coalesce(e2.source_chunks, []) as merged_sources
        
        SET e2.entity_descriptions = merged_desc,
            e2.mention_texts = merged_mentions,
            e2.source_chunks = merged_sources,
            e2.update_time = datetime()
        
        DETACH DELETE e1
        
        RETURN count(e1) as merged_count
        """
        
        async with self._driver.session(database=self.database) as session:
            result = await session.run(merge_query)
            records = await result.data()
        
        merged_count = records[0]['merged_count'] if records else 0
        logger.info(f"  âœ… åŸºç¡€åˆå¹¶å®Œæˆï¼Œåˆå¹¶äº† {merged_count} ä¸ªé‡å¤å®ä½“")

    async def get_graph_statistics(self) -> Dict[str, int]:
        """è·å–å›¾ç»Ÿè®¡ä¿¡æ¯"""
        queries = self._get_statistics_queries()
        
        statistics = {}
        for stat_name, query in queries.items():
            try:
                async with self._driver.session(database=self.database) as session:
                    result = await session.run(query)
                    records = await result.data()
                    if records:
                        statistics[stat_name] = records[0]["count"]
                    else:
                        statistics[stat_name] = 0
            except Exception as e:
                logger.error(f"âš ï¸ è·å–ç»Ÿè®¡ä¿¡æ¯ {stat_name} æ—¶å‡ºé”™: {e}")
                statistics[stat_name] = 0
        
        return statistics

    async def delete_graph_data(self, delete_type: str = "all"):
        """
        åˆ é™¤å›¾æ•°æ®
        
        Args:
            delete_type: åˆ é™¤ç±»å‹ ("all", "entities", "events", "relations")
        """
        logger.info(f"ğŸ—‘ï¸ æ­£åœ¨åˆ é™¤å›¾æ•°æ®: {delete_type}")
        
        delete_queries = self._get_delete_queries()
        
        if delete_type not in delete_queries:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ é™¤ç±»å‹: {delete_type}")
        
        queries = delete_queries[delete_type]
        for query in queries:
            try:
                await self._execute_query(query)
                logger.info(f"  âœ“ æ‰§è¡Œåˆ é™¤æŸ¥è¯¢: {query}")
            except Exception as e:
                logger.error(f"  âŒ åˆ é™¤æŸ¥è¯¢å¤±è´¥: {e}")

    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æµ‹è¯•æ•°æ®åº“è¿æ¥
            async with self._driver.session(database=self.database) as session:
                result = await session.run("RETURN 1 as test")
                records = await result.data()
                if not records or records[0]["test"] != 1:
                    raise Exception("æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥")
            
            # è·å–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            stats = await self.get_graph_statistics()
            
            return {
                "status": "healthy",
                "database": self.database,
                "statistics": stats,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    # =============================================================================
    # æŠ½è±¡æ–¹æ³• - å­ç±»å¿…é¡»å®ç°
    # =============================================================================
    
    @abstractmethod
    async def store_graph(self, documents: List[Document]) -> bool:
        """
        å­˜å‚¨å›¾ç»“æ„åˆ°Neo4jï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            bool: å­˜å‚¨æ˜¯å¦æˆåŠŸ
        """
        pass

    @abstractmethod
    async def _create_constraints_and_indexes(self):
        """åˆ›å»ºæ•°æ®åº“çº¦æŸå’Œç´¢å¼•ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰"""
        pass

    @abstractmethod
    def _get_statistics_queries(self) -> Dict[str, str]:
        """è·å–ç»Ÿè®¡æŸ¥è¯¢è¯­å¥ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰"""
        pass

    @abstractmethod
    def _get_delete_queries(self) -> Dict[str, List[str]]:
        """è·å–åˆ é™¤æŸ¥è¯¢è¯­å¥ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰"""
        pass
