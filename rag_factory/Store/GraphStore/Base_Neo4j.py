"""
Neo4jå›¾å­˜å‚¨åŸºç±»
æä¾›Neo4jå›¾æ•°æ®åº“çš„é€šç”¨æ“ä½œåŠŸèƒ½
"""

import asyncio
import hashlib
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from datetime import datetime
import neo4j

from rag_factory.Embed import Embeddings
from rag_factory.documents.schema import Document

from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)

neo4j_retry_errors = (
    neo4j.exceptions.ServiceUnavailable,
    neo4j.exceptions.TransientError,
    neo4j.exceptions.WriteServiceUnavailable,
    neo4j.exceptions.ClientError,
)


class GraphStoreBaseNeo4j(ABC):
    """
    Neo4jå›¾å­˜å‚¨åŸºç±»
    æä¾›Neo4jæ•°æ®åº“è¿æ¥ã€æŸ¥è¯¢æ‰§è¡Œã€çº¦æŸç®¡ç†ç­‰é€šç”¨åŠŸèƒ½
    """
    
    def __init__(self, url: str, username: str, password: str, database: str, embedding: Optional[Embeddings] = None):
        """
        åˆå§‹åŒ–Neo4jå›¾å­˜å‚¨åŸºç±»
        
        Args:
            url: Neo4jæ•°æ®åº“URL
            username: ç”¨æˆ·å
            password: å¯†ç 
            database: æ•°æ®åº“å
            embedding: åµŒå…¥æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        """
        self._driver = None
        self._driver_lock = asyncio.Lock()
        self.database = database
        self.embedding = embedding
        
        try:
            self._driver: neo4j.AsyncDriver = neo4j.AsyncGraphDatabase.driver(
                url, auth=(username, password)
            )
            print(f"âœ… æˆåŠŸè¿æ¥åˆ°Neo4jæ•°æ®åº“: {url}")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–Neo4jè¿æ¥å¤±è´¥: {e}")
            raise

    async def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self._driver:
            await self._driver.close()
            self._driver = None

    async def __aexit__(self, exc_type, exc, tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡ºæ–¹æ³•"""
        if self._driver:
            await self._driver.close()

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10), retry=retry_if_exception_type(neo4j_retry_errors))
    async def _execute_query(self, query: str, parameters: Dict[str, Any] = None):
        """
        æ‰§è¡ŒNeo4jæŸ¥è¯¢çš„é€šç”¨æ–¹æ³•ï¼Œå¸¦é‡è¯•æœºåˆ¶
        
        Args:
            query: CypheræŸ¥è¯¢è¯­å¥
            parameters: æŸ¥è¯¢å‚æ•°
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        if parameters is None:
            parameters = {}
            
        async with self._driver.session(database=self.database) as session:
            return await session.run(query, **parameters)

    def _generate_unique_id(self, prefix: str, content: str) -> str:
        """
        ç”Ÿæˆå”¯ä¸€ID
        
        Args:
            prefix: IDå‰ç¼€ (å¦‚ "chunk_", "event_", "entity_")
            content: ç”¨äºç”Ÿæˆhashçš„å†…å®¹
            
        Returns:
            å”¯ä¸€IDå­—ç¬¦ä¸²
        """
        hash_value = hashlib.md5(content.encode('utf-8')).hexdigest()[:8]
        return f"{prefix}{hash_value}"

    async def filter_existing_chunks(self, documents: List[Document]) -> List[Document]:
        """
        è¿‡æ»¤å·²å­˜åœ¨çš„chunkï¼Œè¿”å›æœªå¤„ç†è¿‡çš„chunk
        
        Args:
            documents: å¾…æ£€æŸ¥çš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            æœªåœ¨Neo4jä¸­å­˜åœ¨çš„æ–‡æ¡£åˆ—è¡¨
        """
        print(f"ğŸ” æ­£åœ¨æ£€æŸ¥ {len(documents)} ä¸ªchunkæ˜¯å¦å·²å­˜åœ¨...")
        
        # ä¸ºæ–‡æ¡£ç”Ÿæˆchunk ID
        chunk_ids = []
        doc_to_chunk_id = {}
        
        for doc in documents:
            # æ ¹æ®å†…å®¹ç”Ÿæˆå”¯ä¸€çš„chunk ID
            chunk_content = doc.content.strip()
            chunk_id = self._generate_unique_id("chunk_", chunk_content)
            chunk_ids.append(chunk_id)
            doc_to_chunk_id[chunk_id] = doc
            # å°†chunk_idä¿å­˜åˆ°æ–‡æ¡£metadataä¸­
            doc.metadata["chunk_id"] = chunk_id
        
        # æŸ¥è¯¢Neo4jä¸­å·²å­˜åœ¨çš„chunk ID
        existing_chunks = set()
        if chunk_ids:
            query = """
            MATCH (c:Chunk)
            WHERE c.id_ IN $chunk_ids
            RETURN c.id_ as chunk_id
            """
            async with self._driver.session(database=self.database) as session:
                result = await session.run(query, chunk_ids=chunk_ids)
                async for record in result:
                    existing_chunks.add(record["chunk_id"])
        
        # ç­›é€‰å‡ºä¸å­˜åœ¨çš„æ–‡æ¡£
        new_documents = []
        for chunk_id, doc in doc_to_chunk_id.items():
            if chunk_id not in existing_chunks:
                new_documents.append(doc)
            else:
                print(f"  âš ï¸ è·³è¿‡å·²å­˜åœ¨çš„chunk: {chunk_id}")
        
        print(f"  âœ… å‘ç° {len(new_documents)} ä¸ªæ–°chunkï¼Œå·²è·³è¿‡ {len(existing_chunks)} ä¸ªé‡å¤chunk")
        return new_documents

    async def _generate_embeddings(self):
        """è‡ªåŠ¨ä¸ºæ²¡æœ‰embeddingçš„èŠ‚ç‚¹ç”ŸæˆåµŒå…¥å‘é‡"""
        if not self.embedding:
            print("âš ï¸ æœªæä¾›åµŒå…¥æ¨¡å‹ï¼Œè·³è¿‡å‘é‡ç”Ÿæˆ")
            return
            
        print("ğŸ§  æ­£åœ¨è‡ªåŠ¨ç”Ÿæˆç¼ºå¤±çš„åµŒå…¥å‘é‡...")
        
        # ä¸ºchunkç”ŸæˆåµŒå…¥
        chunk_query = """
        MATCH (c:Chunk)
        WHERE c.embedding IS NULL
        RETURN c.id_ as id_, c.content as content
        LIMIT 100
        """
        
        async with self._driver.session(database=self.database) as session:
            result = await session.run(chunk_query)
            chunks_to_embed = []
            chunk_texts = []
            
            async for record in result:
                chunk_id = record["id_"]
                content = record["content"] or ""
                
                chunk_texts.append(content)
                chunks_to_embed.append(chunk_id)
            
            if chunk_texts:
                print(f"  ğŸ§  ä¸º {len(chunk_texts)} ä¸ªchunkç”ŸæˆåµŒå…¥å‘é‡...")
                embeddings = self.embedding.embed_documents(chunk_texts)
                
                for chunk_id, embedding in zip(chunks_to_embed, embeddings):
                    update_query = """
                    MATCH (c:Chunk {id_: $id_})
                    SET c.embedding = $embedding
                    """
                    await self._execute_query(update_query, {"id_": chunk_id, "embedding": embedding})
                
                print(f"  âœ… å®Œæˆ {len(embeddings)} ä¸ªchunkåµŒå…¥å‘é‡ç”Ÿæˆ")
        
        # ä¸ºå®ä½“ç”ŸæˆåµŒå…¥
        entity_query = """
        MATCH (e:Entity)
        WHERE e.embedding IS NULL
        RETURN e.id_ as id_, e.entity_name as name, e.entity_descriptions as descriptions
        LIMIT 100
        """
        
        async with self._driver.session(database=self.database) as session:
            result = await session.run(entity_query)
            entities_to_embed = []
            entity_texts = []
            
            async for record in result:
                entity_id = record["id_"]
                entity_name = record["name"]
                descriptions = record["descriptions"] or []
                
                # æ„å»ºç”¨äºåµŒå…¥çš„æ–‡æœ¬
                text = f"{entity_name}: {' '.join(descriptions)}"
                entity_texts.append(text)
                entities_to_embed.append(entity_id)
            
            if entity_texts:
                print(f"  ğŸ§  ä¸º {len(entity_texts)} ä¸ªå®ä½“ç”ŸæˆåµŒå…¥å‘é‡...")
                embeddings = self.embedding.embed_documents(entity_texts)
                
                for entity_id, embedding in zip(entities_to_embed, embeddings):
                    update_query = """
                    MATCH (e:Entity {id_: $id_})
                    SET e.embedding = $embedding
                    """
                    await self._execute_query(update_query, {"id_": entity_id, "embedding": embedding})
                
                print(f"  âœ… å®Œæˆ {len(embeddings)} ä¸ªå®ä½“åµŒå…¥å‘é‡ç”Ÿæˆ")
        
        # ä¸ºäº‹ä»¶ç”ŸæˆåµŒå…¥
        event_query = """
        MATCH (e:Event)
        WHERE e.embedding IS NULL
        RETURN e.id_ as id_, e.content as content
        LIMIT 100
        """
        
        async with self._driver.session(database=self.database) as session:
            result = await session.run(event_query)
            events_to_embed = []
            event_texts = []
            
            async for record in result:
                event_id = record["id_"]
                content = record["content"] or ""
                
                event_texts.append(content)
                events_to_embed.append(event_id)
            
            if event_texts:
                print(f"  ğŸ§  ä¸º {len(event_texts)} ä¸ªäº‹ä»¶ç”ŸæˆåµŒå…¥å‘é‡...")
                embeddings = self.embedding.embed_documents(event_texts)
                
                for event_id, embedding in zip(events_to_embed, embeddings):
                    update_query = """
                    MATCH (e:Event {id_: $id_})
                    SET e.embedding = $embedding
                    """
                    await self._execute_query(update_query, {"id_": event_id, "embedding": embedding})
                
                print(f"  âœ… å®Œæˆ {len(embeddings)} ä¸ªäº‹ä»¶åµŒå…¥å‘é‡ç”Ÿæˆ")

    async def _merge_duplicate_entities(self):
        """ä½¿ç”¨APOCåˆå¹¶å¯èƒ½é‡å¤çš„å®ä½“èŠ‚ç‚¹ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰"""
        print("ğŸ”„ æ­£åœ¨ä½¿ç”¨APOCåˆå¹¶é‡å¤å®ä½“...")
        
        try:
            # æ£€æŸ¥APOCæ˜¯å¦å¯ç”¨
            apoc_check_query = "RETURN apoc.version() as version"
            await self._execute_query(apoc_check_query)
            print("  âœ… APOCæ’ä»¶å¯ç”¨ï¼Œå¼€å§‹åˆå¹¶é‡å¤å®ä½“")
            
            # æŸ¥æ‰¾åŒåå®ä½“å¹¶åˆå¹¶
            merge_query = """
            CALL apoc.periodic.iterate(
                "MATCH (e1:Entity), (e2:Entity) 
                 WHERE e1.entity_name = e2.entity_name AND id(e1) > id(e2) 
                 RETURN e1, e2",
                "CALL apoc.refactor.mergeNodes([e1, e2], {
                    properties: {
                        entity_descriptions: 'combine',
                        mention_texts: 'combine',
                        source_chunks: 'combine',
                        update_time: 'overwrite'
                    }
                }) YIELD node RETURN node",
                {batchSize: 10, parallel: false}
            )
            """
            
            await self._execute_query(merge_query)
            print("  âœ… å®Œæˆå®ä½“åˆå¹¶")
            
        except Exception as e:
            print(f"  âš ï¸ APOCåˆå¹¶åŠŸèƒ½ä¸å¯ç”¨æˆ–å¤±è´¥: {e}")
            print("  ğŸ’¡ å»ºè®®å®‰è£…APOCæ’ä»¶ä»¥è·å¾—æ›´å¥½çš„å®ä½“åˆå¹¶åŠŸèƒ½")

    async def get_graph_statistics(self) -> Dict[str, int]:
        """è·å–å›¾ç»Ÿè®¡ä¿¡æ¯"""
        queries = self._get_statistics_queries()
        
        statistics = {}
        for stat_name, query in queries.items():
            try:
                async with self._driver.session(database=self.database) as session:
                    result = await session.run(query)
                    records = await result.data()
                    if records:
                        statistics[stat_name] = records[0]["count"]
                    else:
                        statistics[stat_name] = 0
            except Exception as e:
                print(f"âš ï¸ è·å–ç»Ÿè®¡ä¿¡æ¯ {stat_name} æ—¶å‡ºé”™: {e}")
                statistics[stat_name] = 0
        
        return statistics

    async def delete_graph_data(self, delete_type: str = "all"):
        """
        åˆ é™¤å›¾æ•°æ®
        
        Args:
            delete_type: åˆ é™¤ç±»å‹ ("all", "entities", "events", "relations")
        """
        print(f"ğŸ—‘ï¸ æ­£åœ¨åˆ é™¤å›¾æ•°æ®: {delete_type}")
        
        delete_queries = self._get_delete_queries()
        
        if delete_type not in delete_queries:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ é™¤ç±»å‹: {delete_type}")
        
        queries = delete_queries[delete_type]
        for query in queries:
            try:
                await self._execute_query(query)
                print(f"  âœ“ æ‰§è¡Œåˆ é™¤æŸ¥è¯¢: {query}")
            except Exception as e:
                print(f"  âŒ åˆ é™¤æŸ¥è¯¢å¤±è´¥: {e}")

    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æµ‹è¯•æ•°æ®åº“è¿æ¥
            async with self._driver.session(database=self.database) as session:
                result = await session.run("RETURN 1 as test")
                records = await result.data()
                if not records or records[0]["test"] != 1:
                    raise Exception("æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥")
            
            # è·å–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            stats = await self.get_graph_statistics()
            
            return {
                "status": "healthy",
                "database": self.database,
                "statistics": stats,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    # =============================================================================
    # æŠ½è±¡æ–¹æ³• - å­ç±»å¿…é¡»å®ç°
    # =============================================================================
    
    @abstractmethod
    async def store_graph(self, documents: List[Document]) -> bool:
        """
        å­˜å‚¨å›¾ç»“æ„åˆ°Neo4jï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            bool: å­˜å‚¨æ˜¯å¦æˆåŠŸ
        """
        pass

    @abstractmethod
    async def _create_constraints_and_indexes(self):
        """åˆ›å»ºæ•°æ®åº“çº¦æŸå’Œç´¢å¼•ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰"""
        pass

    @abstractmethod
    def _get_statistics_queries(self) -> Dict[str, str]:
        """è·å–ç»Ÿè®¡æŸ¥è¯¢è¯­å¥ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰"""
        pass

    @abstractmethod
    def _get_delete_queries(self) -> Dict[str, List[str]]:
        """è·å–åˆ é™¤æŸ¥è¯¢è¯­å¥ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰"""
        pass
