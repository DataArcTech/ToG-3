from typing import List, Dict, Any
from datetime import datetime

from rag_factory.Store.GraphStore.Base_Neo4j import GraphStoreBaseNeo4j
from rag_factory.Embed import Embeddings
from rag_factory.documents.schema import Document
from rag_factory.documents.pydantic_schema import PydanticUtils

# TODO: æ‰¹é‡å†™å…¥ï¼Œè¾¹æå–è¾¹å†™å…¥...
class HyperRAGNeo4jStore(GraphStoreBaseNeo4j):
    """
    HyperRAGä¸“ç”¨çš„Neo4jå­˜å‚¨ç±»
    ä¸“é—¨ç”¨äºçŸ¥è¯†å›¾è°±çš„æ„å»ºï¼Œå¤„ç†eventsã€mentionsã€entity_relationsç­‰æ•°æ®
    æ”¯æŒchunkå»é‡ã€å®ä½“åˆå¹¶ã€è‡ªåŠ¨å‘é‡åŒ–ç­‰åŠŸèƒ½
    """
    
    def __init__(self, url: str, username: str, password: str, database: str, embedding: Embeddings):
        """
        åˆå§‹åŒ–HyperRAG Neo4jå­˜å‚¨
        
        Args:
            url: Neo4jæ•°æ®åº“URL
            username: ç”¨æˆ·å
            password: å¯†ç 
            database: æ•°æ®åº“å
            embedding: åµŒå…¥æ¨¡å‹
        """
        super().__init__(url, username, password, database, embedding)
    
    async def store_graph(self, documents: list[Document]) -> bool:
        return await self.store_hyperrag_graph(documents)
    
    async def store_hyperrag_graph(self, documents: list[Document]) -> bool:
        """
        å­˜å‚¨HyperRAGå›¾ç»“æ„åˆ°Neo4j
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨(é€šè¿‡GraphExtractoræå–çš„æ–‡æ¡£,åŒ…å«å›¾ç»“æ„)
            
        Returns:
            bool: å­˜å‚¨æ˜¯å¦æˆåŠŸ
        """
        try:
            print("ğŸš€ å¼€å§‹å­˜å‚¨HyperRAGå›¾ç»“æ„åˆ°Neo4j...")
            
            if not documents:
                print("âš ï¸ æ²¡æœ‰æ–‡æ¡£éœ€è¦å¤„ç†")
                return True
            
            # è¿‡æ»¤é‡å¤æ–‡æ¡£
            filtered_documents = await self._filter_duplicate_documents(documents)
            print(f"ğŸ“Š è¿‡æ»¤é‡å¤æ–‡æ¡£: åŸå§‹ {len(documents)} ä¸ªï¼Œè¿‡æ»¤å {len(filtered_documents)} ä¸ª")
            
            if not filtered_documents:
                print("âš ï¸ æ‰€æœ‰æ–‡æ¡£éƒ½å·²å­˜åœ¨ï¼Œæ— éœ€é‡å¤å­˜å‚¨")
                return True
            
            # åˆ›å»ºçº¦æŸå’Œç´¢å¼•
            await self._create_constraints_and_indexes()

            # æå–æ‰€æœ‰æ•°æ®
            all_chunks = []
            all_mentions = []
            all_events = []
            all_entity_relations = []
            all_event_relations = []
            
            for document in filtered_documents:
                # ç”Ÿæˆchunk ID
                chunk_id = document.metadata.get("chunk_id")
                if not chunk_id:
                    chunk_id = self._generate_unique_id("chunk_", document.content)
                    document.metadata["chunk_id"] = chunk_id
                
                # åˆ›å»ºchunk
                chunk = {
                    "id_": chunk_id,
                    "content": document.content,
                    "source": document.metadata.get("source", "unknown"),
                    "create_time": datetime.now().isoformat()
                }
                all_chunks.append(chunk)
                
                # æå–mentionsï¼ˆå®ä½“ï¼‰- ç»Ÿä¸€è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                mentions = document.metadata.get("mentions", [])
                for mention in mentions:
                    # ç»Ÿä¸€è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                    mention_dict = PydanticUtils.to_dict(mention)
                    
                    # ä¸ºæ¯ä¸ªmentionç”Ÿæˆå”¯ä¸€ID
                    entity_name = mention_dict.get('entity_name', '')
                    text = mention_dict.get('text', '')
                    mention_id = self._generate_unique_id("mention_", f"{entity_name}-{text}")
                    
                    # æ·»åŠ å­˜å‚¨æ‰€éœ€å­—æ®µ
                    mention_dict.update({
                        "id_": mention_id,
                        "source_chunk": chunk_id,
                    })
                    all_mentions.append(mention_dict)
                
                # æå–events - ç»Ÿä¸€è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                events = document.metadata.get("events", [])
                for event in events:
                    # ç»Ÿä¸€è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                    event_dict = PydanticUtils.to_dict(event)
                    
                    # ä¸ºæ¯ä¸ªeventç”Ÿæˆå”¯ä¸€ID
                    content = event_dict.get("content", "")
                    event_id = self._generate_unique_id("event_", content)
                    
                    # æ·»åŠ å­˜å‚¨æ‰€éœ€å­—æ®µ
                    event_dict.update({
                        "id_": event_id,
                        "source_chunk": chunk_id,
                    })
                    all_events.append(event_dict)
                
                # æå–å®ä½“å…³ç³» - ç»Ÿä¸€è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                entity_relations = document.metadata.get("entity_relations", [])
                for relation in entity_relations:
                    relation_dict = PydanticUtils.to_dict(relation)
                    relation_dict["source_chunk"] = chunk_id
                    all_entity_relations.append(relation_dict)
                
                # æå–äº‹ä»¶å…³ç³» - ç»Ÿä¸€è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                event_relations = document.metadata.get("event_relations", [])
                for relation in event_relations:
                    relation_dict = PydanticUtils.to_dict(relation)
                    relation_dict["source_chunk"] = chunk_id
                    all_event_relations.append(relation_dict)
            
            # å­˜å‚¨æ•°æ®
            print(f"ğŸ“Š å‡†å¤‡å­˜å‚¨æ•°æ®: {len(all_chunks)} chunks, {len(all_mentions)} mentions, "
                  f"{len(all_events)} events, {len(all_entity_relations)} entity relations, "
                  f"{len(all_event_relations)} event relations")
            
            # 1. å­˜å‚¨chunks
            if all_chunks:
                await self._store_chunks(all_chunks)
            
            # 2. å­˜å‚¨å®ä½“mentionsï¼ˆå¯èƒ½éœ€è¦åˆå¹¶ï¼‰
            if all_mentions:
                await self._store_mentions(all_mentions)
            
            # 3. å­˜å‚¨äº‹ä»¶
            if all_events:
                await self._store_events(all_events)
            
            # 4. å­˜å‚¨å®ä½“å…³ç³»
            if all_entity_relations:
                await self._store_entity_relations(all_entity_relations)
            
            # 5. å­˜å‚¨äº‹ä»¶å…³ç³»
            if all_event_relations:
                await self._store_event_relations(all_event_relations)
            
            # 6. åˆ›å»ºchunk-äº‹ä»¶å…³ç³»
            await self._create_chunk_event_relations(all_chunks, all_events)
            
            # 7. åˆ›å»ºchunk-å®ä½“å…³ç³»
            await self._create_chunk_entity_relations(all_chunks, all_mentions)
            
            # 8. åˆ›å»ºäº‹ä»¶-å®ä½“å‚ä¸å…³ç³»
            await self._create_event_mention_relations(all_events, all_mentions)
            
            # 9. è‡ªåŠ¨ç”ŸæˆåµŒå…¥å‘é‡
            await self._generate_embeddings()
            
            # 9. å¯é€‰ï¼šä½¿ç”¨APOCåˆå¹¶é‡å¤èŠ‚ç‚¹
            await self._merge_duplicate_entities()

            # 10. äº‹ä»¶æ¶ˆæ­§å’ŒåŒä¹‰äº‹ä»¶æŒ–æ˜
            await self._disambiguate_events_with_gds()


            print("âœ… HyperRAGå›¾ç»“æ„å­˜å‚¨å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ å­˜å‚¨HyperRAGå›¾ç»“æ„æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return False

    async def _filter_duplicate_documents(self, documents: list[Document]) -> list[Document]:
        """
        è¿‡æ»¤é‡å¤çš„æ–‡æ¡£
        
        Args:
            documents: åŸå§‹æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            è¿‡æ»¤åçš„æ–‡æ¡£åˆ—è¡¨
        """
        print("ğŸ” æ­£åœ¨æ£€æŸ¥é‡å¤æ–‡æ¡£...")
        
        filtered_documents = []
        
        for document in documents:
            # ç”Ÿæˆæˆ–è·å–chunk ID
            chunk_id = document.metadata.get("chunk_id")
            if not chunk_id:
                chunk_id = self._generate_unique_id("chunk_", document.content)
                document.metadata["chunk_id"] = chunk_id
            
            # æ£€æŸ¥chunkæ˜¯å¦å·²å­˜åœ¨
            check_query = """
            MATCH (c:Chunk {id_: $chunk_id})
            RETURN c.id_ as id
            """
            
            try:
                result = await self._execute_query(check_query, {"chunk_id": chunk_id})
                records = await result.data()
                
                if not records:
                    # chunkä¸å­˜åœ¨ï¼Œæ·»åŠ åˆ°è¿‡æ»¤åçš„åˆ—è¡¨
                    filtered_documents.append(document)
                else:
                    print(f"  â­ï¸ è·³è¿‡é‡å¤chunk: {chunk_id}")
                    
            except Exception as e:
                print(f"  âš ï¸ æ£€æŸ¥chunk {chunk_id} æ—¶å‡ºé”™: {e}")
                # å‡ºé”™æ—¶ä¿å®ˆå¤„ç†ï¼Œä¿ç•™æ–‡æ¡£
                filtered_documents.append(document)
        
        return filtered_documents

    async def _create_constraints_and_indexes(self):
        """åˆ›å»ºæ•°æ®åº“çº¦æŸå’Œç´¢å¼•"""
        print("ğŸ“‹ åˆ›å»ºæ•°æ®åº“çº¦æŸå’Œç´¢å¼•...")
        
        constraints_and_indexes = [
            # Chunkçº¦æŸå’Œç´¢å¼•
            "CREATE CONSTRAINT chunk_id_unique IF NOT EXISTS FOR (c:Chunk) REQUIRE c.id_ IS UNIQUE",
            "CREATE INDEX chunk_source_index IF NOT EXISTS FOR (c:Chunk) ON (c.source)",
            
            # Entity(Mention)çº¦æŸå’Œç´¢å¼•
            "CREATE CONSTRAINT entity_id_unique IF NOT EXISTS FOR (e:Entity) REQUIRE e.id_ IS UNIQUE",
            "CREATE INDEX entity_name_index IF NOT EXISTS FOR (e:Entity) ON (e.entity_name)",
            "CREATE INDEX entity_type_index IF NOT EXISTS FOR (e:Entity) ON (e.entity_type)",
            
            # äº‹ä»¶çº¦æŸå’Œç´¢å¼•
            "CREATE CONSTRAINT event_id_unique IF NOT EXISTS FOR (e:Event) REQUIRE e.id_ IS UNIQUE",
            "CREATE INDEX event_type_index IF NOT EXISTS FOR (e:Event) ON (e.type)",
            "CREATE INDEX event_source_index IF NOT EXISTS FOR (e:Event) ON (e.source_chunk)",
            
            # äº‹ä»¶é›†ç¾¤ç´¢å¼•
            "CREATE INDEX event_cluster_index IF NOT EXISTS FOR (e:Event) ON (e.cluster_id)",
            
            # å‘é‡ç´¢å¼•ï¼ˆå¦‚æœæ”¯æŒï¼‰
            "CREATE VECTOR INDEX entity_embedding_index IF NOT EXISTS FOR (e:Entity) ON (e.embedding) OPTIONS {indexConfig: {`vector.dimensions`: 768, `vector.similarity_function`: 'cosine'}}",
            "CREATE VECTOR INDEX event_embedding_index IF NOT EXISTS FOR (e:Event) ON (e.embedding) OPTIONS {indexConfig: {`vector.dimensions`: 768, `vector.similarity_function`: 'cosine'}}"
        ]
        
        for statement in constraints_and_indexes:
            try:
                await self._execute_query(statement)
            except Exception as e:
                # å¦‚æœçº¦æŸå·²å­˜åœ¨æˆ–ä¸æ”¯æŒå‘é‡ç´¢å¼•ï¼Œè·³è¿‡
                if "already exists" in str(e) or "Unknown procedure" in str(e) or "Unsupported" in str(e):
                    continue
                print(f"âš ï¸ åˆ›å»ºçº¦æŸ/ç´¢å¼•æ—¶è­¦å‘Š: {e}")
    
    async def _store_chunks(self, chunks: List[Dict[str, Any]]):
        """å­˜å‚¨ChunkèŠ‚ç‚¹"""
        print(f"ğŸ“„ æ­£åœ¨å­˜å‚¨ {len(chunks)} ä¸ªChunk...")
        
        for chunk in chunks:
            query = """
            MERGE (c:Chunk {id_: $id_})
            SET c.content = $content,
                c.source = $source,
                c.create_time = $create_time,
                c.update_time = datetime()
            RETURN c
            """
            
            await self._execute_query(query, {
                "id_": chunk["id_"],
                "content": chunk["content"],
                "source": chunk["source"],
                "create_time": chunk["create_time"]
            })
            print(f"  âœ“ å­˜å‚¨Chunk: {chunk['id_']} (æ¥æº: {chunk['source']})")
    
    async def _store_mentions(self, mentions: List[Dict[str, Any]]):
        """å­˜å‚¨å®ä½“mentionsï¼Œæ”¯æŒåŒåå®ä½“çš„å±æ€§åˆå¹¶"""
        print(f"ğŸ·ï¸ æ­£åœ¨å­˜å‚¨ {len(mentions)} ä¸ªå®ä½“mentions...")
        
        for mention in mentions:
            # ä½¿ç”¨ç»Ÿä¸€çš„å±æ€§è·å–æ–¹æ³•
            entity_name = mention.get("entity_name", "")
            entity_type = mention.get("entity_type", "")
            entity_description = mention.get("entity_description", "")
            text = mention.get("text", "")
            source_chunk = mention.get("source_chunk", "")
            
            query = """
            MERGE (e:Entity {entity_name: $entity_name})
            ON CREATE SET 
                e.id_ = $id_,
                e.entity_type = $entity_type,
                e.entity_descriptions = [$entity_description],
                e.mention_texts = [$text],
                e.source_chunks = [$source_chunk],
                e.create_time = datetime(),
                e.update_time = datetime()
            ON MATCH SET
                e.entity_descriptions = CASE 
                    WHEN $entity_description IN e.entity_descriptions THEN e.entity_descriptions 
                    ELSE e.entity_descriptions + [$entity_description] 
                END,
                e.mention_texts = CASE 
                    WHEN $text IN e.mention_texts THEN e.mention_texts 
                    ELSE e.mention_texts + [$text] 
                END,
                e.source_chunks = CASE 
                    WHEN $source_chunk IN e.source_chunks THEN e.source_chunks 
                    ELSE e.source_chunks + [$source_chunk] 
                END,
                e.update_time = datetime()
            RETURN e
            """
            
            await self._execute_query(query, {
                "id_": mention["id_"],
                "entity_name": entity_name,
                "entity_type": entity_type,
                "entity_description": entity_description,
                "text": text,
                "source_chunk": source_chunk
            })
            print(f"  âœ“ å­˜å‚¨å®ä½“: {entity_name} ({entity_type})")
    
    async def _store_events(self, events: List[Dict[str, Any]]):
        """å­˜å‚¨äº‹ä»¶èŠ‚ç‚¹"""
        print(f"ğŸ“… æ­£åœ¨å­˜å‚¨ {len(events)} ä¸ªäº‹ä»¶...")
        
        for event in events:
            query = """
            MERGE (e:Event {id_: $id_})
            SET e.content = $content,
                e.type = $type,
                e.participants = $participants,
                e.source_chunk = $source_chunk,
                e.create_time = datetime(),
                e.update_time = datetime()
            RETURN e
            """
            
            await self._execute_query(query, {
                "id_": event["id_"],
                "content": event.get("content", ""),
                "type": event.get("type", ""),
                "participants": event.get("participants", []),
                "source_chunk": event.get("source_chunk", "")
            })
            print(f"  âœ“ å­˜å‚¨äº‹ä»¶: {event.get('content', '')[:50]}...")
    
    async def _store_entity_relations(self, relations: List[Dict[str, Any]]):
        """å­˜å‚¨å®ä½“å…³ç³»"""
        print(f"ğŸ”— æ­£åœ¨å­˜å‚¨ {len(relations)} ä¸ªå®ä½“å…³ç³»...")
        
        for relation in relations:
            head_entity = relation.get("head_entity", "")
            tail_entity = relation.get("tail_entity", "")
            relation_type = relation.get("relation_type", "")
            description = relation.get("description", "")
            source_chunk = relation.get("source_chunk", "")
            
            query = """
            MATCH (head:Entity {entity_name: $head_entity})
            MATCH (tail:Entity {entity_name: $tail_entity})
            MERGE (head)-[r:ENTITY_RELATION {type: $relation_type}]->(tail)
            SET r.description = $description,
                r.source_chunk = $source_chunk,
                r.create_time = datetime()
            RETURN r
            """
            
            await self._execute_query(query, {
                "head_entity": head_entity,
                "tail_entity": tail_entity,
                "relation_type": relation_type,
                "description": description,
                "source_chunk": source_chunk
            })
            print(f"  âœ“ å­˜å‚¨å®ä½“å…³ç³»: {head_entity} --[{relation_type}]--> {tail_entity}")
    
    async def _store_event_relations(self, relations: List[Dict[str, Any]]):
        """å­˜å‚¨äº‹ä»¶å…³ç³»"""
        print(f"ğŸ”„ æ­£åœ¨å­˜å‚¨ {len(relations)} ä¸ªäº‹ä»¶å…³ç³»...")
        
        for relation in relations:
            # æ”¯æŒä¸¤ç§å¼•ç”¨æ–¹å¼ï¼šäº‹ä»¶å†…å®¹æˆ–äº‹ä»¶ID
            head_event_content = relation.get("head_event_content")
            tail_event_content = relation.get("tail_event_content")
            
            relation_type = relation.get("relation_type", "")
            description = relation.get("description", "")
            source_chunk = relation.get("source_chunk", "")
            
            query = """
            MATCH (head:Event {content: $head_event_content})
            MATCH (tail:Event {content: $tail_event_content})
            MERGE (head)-[r:EVENT_RELATION {type: $relation_type}]->(tail)
            SET r.description = $description,
                r.source_chunk = $source_chunk,
                r.create_time = datetime()
            RETURN head.id_ as head_id, tail.id_ as tail_id
            """
            
            result = await self._execute_query(query, {
                "head_event_content": head_event_content,
                "tail_event_content": tail_event_content,
                "relation_type": relation_type,
                "description": description,
                "source_chunk": source_chunk
            })
            
            # å®‰å…¨å¤„ç†å¯èƒ½ä¸º None çš„å†…å®¹
            if head_event_content:
                print_head = head_event_content[:30] + "..." if len(head_event_content) > 30 else head_event_content
            else:
                print_head = "None"
                
            if tail_event_content:
                print_tail = tail_event_content[:30] + "..." if len(tail_event_content) > 30 else tail_event_content
            else:
                print_tail = "None"
            
            rel_type_emoji = {
                "æ—¶åºå…³ç³»": "â°",
                "å› æœå…³ç³»": "ğŸ”—",
                "å±‚çº§å…³ç³»": "ğŸ“Š",
                "æ¡ä»¶å…³ç³»": "ğŸ”„"
            }.get(relation_type, "ğŸ“")
            
            print(f"  âœ“ {rel_type_emoji} å­˜å‚¨äº‹ä»¶å…³ç³»: {print_head} --[{relation_type}]--> {print_tail}")
    
    async def _create_chunk_event_relations(self, chunks: List[Dict[str, Any]], events: List[Dict[str, Any]]):
        """åˆ›å»ºchunk-äº‹ä»¶å…³ç³»"""
        print("ğŸ“„ æ­£åœ¨åˆ›å»ºchunk-äº‹ä»¶å…³ç³»...")
        
        # åˆ›å»ºchunk_idåˆ°eventsçš„æ˜ å°„
        chunk_to_events = {}
        for event in events:
            source_chunk = event.get("source_chunk", "")
            if source_chunk:
                if source_chunk not in chunk_to_events:
                    chunk_to_events[source_chunk] = []
                chunk_to_events[source_chunk].append(event["id_"])
        
        for chunk in chunks:
            chunk_id = chunk["id_"]
            if chunk_id in chunk_to_events:
                for event_id in chunk_to_events[chunk_id]:
                    query = """
                    MATCH (chunk:Chunk {id_: $chunk_id})
                    MATCH (event:Event {id_: $event_id})
                    MERGE (chunk)-[:CONTAINS]->(event)
                    """
                    
                    await self._execute_query(query, {
                        "chunk_id": chunk_id,
                        "event_id": event_id
                    })
                    print(f"  âœ“ Chunk {chunk_id} åŒ…å«äº‹ä»¶ {event_id}")
    
    async def _create_chunk_entity_relations(self, chunks: List[Dict[str, Any]], mentions: List[Dict[str, Any]]):
        """åˆ›å»ºchunk-å®ä½“å…³ç³»"""
        print("ğŸ·ï¸ æ­£åœ¨åˆ›å»ºchunk-å®ä½“å…³ç³»...")
        
        # åˆ›å»ºchunk_idåˆ°mentionsçš„æ˜ å°„
        chunk_to_entities = {}
        for mention in mentions:
            source_chunk = mention.get("source_chunk", "")
            entity_name = mention.get("entity_name", "")
            if source_chunk and entity_name:
                if source_chunk not in chunk_to_entities:
                    chunk_to_entities[source_chunk] = set()
                chunk_to_entities[source_chunk].add(entity_name)
        
        for chunk in chunks:
            chunk_id = chunk["id_"]
            if chunk_id in chunk_to_entities:
                for entity_name in chunk_to_entities[chunk_id]:
                    query = """
                    MATCH (chunk:Chunk {id_: $chunk_id})
                    MATCH (entity:Entity {entity_name: $entity_name})
                    MERGE (chunk)-[:MENTIONS]->(entity)
                    """
                    
                    await self._execute_query(query, {
                        "chunk_id": chunk_id,
                        "entity_name": entity_name
                    })
                    print(f"  âœ“ Chunk {chunk_id} æåŠå®ä½“ {entity_name}")
    
    async def _create_event_mention_relations(self, events: List[Dict[str, Any]], mentions: List[Dict[str, Any]]):
        """åˆ›å»ºäº‹ä»¶-å®ä½“å‚ä¸å…³ç³»"""
        print("ğŸ‘¥ æ­£åœ¨åˆ›å»ºäº‹ä»¶-å®ä½“å‚ä¸å…³ç³»...")
        
        for event in events:
            participants = event.get("participants", [])
            event_id = event["id_"]
            
            for participant in participants:
                # æŸ¥æ‰¾å¯¹åº”çš„å®ä½“
                query = """
                MATCH (entity:Entity {entity_name: $participant})
                MATCH (event:Event {id_: $event_id})
                MERGE (entity)-[:PARTICIPATES_IN {role: "participant"}]->(event)
                """
                
                await self._execute_query(query, {
                    "participant": participant,
                    "event_id": event_id
                })
                print(f"  âœ“ ğŸ‘¤ {participant} å‚ä¸äº‹ä»¶ {event_id}")
    
    def _get_statistics_queries(self) -> Dict[str, str]:
        """è·å–ç»Ÿè®¡æŸ¥è¯¢è¯­å¥"""
        return {
            "chunks": "MATCH (c:Chunk) RETURN count(c) as count",
            "entities": "MATCH (e:Entity) RETURN count(e) as count",
            "events": "MATCH (e:Event) RETURN count(e) as count",
            "entity_relations": "MATCH ()-[r:ENTITY_RELATION]->() RETURN count(r) as count",
            "event_relations": "MATCH ()-[r:EVENT_RELATION]->() RETURN count(r) as count",
            "similar_events": "MATCH ()-[r:SIMILAR_TO]->() RETURN count(r) as count",
            "participations": "MATCH ()-[r:PARTICIPATES_IN]->() RETURN count(r) as count",
            "contains_events": "MATCH ()-[r:CONTAINS]->() RETURN count(r) as count",
            "mentions": "MATCH ()-[r:MENTIONS]->() RETURN count(r) as count",
            "event_clusters": "MATCH (e:Event) WHERE e.cluster_id IS NOT NULL RETURN count(DISTINCT e.cluster_id) as count",
            "chunks_with_embeddings": "MATCH (c:Chunk) WHERE c.embedding IS NOT NULL RETURN count(c) as count",
            "entities_with_embeddings": "MATCH (e:Entity) WHERE e.embedding IS NOT NULL RETURN count(e) as count",
            "events_with_embeddings": "MATCH (e:Event) WHERE e.embedding IS NOT NULL RETURN count(e) as count"
        }

    def _get_delete_queries(self) -> Dict[str, List[str]]:
        """è·å–åˆ é™¤æŸ¥è¯¢è¯­å¥"""
        return {
            "all": [
                "MATCH (n) DETACH DELETE n",
            ],
            "entities": [
                "MATCH (e:Entity) DETACH DELETE e",
            ],
            "events": [
                "MATCH (e:Event) DETACH DELETE e",
            ],
            "relations": [
                "MATCH ()-[r:ENTITY_RELATION]->() DELETE r",
                "MATCH ()-[r:EVENT_RELATION]->() DELETE r",
                "MATCH ()-[r:SIMILAR_TO]->() DELETE r",
                "MATCH ()-[r:PARTICIPATES_IN]->() DELETE r",
                "MATCH ()-[r:CONTAINS]->() DELETE r",
                "MATCH ()-[r:MENTIONS]->() DELETE r",
            ]
        }

    async def _disambiguate_events_with_gds(self):
        """
        ä½¿ç”¨Neo4j GDSçš„KNNç®—æ³•è¿›è¡Œäº‹ä»¶æ¶ˆæ­§ï¼ŒæŒ–æ˜åŒä¹‰äº‹ä»¶
        """
        print("ğŸ¯ å¼€å§‹äº‹ä»¶æ¶ˆæ­§ - ä½¿ç”¨GDS KNNæŒ–æ˜åŒä¹‰äº‹ä»¶...")

        try:
            # 1. æ£€æŸ¥GDSæ˜¯å¦å¯ç”¨
            gds_check_query = "RETURN gds.version() as version"
            result = await self._execute_query(gds_check_query)
            print("  âœ… Neo4j GDSæ’ä»¶å¯ç”¨")

            # 2. åˆ›å»ºäº‹ä»¶åµŒå…¥å‘é‡çš„å›¾æŠ•å½±
            projection_query = """
            CALL gds.graph.drop('event_similarity', false) YIELD graphName
            """
            try:
                await self._execute_query(projection_query)
                print("  ğŸ“Š æ¸…ç†æ—§çš„å›¾æŠ•å½±")
            except:
                pass  # å¿½ç•¥ä¸å­˜åœ¨çš„å›¾æŠ•å½±é”™è¯¯

            # 3. åˆ›å»ºæ–°çš„å›¾æŠ•å½±
            create_projection_query = """
            CALL gds.graph.project(
                'event_similarity',
                'Event',
                '*',
                {
                    nodeProperties: ['embedding']
                }
            )
            """
            await self._execute_query(create_projection_query)
            print("  ğŸ“Š åˆ›å»ºäº‹ä»¶ç›¸ä¼¼åº¦å›¾æŠ•å½±å®Œæˆ")

            # 4. è¿è¡ŒKNNç®—æ³•è®¡ç®—ç›¸ä¼¼äº‹ä»¶
            knn_query = """
            CALL gds.knn.write(
                'event_similarity',
                {
                    topK: 10,
                    nodeProperties: ['embedding'],
                    writeRelationshipType: 'SIMILAR_TO',
                    writeProperty: 'similarity_score',
                    similarityCutoff: 0.9
                }
            )
            """
            await self._execute_query(knn_query)
            print("  ğŸ¤– KNNç®—æ³•æ‰§è¡Œå®Œæˆï¼Œåˆ›å»ºç›¸ä¼¼äº‹ä»¶å…³ç³»")

            # 5. å¤„ç†å·²å­˜åœ¨çš„äº‹ä»¶å…³ç³»ï¼Œå°†ç›¸ä¼¼åº¦ä¿¡æ¯åˆå¹¶åˆ°ç°æœ‰å…³ç³»ä¸­
            merge_similarity_query = """
            MATCH (e1:Event)-[r:EVENT_RELATION]->(e2:Event)
            MATCH (e1)-[s:SIMILAR_TO]-(e2)
            SET r.similarity_score = s.similarity_score,
                r.semantic_similarity = true,
                r.update_time = datetime()
            DELETE s
            """
            await self._execute_query(merge_similarity_query)
            print("  ğŸ”„ å°†ç›¸ä¼¼åº¦ä¿¡æ¯åˆå¹¶åˆ°ç°æœ‰äº‹ä»¶å…³ç³»ä¸­")

            # 6. æ¸…ç†å›¾æŠ•å½±
            cleanup_query = """
            CALL gds.graph.drop('event_similarity', false)
            """
            await self._execute_query(cleanup_query)
            print("  ğŸ§¹ æ¸…ç†å›¾æŠ•å½±å®Œæˆ")

        except Exception as e:
            print(f"  âš ï¸ GDSäº‹ä»¶æ¶ˆæ­§å¤±è´¥: {e}")

