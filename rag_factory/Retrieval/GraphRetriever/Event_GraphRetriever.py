import asyncio
import numpy as np
from typing import List, Dict, Any, Tuple, Optional, Set
from dataclasses import dataclass
from collections import defaultdict
import logging
import re

from rag_factory.Store.GraphStore.event_graphrag_neo4j import HyperRAGNeo4jStore
from rag_factory.Embed import Embeddings
from rag_factory.llms.llm_base import LLMBase

logger = logging.getLogger(__name__)

@dataclass
class SeedNode:
    """ç§å­èŠ‚ç‚¹"""
    id_: str
    name: str
    type: str  # 'entity' or 'event'
    score: float
    source: str  # 'extracted' or 'linked'

@dataclass
class PPRResult:
    """PageRankç»“æœ"""
    node_scores: Dict[str, float]
    chunk_scores: Dict[str, float]
    traversal_path: List[str]

@dataclass
class RetrievalContext:
    """æ£€ç´¢ä¸Šä¸‹æ–‡"""
    chunks: List[Dict[str, Any]]
    chunk_scores: List[float]
    evidence_sources: List[str]
    seed_nodes: List[SeedNode]
    ppr_result: PPRResult

@dataclass
class GenerationResult:
    """ç”Ÿæˆç»“æœ"""
    answer: str
    evidence_chunks: List[str]
    citations: List[str]
    confidence: float
    retrieval_context: RetrievalContext

class EventGraphRetriever:
    """
    å®ç°å®Œæ•´çš„5æ­¥æµç¨‹ï¼š
    1. æŸ¥è¯¢åˆ†æä¸æ„å›¾ç†è§£
    2. ç§å­èŠ‚ç‚¹è¯†åˆ«
    3. ä¸ªæ€§åŒ–PageRankå›¾éå†
    4. Chunkåˆ†æ•°è®¡ç®—ä¸æ’åº
    5. ä¸Šä¸‹æ–‡æ„å»ºä¸ç­”æ¡ˆç”Ÿæˆ
    """
    
    def __init__(self,
                 graph_store: HyperRAGNeo4jStore,
                 embedding_model: Embeddings,
                 llm_model: LLMBase,
                 max_seed_nodes: int = 10,
                 ppr_iterations: int = 20,
                 ppr_damping: float = 0.85,
                 top_k_chunks: int = 5,
                 similarity_threshold: float = 0.7,
                 enable_fallback: bool = True):
        """
        åˆå§‹åŒ–åœ¨çº¿æ£€ç´¢ç³»ç»Ÿ
        
        Args:
            graph_store: å›¾å­˜å‚¨å®ä¾‹
            embedding_model: åµŒå…¥æ¨¡å‹
            llm_model: å¤§è¯­è¨€æ¨¡å‹
            max_seed_nodes: æœ€å¤§ç§å­èŠ‚ç‚¹æ•°
            ppr_iterations: PageRankè¿­ä»£æ¬¡æ•°
            ppr_damping: PageRanké˜»å°¼å› å­
            top_k_chunks: è¿”å›çš„chunkæ•°é‡
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            enable_fallback: æ˜¯å¦å¯ç”¨å…œåº•æœºåˆ¶
        """
        self.graph_store = graph_store
        self.embedding_model = embedding_model
        self.llm_model = llm_model
        self.max_seed_nodes = max_seed_nodes
        self.ppr_iterations = ppr_iterations
        self.ppr_damping = ppr_damping
        self.top_k_chunks = top_k_chunks
        self.similarity_threshold = similarity_threshold
        self.enable_fallback = enable_fallback
        
        # ç¼“å­˜æ•°æ®
        self.entity_data = []
        self.entity_embeddings = np.array([])
        self.event_data = []
        self.event_embeddings = np.array([])
        self.chunk_data = []
        self.chunk_embeddings = np.array([])
        
        # Graph adjacency for PPR
        self.graph_adjacency = {}
        
    async def initialize(self):
        """åˆå§‹åŒ–ç³»ç»Ÿï¼ŒåŠ è½½å›¾æ•°æ®"""
        print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–æ£€ç´¢ç³»ç»Ÿ...")
        
        # åŠ è½½å®ä½“æ•°æ®
        await self._load_entity_data()
        
        # åŠ è½½äº‹ä»¶æ•°æ®
        await self._load_event_data()
        
        # åŠ è½½chunkæ•°æ®
        await self._load_chunk_data()
        
        # æ„å»ºå›¾é‚»æ¥è¡¨
        await self._build_graph_adjacency()
        
        print("âœ… æ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    async def retrieve_and_generate(self, query: str) -> GenerationResult:
        """
        æ‰§è¡Œå®Œæ•´çš„æ£€ç´¢ä¸ç”Ÿæˆæµç¨‹
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            GenerationResult: ç”Ÿæˆç»“æœ
        """
        print(f"ğŸ” å¼€å§‹å¤„ç†æŸ¥è¯¢: {query}")
        
        # Step 1: æŸ¥è¯¢åˆ†æä¸æ„å›¾ç†è§£
        extracted_entities, extracted_events = await self._step1_query_analysis(query)
        
        # Step 2: ç§å­èŠ‚ç‚¹è¯†åˆ«
        seed_nodes = await self._step2_seed_identification(
            extracted_entities, extracted_events, query
        )
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç§å­èŠ‚ç‚¹ï¼Œä½¿ç”¨å…œåº•æœºåˆ¶
        if not seed_nodes and self.enable_fallback:
            print("âš ï¸ æœªæ‰¾åˆ°ç§å­èŠ‚ç‚¹ï¼Œå¯ç”¨ç¨ å¯†å¬å›å…œåº•æœºåˆ¶")
            return await self._fallback_dense_retrieval(query)
        
        # Step 3: ä¸ªæ€§åŒ–PageRankå›¾éå†
        ppr_result = await self._step3_personalized_pagerank(seed_nodes)
        
        # Step 4: Chunkåˆ†æ•°è®¡ç®—ä¸æ’åº
        ranked_chunks = await self._step4_chunk_scoring(ppr_result)
        
        # Step 5: ä¸Šä¸‹æ–‡æ„å»ºä¸ç­”æ¡ˆç”Ÿæˆ
        generation_result = await self._step5_answer_generation(
            query, ranked_chunks, seed_nodes, ppr_result
        )
        
        return generation_result
    
    async def _step1_query_analysis(self, query: str) -> Tuple[List[str], List[str]]:
        """
        Step 1: æŸ¥è¯¢åˆ†æä¸æ„å›¾ç†è§£
        
        ä»æŸ¥è¯¢ä¸­æå–å®ä½“å’Œäº‹ä»¶
        """
        print("ğŸ“‹ Step 1: æŸ¥è¯¢åˆ†æä¸æ„å›¾ç†è§£")
        
        # å®ä½“æå–prompt
        entity_prompt = f"""
è¯·ä»ä»¥ä¸‹æŸ¥è¯¢ä¸­æå–æ‰€æœ‰é‡è¦çš„å®ä½“åç§°ã€‚åªè¿”å›å®ä½“åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”ã€‚

æŸ¥è¯¢: {query}

å®ä½“åˆ—è¡¨:
"""
        
        # äº‹ä»¶æå–prompt
        event_prompt = f"""
è¯·ä»ä»¥ä¸‹æŸ¥è¯¢ä¸­æå–æ‰€æœ‰äº‹ä»¶æˆ–åŠ¨ä½œã€‚åªè¿”å›äº‹ä»¶åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”ã€‚

æŸ¥è¯¢: {query}

äº‹ä»¶åˆ—è¡¨:
"""
        
        # è°ƒç”¨LLMæå–å®ä½“å’Œäº‹ä»¶
        try:
            entity_response = await self.llm_model.achat([{"role": "user", "content": entity_prompt}])
            event_response = await self.llm_model.achat([{"role": "user", "content": event_prompt}])
            
            # è§£æå“åº”
            entities = [e.strip() for e in entity_response.split(',') if e.strip()]
            events = [e.strip() for e in event_response.split(',') if e.strip()]
            
            print(f"  æå–åˆ°å®ä½“: {entities}")
            print(f"  æå–åˆ°äº‹ä»¶: {events}")
            
            return entities, events
            
        except Exception as e:
            print(f"âš ï¸ LLMæå–å¤±è´¥: {e}")
            return [], []
    
    async def _step2_seed_identification(self, 
                                       entities: List[str], 
                                       events: List[str], 
                                       query: str) -> List[SeedNode]:
        """
        Step 2: ç§å­èŠ‚ç‚¹è¯†åˆ«
        
        å°†å®ä½“å’Œäº‹ä»¶æ˜ å°„åˆ°çŸ¥è¯†å›¾è°±ä¸­çš„èŠ‚ç‚¹
        """
        print("ğŸ¯ Step 2: ç§å­èŠ‚ç‚¹è¯†åˆ«")
        
        seed_nodes = []
        
        # 2.1 å®ä½“é“¾æ¥
        for entity in entities:
            linked_nodes = await self._link_entity(entity)
            seed_nodes.extend(linked_nodes)
        
        # 2.2 äº‹ä»¶é“¾æ¥
        for event in events:
            linked_nodes = await self._link_event(event)
            seed_nodes.extend(linked_nodes)
        
        # 2.3 å…œåº•é€»è¾‘ï¼šä»äº‹ä»¶ä¸­æå–å‚ä¸å®ä½“
        if not seed_nodes and events:
            print("  âš ï¸ ç›´æ¥é“¾æ¥å¤±è´¥ï¼Œä»äº‹ä»¶ä¸­æå–å‚ä¸å®ä½“")
            for event in events:
                participant_entities = await self._extract_event_participants(event)
                for entity in participant_entities:
                    linked_nodes = await self._link_entity(entity)
                    seed_nodes.extend(linked_nodes)
        
        # 2.4 æœ€ç»ˆå…œåº•ï¼šå‘é‡ç›¸ä¼¼åº¦åŒ¹é…
        if not seed_nodes:
            print("  âš ï¸ å®ä½“é“¾æ¥å¤±è´¥ï¼Œä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦åŒ¹é…")
            seed_nodes = await self._vector_based_linking(query)
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        seen_ids = set()
        unique_seeds = []
        for node in seed_nodes:
            if node.id_ not in seen_ids:
                seen_ids.add(node.id_)
                unique_seeds.append(node)
                
        result_seeds = unique_seeds[:self.max_seed_nodes]
        
        print(f"  è¯†åˆ«åˆ° {len(result_seeds)} ä¸ªç§å­èŠ‚ç‚¹")
        for node in result_seeds:
            print(f"    - {node.name} ({node.type}, åˆ†æ•°: {node.score:.3f})")
        
        return result_seeds
    
    async def _step3_personalized_pagerank(self, seed_nodes: List[SeedNode]) -> PPRResult:
        """
        Step 3: ä¸ªæ€§åŒ–PageRankå›¾éå†
        
        ä»ç§å­èŠ‚ç‚¹å‘å¤–æ‰©æ•£ï¼Œå‘ç°ç›¸å…³çš„å®ä½“å’Œäº‹ä»¶
        """
        print("ğŸŒ Step 3: ä¸ªæ€§åŒ–PageRankå›¾éå†")
        
        if not seed_nodes:
            return PPRResult({}, {}, [])
        
        # åˆå§‹åŒ–èŠ‚ç‚¹åˆ†æ•°
        node_scores = defaultdict(float)
        seed_scores = {}
        
        # è®¾ç½®ç§å­èŠ‚ç‚¹åˆå§‹åˆ†æ•°
        total_seed_score = sum(node.score for node in seed_nodes)
        for node in seed_nodes:
            normalized_score = node.score / total_seed_score if total_seed_score > 0 else 1.0 / len(seed_nodes)
            seed_scores[node.id_] = normalized_score
            node_scores[node.id_] = normalized_score
        
        traversal_path = []
        
        # PageRankè¿­ä»£
        for iteration in range(self.ppr_iterations):
            new_scores = defaultdict(float)
            
            # éå†æ‰€æœ‰èŠ‚ç‚¹
            for node_id, score in node_scores.items():
                if node_id in self.graph_adjacency:
                    neighbors = self.graph_adjacency[node_id]
                    if neighbors:
                        # å°†åˆ†æ•°å¹³å‡åˆ†é…ç»™é‚»å±…
                        neighbor_score = score * self.ppr_damping / len(neighbors)
                        for neighbor_id in neighbors:
                            new_scores[neighbor_id] += neighbor_score
                
                # Teleportå›ç§å­èŠ‚ç‚¹
                if node_id in seed_scores:
                    new_scores[node_id] += score * (1 - self.ppr_damping) * seed_scores[node_id]
            
            # æ›´æ–°åˆ†æ•°
            node_scores = new_scores
            
            # è®°å½•éå†è·¯å¾„
            if iteration % 5 == 0:
                top_nodes = sorted(node_scores.items(), key=lambda x: x[1], reverse=True)[:5]
                traversal_path.append(f"Iteration {iteration}: {[f'{nid}({score:.3f})' for nid, score in top_nodes]}")
        
        # è®¡ç®—chunkåˆ†æ•°
        chunk_scores = await self._compute_chunk_scores_from_nodes(node_scores)
        
        print(f"  PageRankå®Œæˆï¼Œå‘ç° {len(node_scores)} ä¸ªç›¸å…³èŠ‚ç‚¹")
        print(f"  è®¡ç®—å‡º {len(chunk_scores)} ä¸ªchunkåˆ†æ•°")
        
        return PPRResult(dict(node_scores), chunk_scores, traversal_path)
    
    async def _step4_chunk_scoring(self, ppr_result: PPRResult) -> List[Dict[str, Any]]:
        """
        Step 4: Chunkåˆ†æ•°è®¡ç®—ä¸æ’åº
        
        å°†èŠ‚ç‚¹åˆ†æ•°æ˜ å°„åˆ°åŸå§‹chunkï¼Œå¾—åˆ°ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æœ¬è¯æ®
        """
        print("ğŸ“Š Step 4: Chunkåˆ†æ•°è®¡ç®—ä¸æ’åº")
        
        # è·å–æ‰€æœ‰chunkåŠå…¶åˆ†æ•°
        chunk_scores = ppr_result.chunk_scores
        
        # æŸ¥è¯¢chunkè¯¦ç»†ä¿¡æ¯
        ranked_chunks = []
        for chunk_id, score in sorted(chunk_scores.items(), key=lambda x: x[1], reverse=True):
            chunk_info = await self._get_chunk_info(chunk_id)
            if chunk_info:
                chunk_info["ppr_score"] = score
                ranked_chunks.append(chunk_info)
        
        print(f"  æ’åºå®Œæˆï¼Œtop-{min(len(ranked_chunks), self.top_k_chunks)} chunks:")
        for i, chunk in enumerate(ranked_chunks[:self.top_k_chunks]):
            print(f"    {i+1}. [åˆ†æ•°: {chunk['ppr_score']:.3f}] {chunk['content'][:100]}...")
        
        return ranked_chunks[:self.top_k_chunks]
    
    async def _step5_answer_generation(self, 
                                     query: str, 
                                     ranked_chunks: List[Dict[str, Any]], 
                                     seed_nodes: List[SeedNode], 
                                     ppr_result: PPRResult) -> GenerationResult:
        """
        Step 5: ä¸Šä¸‹æ–‡æ„å»ºä¸ç­”æ¡ˆç”Ÿæˆ
        
        åŸºäºé«˜åˆ†chunkç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        """
        print("ğŸ¤– Step 5: ä¸Šä¸‹æ–‡æ„å»ºä¸ç­”æ¡ˆç”Ÿæˆ")
        
        if not ranked_chunks:
            return GenerationResult(
                answer="æŠ±æ­‰ï¼Œæˆ‘æ— æ³•åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                evidence_chunks=[],
                citations=[],
                confidence=0.0,
                retrieval_context=RetrievalContext([], [], [], seed_nodes, ppr_result)
            )
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_chunks = ranked_chunks[:self.top_k_chunks]
        context_text = ""
        citations = []
        
        for i, chunk in enumerate(context_chunks):
            context_text += f"\n[è¯æ®{i+1}]: {chunk['content']}\n"
            citations.append(f"è¯æ®{i+1}")
        
        # æ„å»ºç”Ÿæˆprompt
        generation_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹èƒŒæ™¯ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¯·ç¡®ä¿ï¼š
1. å›ç­”åŸºäºæä¾›çš„è¯æ®
2. åœ¨å›ç­”ä¸­å¼•ç”¨ç›¸å…³è¯æ®
3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜

[èƒŒæ™¯ä¿¡æ¯]
{context_text}

[ç”¨æˆ·é—®é¢˜]
{query}

[ä½ çš„å›ç­”]
"""
        
        try:
            # ç”Ÿæˆç­”æ¡ˆ
            response = await self.llm_model.achat([{"role": "user", "content": generation_prompt}])
            
            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºchunkåˆ†æ•°å’Œæ•°é‡ï¼‰
            confidence = min(1.0, sum(chunk.get("ppr_score", 0) for chunk in context_chunks) / len(context_chunks))
            
            print(f"  ç­”æ¡ˆç”Ÿæˆå®Œæˆï¼Œç½®ä¿¡åº¦: {confidence:.3f}")
            
            return GenerationResult(
                answer=response,
                evidence_chunks=[chunk["content"] for chunk in context_chunks],
                citations=citations,
                confidence=confidence,
                retrieval_context=RetrievalContext(
                    context_chunks,
                    [chunk.get("ppr_score", 0) for chunk in context_chunks],
                    citations,
                    seed_nodes,
                    ppr_result
                )
            )
            
        except Exception as e:
            print(f"âš ï¸ ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return GenerationResult(
                answer="æŠ±æ­‰ï¼Œåœ¨ç”Ÿæˆç­”æ¡ˆæ—¶é‡åˆ°äº†é—®é¢˜ã€‚",
                evidence_chunks=[chunk["content"] for chunk in context_chunks],
                citations=citations,
                confidence=0.0,
                retrieval_context=RetrievalContext(context_chunks, [], citations, seed_nodes, ppr_result)
            )
    
    async def _fallback_dense_retrieval(self, query: str) -> GenerationResult:
        """å…œåº•æœºåˆ¶ï¼šç¨ å¯†å‘é‡æ£€ç´¢"""
        print("ğŸ”„ æ‰§è¡Œç¨ å¯†å‘é‡æ£€ç´¢å…œåº•æœºåˆ¶")
        
        if len(self.chunk_embeddings) == 0:
            return GenerationResult(
                answer="æŠ±æ­‰ï¼Œç³»ç»Ÿæš‚æ—¶æ— æ³•å¤„ç†æ‚¨çš„æŸ¥è¯¢ã€‚",
                evidence_chunks=[],
                citations=[],
                confidence=0.0,
                retrieval_context=RetrievalContext([], [], [], [], PPRResult({}, {}, []))
            )
        
        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.embedding_model.embed_documents([query])[0]
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(self.chunk_embeddings, query_embedding)
        
        # è·å–top-k
        top_indices = np.argsort(similarities)[-self.top_k_chunks:][::-1]
        
        top_chunks = []
        for idx in top_indices:
            chunk = self.chunk_data[idx].copy()
            chunk["similarity"] = float(similarities[idx])
            top_chunks.append(chunk)
        
        # ç”Ÿæˆç­”æ¡ˆ
        return await self._step5_answer_generation(
            query, top_chunks, [], PPRResult({}, {}, ["Dense retrieval fallback"])
        )
    
    # === è¾…åŠ©æ–¹æ³• ===
    
    async def _load_entity_data(self):
        """åŠ è½½å®ä½“æ•°æ®"""
        query = """
        MATCH (e:Entity) 
        WHERE e.embedding IS NOT NULL
        RETURN e.id_ as id_, e.entity_name as name, e.entity_type as type, 
               e.entity_descriptions as descriptions, e.embedding as embedding
        """
        
        async with self.graph_store._driver.session(database=self.graph_store.database) as session:
            result = await session.run(query)
            records = await result.data()
            
            entities = []
            embeddings = []
            
            for record in records:
                entity_data = {
                    "id_": record["id_"],
                    "name": record["name"],
                    "type": record["type"],
                    "descriptions": record["descriptions"] or []
                }
                entities.append(entity_data)
                embeddings.append(record["embedding"])
            
            self.entity_data = entities
            self.entity_embeddings = np.array(embeddings) if embeddings else np.array([])
            print(f"  ğŸ“Š åŠ è½½äº† {len(entities)} ä¸ªå®ä½“")
    
    async def _load_event_data(self):
        """åŠ è½½äº‹ä»¶æ•°æ®"""
        query = """
        MATCH (e:Event) 
        WHERE e.embedding IS NOT NULL
        RETURN e.id_ as id_, e.content as content, e.type as type,
               e.participants as participants, e.embedding as embedding
        """
        
        async with self.graph_store._driver.session(database=self.graph_store.database) as session:
            result = await session.run(query)
            records = await result.data()
            
            events = []
            embeddings = []
            
            for record in records:
                event_data = {
                    "id_": record["id_"],
                    "content": record["content"],
                    "type": record["type"],
                    "participants": record["participants"] or []
                }
                events.append(event_data)
                embeddings.append(record["embedding"])
            
            self.event_data = events
            self.event_embeddings = np.array(embeddings) if embeddings else np.array([])
            print(f"  ğŸ“Š åŠ è½½äº† {len(events)} ä¸ªäº‹ä»¶")
    
    async def _load_chunk_data(self):
        """åŠ è½½chunkæ•°æ®"""
        query = """
        MATCH (c:Chunk) 
        WHERE c.embedding IS NOT NULL
        RETURN c.id_ as id_, c.content as content, c.source as source, c.embedding as embedding
        """
        
        async with self.graph_store._driver.session(database=self.graph_store.database) as session:
            result = await session.run(query)
            records = await result.data()
            
            chunks = []
            embeddings = []
            
            for record in records:
                chunk_data = {
                    "id_": record["id_"],
                    "content": record["content"],
                    "source": record["source"]
                }
                chunks.append(chunk_data)
                embeddings.append(record["embedding"])
            
            self.chunk_data = chunks
            self.chunk_embeddings = np.array(embeddings) if embeddings else np.array([])
            print(f"  ğŸ“Š åŠ è½½äº† {len(chunks)} ä¸ªchunk")
    
    async def _build_graph_adjacency(self):
        """æ„å»ºå›¾é‚»æ¥è¡¨ç”¨äºPageRank"""
        print("  ğŸ”— æ„å»ºå›¾é‚»æ¥è¡¨...")
        
        queries = [
            # å®ä½“-å®ä½“å…³ç³»
            "MATCH (a:Entity)-[:ENTITY_RELATION]->(b:Entity) RETURN a.id_ as src, b.id_ as dst",
            # å®ä½“-äº‹ä»¶å…³ç³»
            "MATCH (a:Entity)-[:PARTICIPATES_IN]->(b:Event) RETURN a.id_ as src, b.id_ as dst",
            "MATCH (a:Event)<-[:PARTICIPATES_IN]-(b:Entity) RETURN a.id_ as src, b.id_ as dst",
            # äº‹ä»¶-äº‹ä»¶å…³ç³»
            "MATCH (a:Event)-[:EVENT_RELATION]->(b:Event) RETURN a.id_ as src, b.id_ as dst",
            # chunkå…³ç³»
            "MATCH (a:Chunk)-[:CONTAINS]->(b:Event) RETURN a.id_ as src, b.id_ as dst",
            "MATCH (a:Chunk)-[:MENTIONS]->(b:Entity) RETURN a.id_ as src, b.id_ as dst",
        ]
        
        adjacency = defaultdict(set)
        
        for query in queries:
            async with self.graph_store._driver.session(database=self.graph_store.database) as session:
                result = await session.run(query)
                records = await result.data()
                
                for record in records:
                    src = record["src"]
                    dst = record["dst"]
                    adjacency[src].add(dst)
                    adjacency[dst].add(src)  # æ— å‘å›¾
        
        # è½¬æ¢ä¸ºæ™®é€šdict
        self.graph_adjacency = {k: list(v) for k, v in adjacency.items()}
        
        total_edges = sum(len(neighbors) for neighbors in self.graph_adjacency.values()) // 2
        print(f"    æ„å»ºå®Œæˆ: {len(self.graph_adjacency)} ä¸ªèŠ‚ç‚¹, {total_edges} æ¡è¾¹")
    
    async def _link_entity(self, entity_name: str) -> List[SeedNode]:
        """é“¾æ¥å®ä½“åˆ°å›¾è°±èŠ‚ç‚¹"""
        seed_nodes = []
        
        # ç²¾ç¡®åŒ¹é…
        for entity in self.entity_data:
            if entity["name"].lower() == entity_name.lower():
                seed_nodes.append(SeedNode(
                    id_=entity["id_"],
                    name=entity["name"],
                    type="entity",
                    score=1.0,
                    source="exact_match"
                ))
        
        # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦
        if not seed_nodes and len(self.entity_embeddings) > 0:
            entity_embedding = self.embedding_model.embed_documents([entity_name])[0]
            similarities = np.dot(self.entity_embeddings, entity_embedding)
            
            for i, sim in enumerate(similarities):
                if sim >= self.similarity_threshold:
                    entity = self.entity_data[i]
                    seed_nodes.append(SeedNode(
                        id_=entity["id_"],
                        name=entity["name"],
                        type="entity",
                        score=float(sim),
                        source="vector_match"
                    ))
        
        return seed_nodes
    
    async def _link_event(self, event_text: str) -> List[SeedNode]:
        """é“¾æ¥äº‹ä»¶åˆ°å›¾è°±èŠ‚ç‚¹"""
        seed_nodes = []
        
        # å‘é‡ç›¸ä¼¼åº¦åŒ¹é…
        if len(self.event_embeddings) > 0:
            event_embedding = self.embedding_model.embed_documents([event_text])[0]
            similarities = np.dot(self.event_embeddings, event_embedding)
            
            for i, sim in enumerate(similarities):
                if sim >= self.similarity_threshold:
                    event = self.event_data[i]
                    seed_nodes.append(SeedNode(
                        id_=event["id_"],
                        name=event["content"],
                        type="event",
                        score=float(sim),
                        source="vector_match"
                    ))
        
        return seed_nodes
    
    async def _extract_event_participants(self, event_text: str) -> List[str]:
        """ä»Neo4jä¸­è·å–äº‹ä»¶çš„å‚ä¸è€…å®ä½“"""
        # é€šè¿‡å‘é‡ç›¸ä¼¼åº¦æ‰¾åˆ°æœ€ç›¸ä¼¼çš„äº‹ä»¶ï¼Œç„¶åè·å–å…¶å‚ä¸è€…
        if len(self.event_embeddings) == 0:
            return []
        
        # ç¼–ç äº‹ä»¶æ–‡æœ¬
        event_embedding = self.embedding_model.embed_documents([event_text])[0]
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(self.event_embeddings, event_embedding)
        
        # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„äº‹ä»¶
        most_similar_idx = np.argmax(similarities)
        most_similar_score = similarities[most_similar_idx]
        
        if most_similar_score < self.similarity_threshold:
            return []
        
        # è·å–æœ€ç›¸ä¼¼äº‹ä»¶çš„å‚ä¸è€…
        similar_event = self.event_data[most_similar_idx]
        participants = similar_event.get("participants", [])
        
        # æŸ¥è¯¢è¿™äº›å‚ä¸è€…çš„è¯¦ç»†ä¿¡æ¯
        if participants:
            query = """
            MATCH (e:Entity)
            WHERE e.entity_name IN $participants
            RETURN e.entity_name as name
            """
            
            async with self.graph_store._driver.session(database=self.graph_store.database) as session:
                result = await session.run(query, participants=participants)
                records = await result.data()
                
                entity_names = [record["name"] for record in records]
                return entity_names
        
        return []
    
    async def _vector_based_linking(self, query: str) -> List[SeedNode]:
        """åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„é“¾æ¥"""
        seed_nodes = []
        
        query_embedding = self.embedding_model.embed_documents([query])[0]
        
        # æœç´¢ç›¸ä¼¼å®ä½“
        if len(self.entity_embeddings) > 0:
            similarities = np.dot(self.entity_embeddings, query_embedding)
            top_indices = np.argsort(similarities)[-5:][::-1]
            
            for idx in top_indices:
                if similarities[idx] >= self.similarity_threshold:
                    entity = self.entity_data[idx]
                    seed_nodes.append(SeedNode(
                        id_=entity["id_"],
                        name=entity["name"],
                        type="entity",
                        score=float(similarities[idx]),
                        source="query_vector_match"
                    ))
        
        # æœç´¢ç›¸ä¼¼äº‹ä»¶
        if len(self.event_embeddings) > 0:
            similarities = np.dot(self.event_embeddings, query_embedding)
            top_indices = np.argsort(similarities)[-5:][::-1]
            
            for idx in top_indices:
                if similarities[idx] >= self.similarity_threshold:
                    event = self.event_data[idx]
                    seed_nodes.append(SeedNode(
                        id_=event["id_"],
                        name=event["content"],
                        type="event",
                        score=float(similarities[idx]),
                        source="query_vector_match"
                    ))
        
        return seed_nodes
    
    async def _compute_chunk_scores_from_nodes(self, node_scores: Dict[str, float]) -> Dict[str, float]:
        """ä»èŠ‚ç‚¹åˆ†æ•°è®¡ç®—chunkåˆ†æ•°"""
        chunk_scores = defaultdict(float)
        
        # æŸ¥è¯¢chunkä¸èŠ‚ç‚¹çš„å…³ç³»
        queries = [
            # chunkåŒ…å«çš„äº‹ä»¶
            """
            MATCH (c:Chunk)-[:CONTAINS]->(e:Event)
            WHERE e.id_ IN $node_ids
            RETURN c.id_ as chunk_id, e.id_ as node_id
            """,
            # chunkæåŠçš„å®ä½“
            """
            MATCH (c:Chunk)-[:MENTIONS]->(e:Entity)
            WHERE e.id_ IN $node_ids
            RETURN c.id_ as chunk_id, e.id_ as node_id
            """
        ]
        
        node_ids = list(node_scores.keys())
        
        for query in queries:
            async with self.graph_store._driver.session(database=self.graph_store.database) as session:
                result = await session.run(query, node_ids=node_ids)
                records = await result.data()
                
                for record in records:
                    chunk_id = record["chunk_id"]
                    node_id = record["node_id"]
                    if node_id in node_scores:
                        chunk_scores[chunk_id] += node_scores[node_id]
        
        return dict(chunk_scores)
    
    async def _get_chunk_info(self, chunk_id: str) -> Optional[Dict[str, Any]]:
        """è·å–chunkè¯¦ç»†ä¿¡æ¯"""
        for chunk in self.chunk_data:
            if chunk["id_"] == chunk_id:
                return chunk.copy()
        return None

# ä½¿ç”¨ç¤ºä¾‹
class OnlineRetrievalExample:
    """åœ¨çº¿æ£€ç´¢ç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹"""
    
    @staticmethod
    async def run_example():
        """è¿è¡Œç¤ºä¾‹"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µæä¾›storeå’Œmodels
        # store = HyperRAGNeo4jStore(...)
        # embedding_model = SomeEmbeddingModel(...)
        # llm_model = SomeLLMModel(...)
        
        # system = OnlineRetrievalSystem(store, embedding_model, llm_model)
        # await system.initialize()
        
        # ç¤ºä¾‹æŸ¥è¯¢
        queries = [
            "è‹¹æœå…¬å¸å‘å¸ƒVision Proåï¼Œå“ªäº›ä¾›åº”å•†çš„è‚¡ä»·å—åˆ°äº†å½±å“ï¼Ÿ",
            "å¼ ä¼Ÿåœ¨åæ˜Ÿç§‘æŠ€å…¬å¸çš„å·¥ä½œæƒ…å†µå¦‚ä½•ï¼Ÿ",
            "AIæŠ€æœ¯çš„æœ€æ–°å‘å±•è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
        
        # for query in queries:
        #     print(f"\n{'='*50}")
        #     result = await system.retrieve_and_generate(query)
        #     print(f"æŸ¥è¯¢: {query}")
        #     print(f"ç­”æ¡ˆ: {result.answer}")
        #     print(f"ç½®ä¿¡åº¦: {result.confidence:.3f}")
        #     print(f"è¯æ®æ•°é‡: {len(result.evidence_chunks)}")
        
        print("ç¤ºä¾‹ä»£ç å·²å‡†å¤‡å®Œæ¯•ï¼Œè¯·æ ¹æ®å®é™…ç¯å¢ƒé…ç½®ç›¸å…³ç»„ä»¶")

if __name__ == "__main__":
    asyncio.run(OnlineRetrievalExample.run_example())
