from abc import ABC, abstractmethod
from rag_factory.llms.llm_base import LLMBase
from typing import Callable, Dict, Set, Optional, Tuple
from datetime import datetime
import json
import os
import hashlib
from rag_factory.documents.Prompt import KG_TRIPLES_PROMPT, HYPERRAG_EXTRACTION_PROMPT
from rag_factory.documents.schema import Document
from rag_factory.Store.GraphStore.GraphNode import EntityNode, EventNode, MentionNode, Relation

KG_NODES_KEY = "entities"
KG_RELATIONS_KEY = "relations"
KG_EVENTS_KEY = "events"
KG_MENTIONS_KEY = "mentions"


__all__ = ["GraphExtractorBase", "GraphExtractor", "HyperRAGGraphExtractor", "IncrementalGraphProcessor", "GraphDataCache"]


class GraphDataCache:
    """
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. æœ¬åœ°æ–‡ä»¶ç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—
    2. å†…å­˜+ç£ç›˜åŒé‡ç¼“å­˜
    3. å¢é‡æ›´æ–°æ”¯æŒ
    4. å¿«é€ŸæŸ¥è¯¢æ¥å£
    """
    
    def __init__(self, cache_dir: str = "./graph_cache"):
        """
        åˆå§‹åŒ–å›¾æ•°æ®ç¼“å­˜
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
        """
        self.cache_dir = cache_dir
        self.entities_file = os.path.join(cache_dir, "entities.json")
        self.relations_file = os.path.join(cache_dir, "relations.json")
        self.metadata_file = os.path.join(cache_dir, "metadata.json")
        self.processed_chunks_file = os.path.join(cache_dir, "processed_chunks.json")
        
        # å†…å­˜ç¼“å­˜
        self._entities_cache: Dict[str, dict] = {}  # name -> entity
        self._relations_cache: Dict[str, list] = {}  # (head, tail, type) -> relation
        self._processed_chunks: Set[str] = set()  # å·²å¤„ç†çš„chunk IDé›†åˆ
        self._last_update_time: Optional[datetime] = None
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(cache_dir, exist_ok=True)
        
        # åŠ è½½ç°æœ‰ç¼“å­˜
        self._load_cache()
    
    def _load_cache(self):
        """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½ç¼“å­˜"""
        try:
            # åŠ è½½å…ƒæ•°æ®
            if os.path.exists(self.metadata_file):
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    last_update_str = metadata.get('last_update_time')
                    if last_update_str:
                        self._last_update_time = datetime.fromisoformat(last_update_str)
            
            # åŠ è½½å®ä½“ç¼“å­˜
            if os.path.exists(self.entities_file):
                with open(self.entities_file, 'r', encoding='utf-8') as f:
                    self._entities_cache = json.load(f)
            
            # åŠ è½½å…³ç³»ç¼“å­˜
            if os.path.exists(self.relations_file):
                with open(self.relations_file, 'r', encoding='utf-8') as f:
                    relations_data = json.load(f)
                    # è½¬æ¢ä¸ºä¾¿äºæŸ¥è¯¢çš„æ ¼å¼
                    for rel in relations_data:
                        key = (rel.get('head_id', ''), rel.get('tail_id', ''), rel.get('label', ''))
                        if key not in self._relations_cache:
                            self._relations_cache[key] = []
                        self._relations_cache[key].append(rel)
            
            # åŠ è½½å·²å¤„ç†çš„chunkä¿¡æ¯
            if os.path.exists(self.processed_chunks_file):
                with open(self.processed_chunks_file, 'r', encoding='utf-8') as f:
                    processed_chunks_data = json.load(f)
                    self._processed_chunks = set(processed_chunks_data.get('processed_chunks', []))
            
            if self._entities_cache or self._relations_cache or self._processed_chunks:
                print(f"âœ… ç¼“å­˜åŠ è½½å®Œæˆï¼š{len(self._entities_cache)}ä¸ªå®ä½“ï¼Œ{len(self._relations_cache)}ä¸ªå…³ç³»ç±»å‹ï¼Œ{len(self._processed_chunks)}ä¸ªå·²å¤„ç†chunk")
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥ï¼š{e}")
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜åˆ°æœ¬åœ°æ–‡ä»¶"""
        try:
            print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ç¼“å­˜åˆ° {self.cache_dir}...")
            print(f"  ğŸ“Š å®ä½“æ•°é‡: {len(self._entities_cache)}")
            print(f"  ğŸ”— å…³ç³»æ•°é‡: {sum(len(rels) for rels in self._relations_cache.values())}")
            print(f"  ğŸ“„ å·²å¤„ç†chunkæ•°é‡: {len(self._processed_chunks)}")
            
            # ä¿å­˜å®ä½“
            with open(self.entities_file, 'w', encoding='utf-8') as f:
                json.dump(self._entities_cache, f, indent=2, ensure_ascii=False, default=str)
            print(f"  âœ… å®ä½“æ–‡ä»¶å·²ä¿å­˜: {self.entities_file}")
            
            # ä¿å­˜å…³ç³»ï¼ˆè½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼‰
            relations_list = []
            for relations in self._relations_cache.values():
                relations_list.extend(relations)
            
            with open(self.relations_file, 'w', encoding='utf-8') as f:
                json.dump(relations_list, f, indent=2, ensure_ascii=False, default=str)
            print(f"  âœ… å…³ç³»æ–‡ä»¶å·²ä¿å­˜: {self.relations_file}")
            
            # ä¿å­˜å·²å¤„ç†çš„chunkä¿¡æ¯
            processed_chunks_data = {
                'processed_chunks': list(self._processed_chunks),
                'chunk_count': len(self._processed_chunks),
                'last_update_time': self._last_update_time.isoformat() if self._last_update_time else None
            }
            
            with open(self.processed_chunks_file, 'w', encoding='utf-8') as f:
                json.dump(processed_chunks_data, f, indent=2, ensure_ascii=False)
            print(f"  âœ… å·²å¤„ç†chunkæ–‡ä»¶å·²ä¿å­˜: {self.processed_chunks_file}")
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                'last_update_time': self._last_update_time.isoformat() if self._last_update_time else None,
                'entity_count': len(self._entities_cache),
                'relation_count': len(relations_list),
                'processed_chunk_count': len(self._processed_chunks),
                'save_time': datetime.now().isoformat()
            }
            
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            print(f"  âœ… å…ƒæ•°æ®æ–‡ä»¶å·²ä¿å­˜: {self.metadata_file}")
                
        except Exception as e:
            print(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥ï¼š{e}")
            import traceback
            traceback.print_exc()
    
    def load_data(self, entities: list = None, relations: list = None):
        """
        æ‰‹åŠ¨åŠ è½½æ•°æ®åˆ°ç¼“å­˜
        
        Args:
            entities: å®ä½“åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä½“åº”åŒ…å«nameå­—æ®µ
            relations: å…³ç³»åˆ—è¡¨ï¼Œæ¯ä¸ªå…³ç³»åº”åŒ…å«head_id, tail_id, labelå­—æ®µ
        """
        if entities:
            print(f"ğŸ“¥ åŠ è½½ {len(entities)} ä¸ªå®ä½“åˆ°ç¼“å­˜...")
            for entity in entities:
                name = entity.get('name')
                if name:
                    entity['update_time'] = datetime.now().isoformat()
                    self._entities_cache[name] = entity
        
        if relations:
            print(f"ğŸ“¥ åŠ è½½ {len(relations)} ä¸ªå…³ç³»åˆ°ç¼“å­˜...")
            for relation in relations:
                head = relation.get('head_id', '')
                tail = relation.get('tail_id', '')
                rel_type = relation.get('label', '')
                
                key = (head, tail, rel_type)
                if key not in self._relations_cache:
                    self._relations_cache[key] = []
                
                relation['update_time'] = datetime.now().isoformat()
                self._relations_cache[key].append(relation)
        
        # æ›´æ–°æ—¶é—´æˆ³å¹¶ä¿å­˜
        self._last_update_time = datetime.now()
        self._save_cache()
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼šæ€»è®¡ {len(self._entities_cache)} ä¸ªå®ä½“ï¼Œ{sum(len(rels) for rels in self._relations_cache.values())} ä¸ªå…³ç³»")
    
    def get_entity(self, name: str) -> Optional[dict]:
        """è·å–å®ä½“"""
        return self._entities_cache.get(name)
    
    def get_all_entities(self) -> Dict[str, dict]:
        """è·å–æ‰€æœ‰å®ä½“"""
        return self._entities_cache.copy()
    
    def get_relations(self, head: str = None, tail: str = None, rel_type: str = None) -> list:
        """è·å–å…³ç³»"""
        if head and tail and rel_type:
            # ç²¾ç¡®æŸ¥è¯¢
            key = (head, tail, rel_type)
            return self._relations_cache.get(key, [])
        
        # æ¨¡ç³ŠæŸ¥è¯¢
        results = []
        for (h, t, r), relations in self._relations_cache.items():
            if (head is None or h == head) and \
               (tail is None or t == tail) and \
               (rel_type is None or r == rel_type):
                results.extend(relations)
        
        return results
    
    def get_all_relations(self) -> list:
        """è·å–æ‰€æœ‰å…³ç³»"""
        relations = []
        for rel_list in self._relations_cache.values():
            relations.extend(rel_list)
        return relations
    
    def add_entity(self, entity: dict, save_immediately: bool = True):
        """æ·»åŠ å®ä½“åˆ°ç¼“å­˜"""
        name = entity.get('name')
        if name:
            entity['update_time'] = datetime.now().isoformat()
            self._entities_cache[name] = entity
            # å¯é€‰æ‹©ç«‹å³ä¿å­˜åˆ°ç£ç›˜
            if save_immediately:
                self._save_cache()
    
    def add_relation(self, relation: dict, save_immediately: bool = True):
        """æ·»åŠ å…³ç³»åˆ°ç¼“å­˜"""
        head = relation.get('head_id', '')
        tail = relation.get('tail_id', '')
        rel_type = relation.get('label', '')
        
        key = (head, tail, rel_type)
        if key not in self._relations_cache:
            self._relations_cache[key] = []
        
        # é¿å…é‡å¤
        for existing_rel in self._relations_cache[key]:
            if existing_rel.get('description') == relation.get('description'):
                return  # å·²å­˜åœ¨ç›¸åŒå…³ç³»
        
        relation['update_time'] = datetime.now().isoformat()
        self._relations_cache[key].append(relation)
        # å¯é€‰æ‹©ç«‹å³ä¿å­˜åˆ°ç£ç›˜
        if save_immediately:
            self._save_cache()
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._entities_cache.clear()
        self._relations_cache.clear()
        self._processed_chunks.clear()
        self._last_update_time = None
        self._save_cache()
        print("ğŸ—‘ï¸ ç¼“å­˜å·²æ¸…ç©º")
    
    def get_cache_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_relations = sum(len(rels) for rels in self._relations_cache.values())
        
        return {
            'entity_count': len(self._entities_cache),
            'relation_count': total_relations,
            'processed_chunk_count': len(self._processed_chunks),
            'last_update_time': self._last_update_time.isoformat() if self._last_update_time else None,
            'cache_size_mb': self._estimate_cache_size()
        }
    
    def _estimate_cache_size(self) -> float:
        """ä¼°ç®—ç¼“å­˜å¤§å°ï¼ˆMBï¼‰"""
        try:
            total_size = 0
            for file_path in [self.entities_file, self.relations_file, self.metadata_file, self.processed_chunks_file]:
                if os.path.exists(file_path):
                    total_size += os.path.getsize(file_path)
            return round(total_size / 1024 / 1024, 2)
        except:
            return 0.0
    
    def is_chunk_processed(self, chunk_id: str) -> bool:
        """
        æ£€æŸ¥æŒ‡å®šchunkæ˜¯å¦å·²ç»å¤„ç†è¿‡
        
        Args:
            chunk_id: chunkçš„å”¯ä¸€æ ‡è¯†ç¬¦
            
        Returns:
            bool: å¦‚æœå·²å¤„ç†è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        return chunk_id in self._processed_chunks
    
    def mark_chunk_processed(self, chunk_id: str, save_immediately: bool = True):
        """
        æ ‡è®°chunkä¸ºå·²å¤„ç†
        
        Args:
            chunk_id: chunkçš„å”¯ä¸€æ ‡è¯†ç¬¦
            save_immediately: æ˜¯å¦ç«‹å³ä¿å­˜åˆ°ç£ç›˜
        """
        if chunk_id not in self._processed_chunks:
            self._processed_chunks.add(chunk_id)
            self._last_update_time = datetime.now()
            
            if save_immediately:
                self._save_cache()
    
    def mark_chunks_processed(self, chunk_ids: list[str], save_immediately: bool = True):
        """
        æ‰¹é‡æ ‡è®°chunksä¸ºå·²å¤„ç†
        
        Args:
            chunk_ids: chunk IDåˆ—è¡¨
            save_immediately: æ˜¯å¦ç«‹å³ä¿å­˜åˆ°ç£ç›˜
        """
        new_chunks = [cid for cid in chunk_ids if cid not in self._processed_chunks]
        if new_chunks:
            self._processed_chunks.update(new_chunks)
            self._last_update_time = datetime.now()
            
            if save_immediately:
                self._save_cache()
            
            print(f"ğŸ“„ æ ‡è®°äº† {len(new_chunks)} ä¸ªæ–°chunkä¸ºå·²å¤„ç†")
    
    def get_processed_chunks(self) -> Set[str]:
        """
        è·å–æ‰€æœ‰å·²å¤„ç†çš„chunk ID
        
        Returns:
            Set[str]: å·²å¤„ç†çš„chunk IDé›†åˆ
        """
        return self._processed_chunks.copy()
    
    def filter_unprocessed_chunks(self, documents: list) -> list:
        """
        è¿‡æ»¤å‡ºæœªå¤„ç†çš„æ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åº”åŒ…å«metadata.chunk_id
            
        Returns:
            list: æœªå¤„ç†çš„æ–‡æ¡£åˆ—è¡¨
        """
        unprocessed_docs = []
        for doc in documents:
            chunk_id = None
            
            # å°è¯•ä»ä¸åŒæ¥æºè·å–chunk_id
            if hasattr(doc, 'metadata') and isinstance(doc.metadata, dict):
                chunk_id = doc.metadata.get('chunk_id')
            elif hasattr(doc, 'chunk_id'):
                chunk_id = doc.chunk_id
            elif isinstance(doc, dict):
                chunk_id = doc.get('chunk_id') or doc.get('metadata', {}).get('chunk_id')
            
            if chunk_id and not self.is_chunk_processed(chunk_id):
                unprocessed_docs.append(doc)
            elif not chunk_id:
                # å¦‚æœæ²¡æœ‰chunk_idï¼Œåˆ™è®¤ä¸ºéœ€è¦å¤„ç†
                unprocessed_docs.append(doc)
        
        print(f"ğŸ” ä» {len(documents)} ä¸ªæ–‡æ¡£ä¸­ç­›é€‰å‡º {len(unprocessed_docs)} ä¸ªæœªå¤„ç†çš„æ–‡æ¡£")
        return unprocessed_docs
    
    def remove_processed_chunk(self, chunk_id: str, save_immediately: bool = True):
        """
        ç§»é™¤å·²å¤„ç†çš„chunkæ ‡è®°ï¼ˆç”¨äºé‡æ–°å¤„ç†ï¼‰
        
        Args:
            chunk_id: chunkçš„å”¯ä¸€æ ‡è¯†ç¬¦
            save_immediately: æ˜¯å¦ç«‹å³ä¿å­˜åˆ°ç£ç›˜
        """
        if chunk_id in self._processed_chunks:
            self._processed_chunks.remove(chunk_id)
            
            if save_immediately:
                self._save_cache()
            
            print(f"ğŸ”„ å·²ç§»é™¤chunk {chunk_id} çš„å¤„ç†æ ‡è®°ï¼Œå°†é‡æ–°å¤„ç†")


class GraphExtractorBase(ABC):
    """
    å›¾æå–å™¨åŸºç±»ï¼Œå®šä¹‰äº†æ‰€æœ‰å›¾æå–å™¨çš„é€šç”¨æ¥å£å’ŒåŠŸèƒ½ã€‚
    
    å­ç±»éœ€è¦å®ç°ï¼š
    - _aextract: å¼‚æ­¥æå–å•ä¸ªæ–‡æ¡£çš„å›¾ç»“æ„
    - class_name: è¿”å›ç±»åçš„ç±»æ–¹æ³•
    
    æä¾›çš„é€šç”¨åŠŸèƒ½ï¼š
    - å¹¶å‘æ§åˆ¶
    - æ‰¹é‡å¤„ç†
    - è¿›åº¦æ˜¾ç¤º
    - åŒæ­¥/å¼‚æ­¥è°ƒç”¨æ¥å£
    """
    
    def __init__(
        self,
        llm: LLMBase,
        extract_prompt: str = None,
        parse_fn: Callable = None,
        max_concurrent: int = 100,
        enable_incremental: bool = False,
        cache_dir: str = None
    ) -> None:
        """
        åˆå§‹åŒ–å›¾æå–å™¨åŸºç±»
        
        Args:
            llm: å¤§è¯­è¨€æ¨¡å‹å®ä¾‹
            extract_prompt: æå–æç¤ºæ¨¡æ¿
            parse_fn: è§£æå‡½æ•°
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            enable_incremental: æ˜¯å¦å¯ç”¨å¢é‡å¤„ç†
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸æä¾›åˆ™ä¸å¯ç”¨æŒä¹…åŒ–
        """
        self.llm = llm
        self.extract_prompt = extract_prompt
        self.parse_fn = parse_fn
        self.max_concurrent = max_concurrent
        self.enable_incremental = enable_incremental
        self.cache_dir = cache_dir
        
        # ä½¿ç”¨GraphDataCacheæ¥å¤„ç†ç¼“å­˜
        if self.cache_dir and self.enable_incremental:
            self.cache = GraphDataCache(cache_dir)
        else:
            self.cache = None

    @abstractmethod
    async def _aextract(self, document: Document, semaphore) -> Document:
        """
        å¼‚æ­¥æå–å•ä¸ªæ–‡æ¡£çš„å›¾ç»“æ„ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰
        
        Args:
            document: å¾…å¤„ç†çš„æ–‡æ¡£
            semaphore: ä¿¡å·é‡ï¼Œç”¨äºæ§åˆ¶å¹¶å‘
            
        Returns:
            Document: å¤„ç†åçš„æ–‡æ¡£ï¼Œmetadataä¸­åŒ…å«æå–çš„å›¾ç»“æ„
        """
        pass

    def _filter_documents_for_incremental(self, documents: list[Document]) -> list[Document]:
        """
        è¿‡æ»¤éœ€è¦å¢é‡å¤„ç†çš„æ–‡æ¡£
        
        Args:
            documents: è¾“å…¥æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            éœ€è¦å¤„ç†çš„æ–‡æ¡£åˆ—è¡¨
        """
        if not self.enable_incremental or not self.cache:
            return documents
        
        # ä½¿ç”¨GraphDataCacheçš„filter_unprocessed_chunksæ–¹æ³•
        return self.cache.filter_unprocessed_chunks(documents)

    def _update_incremental_state(self, documents: list[Document]):
        """
        æ›´æ–°å¢é‡å¤„ç†çŠ¶æ€
        
        Args:
            documents: å·²å¤„ç†çš„æ–‡æ¡£åˆ—è¡¨
        """
        if not self.enable_incremental or not self.cache:
            return
            
        # ä½¿ç”¨GraphDataCacheæ ‡è®°chunksä¸ºå·²å¤„ç†
        chunk_ids = []
        for doc in documents:
            chunk_id = doc.metadata.get("chunk_id")
            if chunk_id:
                chunk_ids.append(chunk_id)
        
        if chunk_ids:
            self.cache.mark_chunks_processed(chunk_ids, save_immediately=False)

    async def acall(self, documents: list[Document], show_progress: bool = False) -> list[Document]:
        """å¼‚æ­¥æå–æ‰€æœ‰æ–‡æ¡£çš„å›¾ç»“æ„"""
        import asyncio
        
        # å¦‚æœå¯ç”¨å¢é‡å¤„ç†ï¼Œå…ˆè¿‡æ»¤æ–‡æ¡£
        docs_to_process = self._filter_documents_for_incremental(documents)
        
        if self.enable_incremental and show_progress:
            print(f"å¢é‡å¤„ç†ï¼šä»{len(documents)}ä¸ªæ–‡æ¡£ä¸­ç­›é€‰å‡º{len(docs_to_process)}ä¸ªéœ€è¦å¤„ç†")
        
        if not docs_to_process:
            if show_progress:
                print("æ— éœ€å¤„ç†æ–°æ–‡æ¡£")
            return documents  # è¿”å›åŸå§‹æ–‡æ¡£åˆ—è¡¨
        
        # åˆ›å»ºä¿¡å·é‡æ¥æ§åˆ¶å¹¶å‘æ•°é‡
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = [self._aextract(document, semaphore) for document in docs_to_process]
        
        if show_progress:
            print(f"å¼€å§‹ä»{len(docs_to_process)}ä¸ªChunkä¸­æå–å›¾ç»“æ„...")
            from tqdm.asyncio import tqdm_asyncio
            results = await tqdm_asyncio.gather(*tasks, desc="æå–å›¾ç»“æ„")
        else:
            results = await asyncio.gather(*tasks)
        
        # æ›´æ–°å¢é‡å¤„ç†çŠ¶æ€
        self._update_incremental_state(results)
        
        # ä¿å­˜ç¼“å­˜çŠ¶æ€
        if self.enable_incremental and self.cache:
            self.cache._save_cache()
        
        if show_progress:
            print("å›¾ç»“æ„æå–å®Œæˆ")
        
        # å¦‚æœæ˜¯å¢é‡å¤„ç†ï¼Œéœ€è¦åˆå¹¶ç»“æœ
        if self.enable_incremental and len(docs_to_process) < len(documents):
            # åˆ›å»ºç»“æœæ˜ å°„
            processed_map = {doc.metadata.get("chunk_id"): doc for doc in results}
            
            # åˆå¹¶ç»“æœï¼šä¿ç•™åŸæ–‡æ¡£ï¼Œæ›´æ–°å·²å¤„ç†çš„éƒ¨åˆ†
            final_results = []
            for original_doc in documents:
                chunk_id = original_doc.metadata.get("chunk_id")
                if chunk_id in processed_map:
                    final_results.append(processed_map[chunk_id])
                else:
                    final_results.append(original_doc)
            return final_results
        
        return results

    def __call__(self, documents: list[Document], show_progress: bool = False) -> list[Document]:
        """åŒæ­¥æ¥å£ï¼šæå–å›¾ç»“æ„"""
        import asyncio
        return asyncio.run(self.acall(documents, show_progress=show_progress))

    @classmethod
    @abstractmethod
    def class_name(cls) -> str:
        """è¿”å›ç±»åï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰"""
        pass


class GraphExtractor(GraphExtractorBase):
    """
    GraphExtractor is a class that extracts triples from a graph.

    Uses an LLM and a simple prompt + output parsing to extract paths (i.e. triples) and entity, relation descriptions from text.
    """
    
    def __init__(
        self,
        llm: LLMBase,
        extract_prompt: str = None,
        parse_fn: Callable = None,
        max_concurrent: int = 100,
        enable_incremental: bool = False,
        cache_dir: str = None
    ) -> None:
        extract_prompt = extract_prompt if extract_prompt is not None else KG_TRIPLES_PROMPT
        super().__init__(llm, extract_prompt, parse_fn, max_concurrent, enable_incremental, cache_dir)


    async def _aextract(self, document: Document, semaphore) -> dict:
        """ä»documentsä¸­å¼‚æ­¥æå–å®ä½“ã€ä¸‰å…ƒç»„"""
        async with semaphore:
            content = document.content
            if not content:
                return document
                
            try:
                prompt = self.extract_prompt.format(
                    text=content
                )
                messages = [
                    {"role": "user", "content": prompt}
                ]
                llm_response = await self.llm.achat(messages, response_format={"type": "json_object"})
                if self.parse_fn is None:
                    print("é”™è¯¯ï¼šparse_fnä¸ºNoneï¼")
                    entities, relationships = [], []
                else:
                    entities, relationships = self.parse_fn(llm_response)
                    print(f"GraphExtractor parse_fnè¿”å›: entities={len(entities)}, relationships={len(relationships)}")
            except Exception as e:
                print(f"æå–ä¸‰å…ƒç»„æ—¶å‡ºé”™: {e}")
                entities = []
                relationships = []
            
            # è·å–å·²æœ‰å®ä½“ã€å…³ç³»ï¼ˆä»documentä¸­å·²æå–ä¸‰å…ƒç»„ï¼Œéœ€è¦æ›´æ–°entityã€relationï¼‰
            existing_nodes = document.metadata.pop(KG_NODES_KEY, [])
            existing_relations = document.metadata.pop(KG_RELATIONS_KEY, [])
            entity_metadata = document.metadata.copy()

            # æ„å»ºentity
            for entity, entity_type, description in entities:
                entity_metadata["entity_description"] = description
                entity_metadata["source_chunk_id"] = document.metadata["chunk_id"]
                # ç”Ÿæˆå”¯ä¸€çš„å®ä½“ID
                entity_id = f"Entity_{hash(entity) % 1000000:06d}"
                entity_node = EntityNode(
                    id_=entity_id,
                    name=entity, 
                    label=entity_type, 
                    metadatas=entity_metadata
                )
                existing_nodes.append(entity_node)

            relation_metadata = document.metadata.copy()

            # æ„å»ºrelations
            for triple in relationships:
                head, tail, rel, description = triple
                relation_metadata["relationship_description"] = description
                rel_node = Relation(
                    label=rel,
                    head_id=head,
                    tail_id=tail,
                    metadatas=relation_metadata
                )
                existing_relations.append(rel_node)

            document.metadata[KG_NODES_KEY] = existing_nodes
            document.metadata[KG_RELATIONS_KEY] = existing_relations
            return document




    @classmethod
    def class_name(cls) -> str:
        return "GraphExtractor"


class HyperRAGGraphExtractor(GraphExtractorBase):
    """
    HyperRAGGraphExtractorç”¨äºæå–å®Œæ•´çš„å›¾ç»“æ„ï¼ŒåŒ…æ‹¬äº‹ä»¶ã€mentionã€å®ä½“å’Œå…³ç³»ã€‚
    
    ä½¿ç”¨ä¸“é—¨çš„æç¤ºæ¨¡æ¿æ¥æå–å±‚æ¬¡åŒ–çš„å›¾ç»“æ„ã€‚
    """
    
    def __init__(
        self,
        llm: LLMBase,
        extract_prompt: str = None,
        parse_fn: Callable = None,
        max_concurrent: int = 100,
        enable_incremental: bool = False,
        cache_dir: str = None
    ) -> None:
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        extract_prompt = extract_prompt if extract_prompt is not None else HYPERRAG_EXTRACTION_PROMPT
        super().__init__(llm, extract_prompt, parse_fn, max_concurrent, enable_incremental, cache_dir)



    async def _aextract(self, document: Document, semaphore) -> Document:
        """ä»documentsä¸­å¼‚æ­¥æå–å®Œæ•´çš„å›¾ç»“æ„"""
        async with semaphore:
            content = document.content
            if not content:
                return document
                
            try:
                chunk_id = document.metadata.get("chunk_id", f"chunk_{hash(content)}")
                
                prompt = self.extract_prompt.format(text=content)
                messages = [{"role": "user", "content": prompt}]
                llm_response = await self.llm.achat(messages, response_format={"type": "json_object"})
                print(f"HyperRAGGraphExtractor æå–ç»“æœ: {llm_response}")
                if self.parse_fn is None:
                    print("é”™è¯¯ï¼šparse_fnä¸ºNoneï¼")
                    result = {"events": [], "mentions": [], "event_relations": [], "entity_relations": []}
                else:
                    result = self.parse_fn(llm_response, chunk_id)
                    print(f"HyperRAGGraphExtractor è§£æç»“æœ: {len(result['events'])} ä¸ªäº‹ä»¶, "
                          f"{len(result['mentions'])} ä¸ªmentions, "
                          f"{len(result.get('event_relations', []))} ä¸ªäº‹ä»¶å…³ç³»")
                    
            except Exception as e:
                print(f"æå–å›¾ç»“æ„æ—¶å‡ºé”™: {e}")
                result = {"events": [], "mentions": [], "event_relations": [], "entity_relations": []}
            
            # å°†ç»“æœå­˜å‚¨åˆ°document metadataä¸­
            document.metadata[KG_EVENTS_KEY] = result["events"]
            document.metadata[KG_MENTIONS_KEY] = result["mentions"]
            document.metadata["event_relations"] = result.get("event_relations", [])
            document.metadata["entity_relations"] = result.get("entity_relations", [])
            
            return document

    @classmethod
    def class_name(cls) -> str:
        return "HyperRAGGraphExtractor"




class IncrementalGraphProcessor:
    """
    å¢é‡å›¾å¤„ç†å™¨ - é›†æˆHyperRAGå›¾ç»“æ„å¤„ç†å’Œå¢é‡æ•°æ®å¤„ç†
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. å¤„ç†HyperRAGæå–çš„å®Œæ•´å›¾ç»“æ„ï¼ˆeventsã€mentionsã€entitiesã€relationsï¼‰
    2. å¯¹mentionsè¿›è¡Œå»é‡ï¼Œæ„å»ºæœ€ç»ˆçš„entities
    3. æ£€æµ‹æ–°å¢å’Œæ›´æ–°çš„å®ä½“/å…³ç³»
    4. åˆå¹¶æ–°æ—§æ•°æ®ï¼Œé¿å…é‡å¤
    5. ç»´æŠ¤æ•°æ®ç‰ˆæœ¬å’Œæ—¶é—´æˆ³
    6. æ”¯æŒæœ¬åœ°ç¼“å­˜æœºåˆ¶
    """
    
    def __init__(self, cache: GraphDataCache = None, existing_entities: list = None, existing_relations: list = None):
        """
        åˆå§‹åŒ–å¢é‡å›¾å¤„ç†å™¨
        
        Args:
            cache: å›¾æ•°æ®ç¼“å­˜å®ä¾‹ï¼ˆæ¨èï¼‰
            existing_entities: å·²å­˜åœ¨çš„å®ä½“åˆ—è¡¨ï¼ˆå¦‚æœä¸ä½¿ç”¨ç¼“å­˜ï¼‰
            existing_relations: å·²å­˜åœ¨çš„å…³ç³»åˆ—è¡¨ï¼ˆå¦‚æœä¸ä½¿ç”¨ç¼“å­˜ï¼‰
        """
        self.cache = cache
        
        # å¦‚æœä½¿ç”¨ç¼“å­˜ï¼Œä»ç¼“å­˜åŠ è½½æ•°æ®ï¼›å¦åˆ™ä½¿ç”¨ä¼ å…¥çš„æ•°æ®
        if self.cache:
            self.existing_entities = list(self.cache.get_all_entities().values())
            self.existing_relations = self.cache.get_all_relations()
        else:
            self.existing_entities = existing_entities or []
            self.existing_relations = existing_relations or []
        
        # å¢é‡å¤„ç†çŠ¶æ€
        self.entity_name_map = {}  # å®ä½“åç§°åˆ°å®ä½“å¯¹è±¡çš„æ˜ å°„
        self.new_entities = []  # æ–°å¢å®ä½“
        self.updated_entities = []  # æ›´æ–°å®ä½“
        self.new_relations = []  # æ–°å¢å…³ç³»
        
        # HyperRAGå¤„ç†çŠ¶æ€
        self.all_mentions = []
        self.all_events = []
        self.all_event_relations = []
        self.all_entity_relations = []
        
        # æ„å»ºç°æœ‰å®ä½“çš„æ˜ å°„
        self._build_entity_name_map()

    def process_documents(self, documents: list[Document]) -> dict:
        """
        å¤„ç†æ‰€æœ‰æ–‡æ¡£ï¼Œè¿”å›æœ€ç»ˆçš„å›¾ç»“æ„
        
        Args:
            documents: å·²ç»é€šè¿‡HyperRAGGraphExtractorå¤„ç†çš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            dict: åŒ…å«æœ€ç»ˆentities, relations, events, event_relationsçš„å­—å…¸
        """
        # æ”¶é›†æ‰€æœ‰mentionså’Œevents
        self._collect_mentions_and_events(documents)
        
        # å¯¹mentionsè¿›è¡Œå»é‡ï¼Œæ„å»ºentities
        entities = self._deduplicate_mentions_to_entities()
        
        # æ„å»ºåŸºäºäº‹ä»¶çš„å®ä½“å…³ç³»
        event_based_entity_relations = self._build_entity_relations(entities)
        
        # æ•´åˆç›´æ¥æå–çš„å®ä½“å…³ç³»
        direct_entity_relations = self._consolidate_entity_relations()
        
        # åˆå¹¶æ‰€æœ‰å®ä½“å…³ç³»
        all_entity_relations = event_based_entity_relations + direct_entity_relations
        
        # æ•´åˆäº‹ä»¶å…³ç³»
        event_relations = self._consolidate_event_relations()
        
        return {
            "entities": entities,
            "entity_relations": all_entity_relations,
            "events": self.all_events,
            "event_relations": event_relations,
            "mentions": self.all_mentions  # ä¿ç•™åŸå§‹mentionsç”¨äºè°ƒè¯•
        }

    @classmethod
    def create_with_cache(cls, cache_dir: str = "./graph_cache", initial_entities: list = None, initial_relations: list = None):
        """
        ä½¿ç”¨ç¼“å­˜åˆ›å»ºå¢é‡å¤„ç†å™¨å®ä¾‹ï¼ˆæ¨èæ–¹å¼ï¼‰
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•
            initial_entities: åˆå§‹å®ä½“æ•°æ®ï¼ˆå¯é€‰ï¼‰
            initial_relations: åˆå§‹å…³ç³»æ•°æ®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            IncrementalGraphProcessor: å¸¦ç¼“å­˜çš„å¤„ç†å™¨å®ä¾‹
        """
        # åˆ›å»ºç¼“å­˜å®ä¾‹
        cache = GraphDataCache(cache_dir)
        
        # å¦‚æœæä¾›äº†åˆå§‹æ•°æ®ä¸”ç¼“å­˜ä¸ºç©ºï¼Œåˆ™åŠ è½½åˆå§‹æ•°æ®
        if (initial_entities or initial_relations) and cache.get_cache_stats()['entity_count'] == 0:
            print("ğŸ”„ åŠ è½½åˆå§‹æ•°æ®åˆ°ç¼“å­˜...")
            cache.load_data(initial_entities, initial_relations)
        
        # åˆ›å»ºå¤„ç†å™¨å®ä¾‹
        processor = cls(cache=cache)
        
        return processor
    
    def update_cache_with_results(self, incremental_result: dict):
        """
        å°†å¢é‡å¤„ç†ç»“æœæ›´æ–°åˆ°ç¼“å­˜
        
        Args:
            incremental_result: å¢é‡å¤„ç†ç»“æœ
        """
        if not self.cache:
            return
        
        print(f"ğŸ’¾ æ­£åœ¨æ›´æ–°ç¼“å­˜...")
        entity_count = 0
        relation_count = 0
        
        # æ›´æ–°æ–°å®ä½“åˆ°ç¼“å­˜ï¼ˆæ‰¹é‡ï¼Œä¸ç«‹å³ä¿å­˜ï¼‰
        for entity in incremental_result.get('new_entities', []):
            if hasattr(entity, 'name'):
                entity_dict = {
                    'name': entity.name,
                    'type': getattr(entity, 'label', 'Unknown'),
                    'summary': getattr(entity, 'summary', ''),
                    'aliases': getattr(entity, 'aliases', []),
                }
            else:
                entity_dict = entity
            
            self.cache.add_entity(entity_dict, save_immediately=False)
            entity_count += 1
        
        # æ›´æ–°ä¿®æ”¹çš„å®ä½“åˆ°ç¼“å­˜ï¼ˆæ‰¹é‡ï¼Œä¸ç«‹å³ä¿å­˜ï¼‰
        for entity in incremental_result.get('updated_entities', []):
            if hasattr(entity, 'name'):
                entity_dict = {
                    'name': entity.name,
                    'type': getattr(entity, 'label', 'Unknown'),
                    'summary': getattr(entity, 'summary', ''),
                    'aliases': getattr(entity, 'aliases', []),
                }
            else:
                entity_dict = entity
            
            self.cache.add_entity(entity_dict, save_immediately=False)
            entity_count += 1
        
        # æ›´æ–°æ–°å…³ç³»åˆ°ç¼“å­˜ï¼ˆæ‰¹é‡ï¼Œä¸ç«‹å³ä¿å­˜ï¼‰
        for relation in incremental_result.get('new_relations', []):
            if hasattr(relation, 'head_id'):
                relation_dict = {
                    'head_id': relation.head_id,
                    'tail_id': relation.tail_id,
                    'label': relation.label,
                    'description': getattr(relation, 'metadatas', {}).get('description', '') if hasattr(relation, 'metadatas') else '',
                }
            else:
                relation_dict = relation
            
            self.cache.add_relation(relation_dict, save_immediately=False)
            relation_count += 1
        
        # æœ€åç»Ÿä¸€ä¿å­˜åˆ°ç£ç›˜
        if entity_count > 0 or relation_count > 0:
            self.cache._save_cache()
            print(f"âœ… ç¼“å­˜æ›´æ–°å®Œæˆ: {entity_count} ä¸ªå®ä½“, {relation_count} ä¸ªå…³ç³»")
    
    def _build_entity_name_map(self):
        """æ„å»ºå®ä½“åç§°åˆ°å®ä½“å¯¹è±¡çš„æ˜ å°„"""
        for entity in self.existing_entities:
            if hasattr(entity, 'name'):
                self.entity_name_map[entity.name] = entity
            elif isinstance(entity, dict):
                name = entity.get('name')
                if name:
                    self.entity_name_map[name] = entity
    
    def process_documents_incremental(self, documents: list[Document]) -> dict:
        """
        å¢é‡å¤„ç†æ–‡æ¡£ï¼Œè¿”å›æ–°å¢å’Œæ›´æ–°çš„å›¾ç»“æ„
        
        Args:
            documents: å·²ç»é€šè¿‡HyperRAGGraphExtractorå¤„ç†çš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            dict: åŒ…å«æ–°å¢å’Œæ›´æ–°çš„entities, relations, eventsçš„å­—å…¸
        """
        # å¦‚æœä½¿ç”¨ç¼“å­˜ï¼Œå…ˆè¿‡æ»¤å‡ºæœªå¤„ç†çš„æ–‡æ¡£
        if self.cache:
            unprocessed_documents = self.cache.filter_unprocessed_chunks(documents)
            if len(unprocessed_documents) < len(documents):
                print(f"ğŸ“Š ç¼“å­˜è¿‡æ»¤ï¼šä» {len(documents)} ä¸ªæ–‡æ¡£ä¸­è·³è¿‡äº† {len(documents) - len(unprocessed_documents)} ä¸ªå·²å¤„ç†çš„chunk")
        else:
            unprocessed_documents = documents
        
        # å¦‚æœæ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡æ¡£ï¼Œè¿”å›ç©ºç»“æœ
        if not unprocessed_documents:
            print("âœ… æ‰€æœ‰æ–‡æ¡£éƒ½å·²å¤„ç†è¿‡ï¼Œè·³è¿‡å¤„ç†")
            return {
                "new_entities": [],
                "updated_entities": [],
                "new_relations": [],
                "all_entities": list(self.cache.get_all_entities().values()) if self.cache else [],
                "all_relations": self.cache.get_all_relations() if self.cache else [],
                "events": [],
                "event_relations": [],
                "mentions": []
            }
        
        # å…ˆæ‰§è¡ŒåŸºç¡€å¤„ç†ï¼ˆä»…å¤„ç†æœªå¤„ç†çš„æ–‡æ¡£ï¼‰
        result = self.process_documents(unprocessed_documents)
        
        # è¿›è¡Œå¢é‡åˆ†æ
        self._analyze_incremental_changes(result)
        
        # æ„å»ºè¿”å›ç»“æœ
        incremental_result = {
            "new_entities": self.new_entities,
            "updated_entities": self.updated_entities,
            "new_relations": self.new_relations,
            "all_entities": result["entities"],
            "all_relations": result["entity_relations"],
            "events": result["events"],
            "event_relations": result["event_relations"],
            "mentions": result["mentions"]
        }
        
        # å¦‚æœä½¿ç”¨ç¼“å­˜ï¼Œæ›´æ–°ç¼“å­˜å¹¶æ ‡è®°chunkä¸ºå·²å¤„ç†
        if self.cache:
            self.update_cache_with_results(incremental_result)
            
            # æ ‡è®°æ‰€æœ‰å¤„ç†è¿‡çš„chunk
            processed_chunk_ids = []
            for doc in unprocessed_documents:
                chunk_id = doc.metadata.get('chunk_id') if hasattr(doc, 'metadata') else None
                if chunk_id:
                    processed_chunk_ids.append(chunk_id)
            
            if processed_chunk_ids:
                self.cache.mark_chunks_processed(processed_chunk_ids, save_immediately=True)
        
        return incremental_result
    
    def _analyze_incremental_changes(self, processed_result: dict):
        """
        åˆ†æå¢é‡å˜åŒ–
        
        Args:
            processed_result: å¤„ç†åçš„å›¾ç»“æ„ç»“æœ
        """
        current_entities = processed_result["entities"]
        current_relations = processed_result["entity_relations"]
        
        # åˆ†æå®ä½“å˜åŒ–
        self._analyze_entity_changes(current_entities)
        
        # åˆ†æå…³ç³»å˜åŒ–
        self._analyze_relation_changes(current_relations)
    
    def _analyze_entity_changes(self, current_entities: list):
        """åˆ†æå®ä½“å˜åŒ–"""
        current_time = datetime.now()
        
        for entity in current_entities:
            entity_name = entity.name if hasattr(entity, 'name') else entity.get('name')
            
            if entity_name in self.entity_name_map:
                # å®ä½“å·²å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                existing_entity = self.entity_name_map[entity_name]
                if self._entity_needs_update(existing_entity, entity):
                    # åˆå¹¶å®ä½“ä¿¡æ¯
                    updated_entity = self._merge_entity_data(existing_entity, entity)
                    updated_entity.update_time = current_time
                    self.updated_entities.append(updated_entity)
            else:
                # æ–°å®ä½“
                if hasattr(entity, 'update_time'):
                    entity.update_time = current_time
                elif isinstance(entity, dict):
                    entity['update_time'] = current_time
                self.new_entities.append(entity)
                self.entity_name_map[entity_name] = entity
    
    def _analyze_relation_changes(self, current_relations: list):
        """åˆ†æå…³ç³»å˜åŒ–"""
        # æ„å»ºç°æœ‰å…³ç³»çš„æ ‡è¯†é›†åˆ
        existing_rel_keys = set()
        for rel in self.existing_relations:
            if hasattr(rel, 'head_id') and hasattr(rel, 'tail_id') and hasattr(rel, 'label'):
                key = (rel.head_id, rel.tail_id, rel.label)
            elif isinstance(rel, dict):
                key = (rel.get('head_id'), rel.get('tail_id'), rel.get('label'))
            else:
                continue
            existing_rel_keys.add(key)
        
        # æ£€æŸ¥æ–°å…³ç³»
        for rel in current_relations:
            if hasattr(rel, 'head_id') and hasattr(rel, 'tail_id') and hasattr(rel, 'label'):
                key = (rel.head_id, rel.tail_id, rel.label)
            elif isinstance(rel, dict):
                key = (rel.get('head_id'), rel.get('tail_id'), rel.get('label'))
            else:
                continue
                
            if key not in existing_rel_keys:
                self.new_relations.append(rel)
                existing_rel_keys.add(key)
    
    def _entity_needs_update(self, existing_entity, new_entity) -> bool:
        """
        æ£€æŸ¥å®ä½“æ˜¯å¦éœ€è¦æ›´æ–°
        
        Args:
            existing_entity: ç°æœ‰å®ä½“
            new_entity: æ–°æå–çš„å®ä½“
            
        Returns:
            bool: æ˜¯å¦éœ€è¦æ›´æ–°
        """
        # æ¯”è¾ƒæè¿°ä¿¡æ¯
        existing_desc = self._get_entity_description(existing_entity)
        new_desc = self._get_entity_description(new_entity)
        
        if new_desc and new_desc != existing_desc:
            return True
        
        # æ¯”è¾ƒåˆ«å
        existing_aliases = self._get_entity_aliases(existing_entity)
        new_aliases = self._get_entity_aliases(new_entity)
        
        if new_aliases and set(new_aliases) != set(existing_aliases):
            return True
        
        return False
    
    def _get_entity_description(self, entity) -> str:
        """è·å–å®ä½“æè¿°"""
        if hasattr(entity, 'summary'):
            return entity.summary or ""
        elif hasattr(entity, 'description'):
            return entity.description or ""
        elif isinstance(entity, dict):
            return entity.get('summary', entity.get('description', ""))
        return ""
    
    def _get_entity_aliases(self, entity) -> list:
        """è·å–å®ä½“åˆ«å"""
        if hasattr(entity, 'aliases'):
            return entity.aliases or []
        elif isinstance(entity, dict):
            return entity.get('aliases', [])
        return []
    
    def _merge_entity_data(self, existing_entity, new_entity):
        """
        åˆå¹¶å®ä½“æ•°æ®
        
        Args:
            existing_entity: ç°æœ‰å®ä½“
            new_entity: æ–°å®ä½“æ•°æ®
            
        Returns:
            åˆå¹¶åçš„å®ä½“
        """
        # åˆå¹¶æè¿°
        existing_desc = self._get_entity_description(existing_entity)
        new_desc = self._get_entity_description(new_entity)
        
        merged_desc = existing_desc
        if new_desc and new_desc not in existing_desc:
            merged_desc = f"{existing_desc}; {new_desc}" if existing_desc else new_desc
        
        # åˆå¹¶åˆ«å
        existing_aliases = self._get_entity_aliases(existing_entity)
        new_aliases = self._get_entity_aliases(new_entity)
        merged_aliases = list(set(existing_aliases + new_aliases))
        
        # åˆ›å»ºåˆå¹¶åçš„å®ä½“
        if hasattr(existing_entity, 'name'):
            # EntityNode å¯¹è±¡
            existing_entity.summary = merged_desc
            existing_entity.aliases = merged_aliases
            return existing_entity
        else:
            # å­—å…¸æ ¼å¼
            merged_entity = existing_entity.copy()
            merged_entity['summary'] = merged_desc
            merged_entity['aliases'] = merged_aliases
            return merged_entity

    def _collect_mentions_and_events(self, documents: list[Document]):
        """æ”¶é›†æ‰€æœ‰æ–‡æ¡£çš„mentionså’Œevents"""
        self.all_mentions = []
        self.all_events = []
        self.all_event_relations = []
        self.all_entity_relations = []
        
        for doc in documents:
            # æ”¶é›†mentions
            mentions = doc.metadata.get(KG_MENTIONS_KEY, [])
            for mention in mentions:
                mention['source_doc'] = doc.metadata.get('chunk_id', 'unknown')
                self.all_mentions.append(mention)
            
            # æ”¶é›†events
            events = doc.metadata.get(KG_EVENTS_KEY, [])
            for event in events:
                event['source_doc'] = doc.metadata.get('chunk_id', 'unknown')
                self.all_events.append(event)
            
            # æ”¶é›†event_relations
            event_relations = doc.metadata.get('event_relations', [])
            for rel in event_relations:
                rel['source_doc'] = doc.metadata.get('chunk_id', 'unknown')
                self.all_event_relations.append(rel)
            
            # æ”¶é›†entity_relations
            entity_relations = doc.metadata.get('entity_relations', [])
            for rel in entity_relations:
                rel['source_doc'] = doc.metadata.get('chunk_id', 'unknown')
                self.all_entity_relations.append(rel)
    
    def _deduplicate_mentions_to_entities(self) -> list:
        """å¯¹mentionsè¿›è¡Œå»é‡ï¼Œæ„å»ºæœ€ç»ˆçš„entities"""
        entity_groups = {}
        
        # æŒ‰entity_nameåˆ†ç»„mentions
        for mention in self.all_mentions:
            entity_name = mention.get('entity_name', '')
            if entity_name not in entity_groups:
                entity_groups[entity_name] = []
            entity_groups[entity_name].append(mention)
        
        entities = []
        for entity_name, mentions in entity_groups.items():
            # åˆå¹¶åŒä¸€å®ä½“çš„æ‰€æœ‰ä¿¡æ¯
            entity = self._merge_mentions_to_entity(entity_name, mentions)
            entities.append(entity)
        
        return entities
    
    def _merge_mentions_to_entity(self, entity_name: str, mentions: list) -> dict:
        """å°†åŒä¸€å®ä½“çš„æ‰€æœ‰mentionsåˆå¹¶ä¸ºä¸€ä¸ªentity"""
        # è·å–æœ€å¸¸è§çš„entity_type
        types = [m.get('entity_type', '') for m in mentions if m.get('entity_type')]
        entity_type = max(set(types), key=types.count) if types else "Unknown"
        
        # åˆå¹¶æè¿°
        descriptions = [m.get('entity_description', '') for m in mentions if m.get('entity_description')]
        entity_description = '; '.join(set(descriptions)) if descriptions else ""
        
        # æ”¶é›†æ‰€æœ‰æåŠæ–‡æœ¬ä½œä¸ºaliases
        aliases = list(set([m.get('text', '') for m in mentions if m.get('text')]))
        
        # æ”¶é›†æ‰€æœ‰ç›¸å…³äº‹ä»¶
        related_events = []
        for m in mentions:
            event_indices = m.get('event_indices', [])
            for event_idx in event_indices:
                # æ‰¾åˆ°å¯¹åº”çš„äº‹ä»¶
                for event in self.all_events:
                    if event.get('id', '').endswith(f'_{event_idx}'):
                        related_events.append(event.get('id', ''))
                        break
        
        # ç”Ÿæˆå®ä½“ID
        entity_id = f"Entity_{hash(entity_name) % 1000000:06d}"
        
        # åˆ›å»ºæè¿°æ˜ å°„ - ä½¿ç”¨chunk_id:descriptionæ ¼å¼
        description_mapping = {}
        for mention in mentions:
            chunk_id = mention.get('source_doc', 'unknown_chunk')
            desc = mention.get('entity_description', '')
            if desc:
                if chunk_id in description_mapping:
                    # å¦‚æœåŒä¸€ä¸ªchunkæœ‰å¤šä¸ªæè¿°ï¼Œç”¨åˆ†å·è¿æ¥
                    description_mapping[chunk_id] += f"; {desc}"
                else:
                    description_mapping[chunk_id] = desc
        
        return EntityNode(
            id_=entity_id,
            name=entity_name,
            label=entity_type,
            aliases=aliases,
            description=description_mapping,
            summary=entity_description
        )
    
    def _build_entity_relations(self, entities: list) -> list:
        """æ„å»ºå®ä½“ä¹‹é—´çš„å…³ç³»ï¼šåŸºäºäº‹ä»¶å‚ä¸å…³ç³» + åŸºäºchunkå…±ç°å…³ç³»"""
        relations = []
        
        # 1. åŸºäºå…±åŒå‚ä¸çš„äº‹ä»¶å»ºç«‹å®ä½“å…³ç³»
        for event in self.all_events:
            participants = event.get('participants', [])
            if len(participants) >= 2:
                # ä¸ºå‚ä¸åŒä¸€äº‹ä»¶çš„å®ä½“å»ºç«‹å…³ç³»
                for i in range(len(participants)):
                    for j in range(i + 1, len(participants)):
                        # æ‰¾åˆ°å¯¹åº”çš„å®ä½“
                        head_entity = next((e for e in entities if e.name == participants[i]), None)
                        tail_entity = next((e for e in entities if e.name == participants[j]), None)
                        
                        if head_entity and tail_entity:
                            relation = Relation(
                                label="å…±åŒå‚ä¸äº‹ä»¶",
                                head_id=head_entity.name,
                                tail_id=tail_entity.name,
                                metadatas={
                                    "description": f"åœ¨äº‹ä»¶'{event.get('content', '')}'ä¸­å…±åŒå‚ä¸",
                                    "event_id": event.get('id', ''),
                                    "event_type": event.get('type', ''),
                                    "source_doc": event.get('source_doc', ''),
                                    "relation_source": "event_based"
                                }
                            )
                            relations.append(relation)
        
        # 2. åŸºäºchunkå…±ç°å…³ç³»å»ºç«‹å®ä½“è¿æ¥ï¼ˆé€‚ç”¨äºæ‰€æœ‰chunkï¼ŒåŒ…æ‹¬æ²¡æœ‰eventçš„chunkï¼‰
        chunk_entities = {}  # chunk_id -> [entities]
        
        # æŒ‰chunkåˆ†ç»„å®ä½“
        for entity in entities:
            # ä»å®ä½“çš„descriptionä¸­è·å–æ‰€æœ‰ç›¸å…³çš„chunk_id
            if hasattr(entity, 'description') and isinstance(entity.description, dict):
                for chunk_id in entity.description.keys():
                    if chunk_id not in chunk_entities:
                        chunk_entities[chunk_id] = []
                    chunk_entities[chunk_id].append(entity)
        
        # ä¸ºæ¯ä¸ªchunkä¸­å…±ç°çš„å®ä½“å»ºç«‹å…³ç³»
        for chunk_id, chunk_entity_list in chunk_entities.items():
            if len(chunk_entity_list) >= 2:
                for i in range(len(chunk_entity_list)):
                    for j in range(i + 1, len(chunk_entity_list)):
                        head_entity = chunk_entity_list[i]
                        tail_entity = chunk_entity_list[j]
                        
                        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰åŸºäºäº‹ä»¶çš„å…³ç³»ï¼Œé¿å…é‡å¤
                        existing_relation = any(
                            rel.head_id == head_entity.name and rel.tail_id == tail_entity.name
                            and rel.metadatas.get("relation_source") == "event_based"
                            for rel in relations
                        )
                        
                        if not existing_relation:
                            relation = Relation(
                                label="æ–‡æ¡£å…±ç°",
                                head_id=head_entity.name,
                                tail_id=tail_entity.name,
                                metadatas={
                                    "description": f"åœ¨æ–‡æ¡£chunk '{chunk_id}' ä¸­å…±åŒå‡ºç°",
                                    "source_doc": chunk_id,
                                    "relation_source": "cooccurrence_based"
                                }
                            )
                            relations.append(relation)
        
        return relations
    
    def _consolidate_entity_relations(self) -> list:
        """æ•´åˆç›´æ¥æå–çš„å®ä½“å…³ç³»"""
        relations = []
        
        # å»é‡å®ä½“å…³ç³»
        seen = set()
        for rel in self.all_entity_relations:
            # å°è¯•ä¸åŒçš„é”®åæ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
            head = rel.get('head', rel.get('head_entity', ''))
            tail = rel.get('tail', rel.get('tail_entity', ''))
            rel_type = rel.get('type', rel.get('relation_type', ''))
            
            key = (head, tail, rel_type)
            if key not in seen and head and tail and rel_type:
                seen.add(key)
                relation = Relation(
                    label=rel_type,
                    head_id=head,
                    tail_id=tail,
                    metadatas={
                        "description": rel.get('description', ''),
                        "source_doc": rel.get('source_doc', ''),
                        "relation_source": "direct_extraction"
                    }
                )
                relations.append(relation)
        
        return relations
    
    def _consolidate_event_relations(self) -> list:
        """æ•´åˆäº‹ä»¶å…³ç³»"""
        # å»é‡äº‹ä»¶å…³ç³»
        unique_relations = []
        seen = set()
        
        for rel in self.all_event_relations:
            key = (rel.get('head_event', ''), rel.get('tail_event', ''), rel.get('relation_type', ''))
            if key not in seen:
                seen.add(key)
                unique_relations.append(rel)
        
        return unique_relations