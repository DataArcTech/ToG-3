import asyncio
import numpy as np
from typing import List, Dict, Any, Tuple, Optional, Set, Union

from collections import defaultdict
import logging

from rag_factory.store.graph.event_graphrag import HyperRAGNeo4jStore
from rag_factory.embeddings.base import Embeddings
from rag_factory.llm.base import LLMBase
from rag_factory.data_model.graph import GenerationResult, RetrievalContext, PPRResult, RetrievalItem, SeedNode

from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)



class QueryPreference(BaseModel):
    """æŸ¥è¯¢åå¥½"""
    chunk_weight: float = Field(default=0.5, description="chunkæƒé‡", ge=0, le=1)
    event_weight: float = Field(default=0.5, description="eventæƒé‡", ge=0, le=1)
    
    @classmethod
    def model_validate(cls, v):
        """ç¡®ä¿chunk_weight + event_weight = 1"""
        if isinstance(v, dict) and abs(v.get('chunk_weight', 0.5) + v.get('event_weight', 0.5) - 1.0) > 0.01:
            raise ValueError("chunk_weightå’Œevent_weightä¹‹å’Œå¿…é¡»ä¸º1")
        return super().model_validate(v)


class QueryAnalysisResult(BaseModel):
    """æŸ¥è¯¢åˆ†æç»“æœ"""
    extracted_entities: List[str] = Field(default_factory=list, description="æå–çš„å®ä½“")
    extracted_events: List[str] = Field(default_factory=list, description="æå–çš„äº‹ä»¶")
    query_preference: QueryPreference = Field(default_factory=QueryPreference, description="æŸ¥è¯¢åå¥½")


# ===== ä¸»è¦ç±»å®šä¹‰ =====

class EventGraphRetriever:
    """åŸºäºäº‹ä»¶å›¾è°±çš„æ£€ç´¢å™¨"""
    
    # å¸¸é‡å®šä¹‰
    DEFAULT_SIMILARITY_THRESHOLD = 0.8
    DEFAULT_PPR_DAMPING = 0.85
    DEFAULT_CONVERGENCE_TOLERANCE = 1e-6
    
    # æƒé‡é…ç½®
    SOURCE_WEIGHTS = {
        "exact_match": 1.0,
        "vector_match": 0.8,
        "query_vector_match": 0.6,
        "extracted": 0.9
    }
    
    TYPE_WEIGHTS = {
        "entity": 1.0,
        "event": 1.1  # äº‹ä»¶é€šå¸¸æ›´é‡è¦
    }
    
    def __init__(self,
                 graph_store: HyperRAGNeo4jStore,
                 embedding_model: Embeddings,
                 llm_model: LLMBase,
                 max_seed_nodes: int = 10,
                 ppr_iterations: int = 50,
                 ppr_damping: float = DEFAULT_PPR_DAMPING,
                 top_k_items: int = 10,
                 similarity_threshold: float = DEFAULT_SIMILARITY_THRESHOLD,
                 enable_fallback: bool = True,
                 chunk_event_balance: float = 0.5,
                 convergence_tolerance: float = DEFAULT_CONVERGENCE_TOLERANCE):
        """
        åˆå§‹åŒ–æ£€ç´¢ç³»ç»Ÿ
        
        Args:
            chunk_event_balance: 0.0åå‘chunkï¼Œ1.0åå‘eventï¼Œ0.5å¹³è¡¡
            convergence_tolerance: PageRankæ”¶æ•›å®¹å¿åº¦
        """
        # æ ¸å¿ƒç»„ä»¶
        self.graph_store = graph_store
        self.embedding_model = embedding_model
        self.llm_model = llm_model
        
        # é…ç½®å‚æ•°
        self.max_seed_nodes = max_seed_nodes
        self.ppr_iterations = ppr_iterations
        self.ppr_damping = ppr_damping
        self.top_k_items = top_k_items
        self.similarity_threshold = similarity_threshold
        self.enable_fallback = enable_fallback
        self.chunk_event_balance = chunk_event_balance
        self.convergence_tolerance = convergence_tolerance
        
        # æ•°æ®ç¼“å­˜
        self._reset_cache()
        
        # åˆå§‹åŒ–æ ‡å¿—
        self.is_initialized = False
    
    def _reset_cache(self):
        """é‡ç½®ç¼“å­˜æ•°æ®"""
        self.entity_data = []
        self.entity_embeddings = np.array([])
        self.event_data = []
        self.event_embeddings = np.array([])
        self.chunk_data = []
        self.chunk_embeddings = np.array([])
        self.graph_adjacency = {}
        self.node_types = {}
    
    # ===== åˆå§‹åŒ–æ–¹æ³• =====
    
    async def initialize(self):
        """åˆå§‹åŒ–ç³»ç»Ÿï¼ŒåŠ è½½å›¾æ•°æ®"""
        if self.is_initialized:
            logger.info("ç³»ç»Ÿå·²åˆå§‹åŒ–ï¼Œè·³è¿‡")
            return
            
        logger.info("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–æ£€ç´¢ç³»ç»Ÿ...")
        
        try:
            await self._load_entity_data()
            await self._load_event_data() 
            await self._load_chunk_data()
            await self._build_directed_graph()
            
            self.is_initialized = True
            logger.info("âœ… æ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            self._reset_cache()
            raise
    
    async def _load_entity_data(self):
        """åŠ è½½å®ä½“æ•°æ®"""
        query = """
        MATCH (e:Entity) 
        WHERE e.embedding IS NOT NULL
        RETURN e.id_ as id_, e.entity_name as name, e.entity_type as type, 
               e.entity_descriptions as descriptions, e.embedding as embedding
        """
        
        try:
            async with self.graph_store._driver.session(database=self.graph_store.database) as session:
                result = await session.run(query)
                records = await result.data()
                
                entities = []
                embeddings = []
                
                for record in records:
                    entity_data = {
                        "id_": record["id_"],
                        "name": record["name"] or "",
                        "type": record["type"] or "unknown",
                        "descriptions": record["descriptions"] or []
                    }
                    entities.append(entity_data)
                    embeddings.append(record["embedding"])
                
                self.entity_data = entities
                self.entity_embeddings = np.array(embeddings) if embeddings else np.array([])
                logger.info(f"  ğŸ“Š åŠ è½½äº† {len(entities)} ä¸ªå®ä½“")
                
        except Exception as e:
            logger.error(f"åŠ è½½å®ä½“æ•°æ®å¤±è´¥: {e}")
            raise
    
    async def _load_event_data(self):
        """åŠ è½½äº‹ä»¶æ•°æ®"""
        query = """
        MATCH (e:Event) 
        WHERE e.embedding IS NOT NULL
        RETURN e.id_ as id_, e.content as content, e.type as type,
               e.participants as participants, e.embedding as embedding
        """
        
        try:
            async with self.graph_store._driver.session(database=self.graph_store.database) as session:
                result = await session.run(query)
                records = await result.data()
                
                events = []
                embeddings = []
                
                for record in records:
                    event_data = {
                        "id_": record["id_"],
                        "content": record["content"] or "",
                        "type": record["type"] or "unknown",
                        "participants": record["participants"] or []
                    }
                    events.append(event_data)
                    embeddings.append(record["embedding"])
                
                self.event_data = events
                self.event_embeddings = np.array(embeddings) if embeddings else np.array([])
                logger.info(f"  ğŸ“Š åŠ è½½äº† {len(events)} ä¸ªäº‹ä»¶")
                
        except Exception as e:
            logger.error(f"åŠ è½½äº‹ä»¶æ•°æ®å¤±è´¥: {e}")
            raise
    
    async def _load_chunk_data(self):
        """åŠ è½½chunkæ•°æ®"""
        query = """
        MATCH (c:Chunk) 
        WHERE c.embedding IS NOT NULL
        RETURN c.id_ as id_, c.content as content, c.source as source, c.embedding as embedding
        """
        
        try:
            async with self.graph_store._driver.session(database=self.graph_store.database) as session:
                result = await session.run(query)
                records = await result.data()
                
                chunks = []
                embeddings = []
                
                for record in records:
                    chunk_data = {
                        "id_": record["id_"],
                        "content": record["content"] or "",
                        "source": record["source"] or "unknown"
                    }
                    chunks.append(chunk_data)
                    embeddings.append(record["embedding"])
                
                self.chunk_data = chunks
                self.chunk_embeddings = np.array(embeddings) if embeddings else np.array([])
                logger.info(f"  ğŸ“Š åŠ è½½äº† {len(chunks)} ä¸ªchunk")
                
        except Exception as e:
            logger.error(f"åŠ è½½chunkæ•°æ®å¤±è´¥: {e}")
            raise
    
    async def _build_directed_graph(self):
        """æ„å»ºæœ‰å‘å›¾é‚»æ¥è¡¨"""
        logger.info("  ğŸ”— æ„å»ºæœ‰å‘å›¾é‚»æ¥è¡¨...")
        
        # å®šä¹‰æœ‰å‘å…³ç³»
        # directed_relations = [
        #     # å®ä½“å‚ä¸äº‹ä»¶ï¼ˆå•å‘ï¼šå®ä½“ -> äº‹ä»¶ï¼‰
        #     ("MATCH (e:Entity)-[:PARTICIPATES_IN]->(v:Event) RETURN e.id_ as src, v.id_ as dst", True),
        #     # chunkåŒ…å«äº‹ä»¶ï¼ˆå•å‘ï¼šchunk -> äº‹ä»¶ï¼‰  
        #     ("MATCH (c:Chunk)-[:CONTAINS]->(e:Event) RETURN c.id_ as src, e.id_ as dst", True),
        #     # chunkæåŠå®ä½“ï¼ˆå•å‘ï¼šchunk -> å®ä½“ï¼‰
        #     ("MATCH (c:Chunk)-[:MENTIONS]->(e:Entity) RETURN c.id_ as src, e.id_ as dst", True),
        #     # å®ä½“é—´å…³ç³»ï¼ˆåŒå‘ï¼‰
        #     ("MATCH (a:Entity)-[:ENTITY_RELATION]->(b:Entity) RETURN a.id_ as src, b.id_ as dst", False),
        #     # äº‹ä»¶é—´å…³ç³»ï¼ˆåŒå‘ï¼‰
        #     ("MATCH (a:Event)-[:EVENT_RELATION]->(b:Event) RETURN a.id_ as src, b.id_ as dst", False),
        # ]

        directed_relations = [
            # å®ä½“å‚ä¸äº‹ä»¶ï¼ˆå•å‘ï¼šå®ä½“ -> äº‹ä»¶ï¼‰
            ("MATCH (e:Entity)-[:PARTICIPATES_IN]->(v:Event) RETURN e.id_ as src, v.id_ as dst", False),
            # chunkåŒ…å«äº‹ä»¶ï¼ˆå•å‘ï¼šchunk -> äº‹ä»¶ï¼‰  
            ("MATCH (c:Chunk)-[:CONTAINS]->(e:Event) RETURN c.id_ as src, e.id_ as dst", False),
            # chunkæåŠå®ä½“ï¼ˆå•å‘ï¼šchunk -> å®ä½“ï¼‰
            ("MATCH (c:Chunk)-[:MENTIONS]->(e:Entity) RETURN c.id_ as src, e.id_ as dst", False),
            # å®ä½“é—´å…³ç³»ï¼ˆåŒå‘ï¼‰
            ("MATCH (a:Entity)-[:ENTITY_RELATION]->(b:Entity) RETURN a.id_ as src, b.id_ as dst", True),
            # äº‹ä»¶é—´å…³ç³»ï¼ˆåŒå‘ï¼‰
            ("MATCH (a:Event)-[:EVENT_RELATION]->(b:Event) RETURN a.id_ as src, b.id_ as dst", True),
        ]
        
        try:
            adjacency = defaultdict(set)
            
            for query, is_directed in directed_relations:
                async with self.graph_store._driver.session(database=self.graph_store.database) as session:
                    result = await session.run(query)
                    records = await result.data()
                    
                    for record in records:
                        src = record["src"]
                        dst = record["dst"]
                        if src and dst:  # ç¡®ä¿IDæœ‰æ•ˆ
                            adjacency[src].add(dst)
                            
                            # åŒå‘å…³ç³»
                            if not is_directed:
                                adjacency[dst].add(src)
            
            # æ„å»ºèŠ‚ç‚¹ç±»å‹æ˜ å°„
            await self._build_node_type_mapping()
            
            # ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½åœ¨é‚»æ¥è¡¨ä¸­
            all_nodes = set(self.node_types.keys())
            for node_id in all_nodes:
                if node_id not in adjacency:
                    adjacency[node_id] = set()
            
            # è½¬æ¢ä¸ºæ™®é€šdict
            self.graph_adjacency = {k: list(v) for k, v in adjacency.items()}
            
            total_edges = sum(len(neighbors) for neighbors in self.graph_adjacency.values())
            logger.info(f"    æ„å»ºå®Œæˆ: {len(self.graph_adjacency)} ä¸ªèŠ‚ç‚¹, {total_edges} æ¡æœ‰å‘è¾¹")
            
        except Exception as e:
            logger.error(f"æ„å»ºå›¾ç»“æ„å¤±è´¥: {e}")
            raise
    
    async def _build_node_type_mapping(self):
        """æ„å»ºèŠ‚ç‚¹ç±»å‹æ˜ å°„"""
        queries = [
            ("MATCH (e:Entity) RETURN e.id_ as id_, 'entity' as type", "entity"),
            ("MATCH (e:Event) RETURN e.id_ as id_, 'event' as type", "event"), 
            ("MATCH (c:Chunk) RETURN c.id_ as id_, 'chunk' as type", "chunk")
        ]
        
        for query, node_type in queries:
            async with self.graph_store._driver.session(database=self.graph_store.database) as session:
                result = await session.run(query)
                records = await result.data()
                
                for record in records:
                    node_id = record["id_"]
                    if node_id:  # ç¡®ä¿IDæœ‰æ•ˆ
                        self.node_types[node_id] = node_type
    

    # ===== ä¸»è¦æ£€ç´¢æµç¨‹ =====
    
    async def retrieve_and_generate(self, query: str) -> GenerationResult:
        """æ‰§è¡Œå®Œæ•´çš„æ£€ç´¢ä¸ç”Ÿæˆæµç¨‹"""
        if not self.is_initialized:
            await self.initialize()
            
        logger.info(f"ğŸ” å¼€å§‹å¤„ç†æŸ¥è¯¢: {query}")
        
        try:
            # Step 1: æŸ¥è¯¢åˆ†æä¸æ„å›¾ç†è§£
            entities, events, preference = await self._step1_query_analysis(query)
            
            # Step 2: ç§å­èŠ‚ç‚¹è¯†åˆ«
            seed_nodes = await self._step2_seed_identification(entities, events, query)
            
            if not seed_nodes and self.enable_fallback:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ç§å­èŠ‚ç‚¹ï¼Œå¯ç”¨ç¨ å¯†å¬å›å…œåº•æœºåˆ¶")
                return await self._fallback_dense_retrieval(query)
            
            # Step 3: ä¸ªæ€§åŒ–PageRank
            ppr_result = await self._step3_personalized_pagerank(seed_nodes)
            
            # Step 4: é€‰æ‹©chunkæˆ–event
            ranked_items = await self._step4_item_selection(
                ppr_result, query, preference
            )
            
            # Step 5: ä¸Šä¸‹æ–‡æ„å»ºä¸ç­”æ¡ˆç”Ÿæˆ
            generation_result = await self._step5_answer_generation(
                query, ranked_items, seed_nodes, ppr_result
            )
            
            return generation_result
            
        except Exception as e:
            logger.error(f"æ£€ç´¢ç”Ÿæˆè¿‡ç¨‹å¤±è´¥: {e}")
            # è¿”å›é”™è¯¯ç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return GenerationResult(
                answer=f"å¤„ç†æŸ¥è¯¢æ—¶é‡åˆ°é”™è¯¯: {str(e)}",
                evidence_items=[],
                citations=[],
                confidence=0.0,
                retrieval_context=RetrievalContext([], [], PPRResult({}, {}, [], {}))
            )
    
    async def _step1_query_analysis(self, query: str) -> Tuple[List[str], List[str], QueryPreference]:
        """Step 1: æŸ¥è¯¢åˆ†æä¸æ„å›¾ç†è§£"""
        logger.info("ğŸ“‹ Step 1: æŸ¥è¯¢åˆ†æä¸æ„å›¾ç†è§£")
        
        prompt = self._build_query_analysis_prompt(query)
        messages = [{"role": "user", "content": prompt}]
        
        try:
            response = await self.llm_model.aparse_chat(messages, QueryAnalysisResult)
            
            entities = response.extracted_entities
            events = response.extracted_events
            preference = response.query_preference
            
            logger.info(f"  æå–åˆ°å®ä½“: {entities}")
            logger.info(f"  æå–åˆ°äº‹ä»¶: {events}")
            logger.info(f"  æŸ¥è¯¢åå¥½: chunk={preference.chunk_weight:.2f}, event={preference.event_weight:.2f}")
            
            return entities, events, preference
            
        except Exception as e:
            logger.warning(f"âš ï¸ æŸ¥è¯¢åˆ†æå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return [], [], QueryPreference()
    
    def _build_query_analysis_prompt(self, query: str) -> str:
        """æ„å»ºæŸ¥è¯¢åˆ†æçš„prompt"""
        return f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡Œæµ‹èµ„æ–™åˆ†æä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·æŸ¥è¯¢å¹¶æå–å…³é”®ä¿¡æ¯ï¼š

1. **æå–å®ä½“ (extracted_entities)**ï¼š
   - è¯†åˆ«è¡Œæµ‹èµ„æ–™åˆ†æä¸­çš„ä¸“ä¸šæœ¯è¯­å’Œæ¦‚å¿µ
   - ä¾‹å¦‚ï¼š'åŒæ¯”å¢é•¿ç‡', 'ç¯æ¯”å¢é•¿ç‡', 'æ¯”é‡', 'å¹³å‡æ•°', 'åŸºæœŸ', 'ç°æœŸ'ç­‰
   - å¿½ç•¥å…·ä½“çš„å…¬å¸åã€äº§å“åç­‰èƒŒæ™¯ä¿¡æ¯

2. **æå–äº‹ä»¶ (extracted_events)**ï¼š
   - è¯†åˆ«æŸ¥è¯¢å¯¹åº”çš„è¡Œæµ‹åˆ†æé¢˜å‹æˆ–è§£é¢˜è¡Œä¸º
   - ä¾‹å¦‚ï¼š'è®¡ç®—å¢é•¿é‡', 'è®¡ç®—å¢é•¿ç‡', 'æ¯”è¾ƒå¢é•¿å¿«æ…¢', 'è®¡ç®—æ¯”é‡'ç­‰

3. **è®¾å®šæŸ¥è¯¢åå¥½ (query_preference)**ï¼š
   - å¦‚æœæŸ¥è¯¢éœ€è¦å…·ä½“æ•°æ®è®¡ç®—ï¼Œè®¾ç½®æ›´é«˜çš„chunk_weightï¼ˆ0.7-0.8ï¼‰
   - å¦‚æœæŸ¥è¯¢è¯¢é—®æ¦‚å¿µæˆ–æ–¹æ³•ï¼Œè®¾ç½®æ›´é«˜çš„event_weightï¼ˆ0.7-0.8ï¼‰
   - æ ‡å‡†è®¡ç®—é¢˜å¯è®¾ç½®å‡ç­‰æƒé‡ï¼ˆå„0.5ï¼‰

æŸ¥è¯¢: {query}

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœã€‚
"""
    
    async def _step2_seed_identification(self, 
                                       entities: List[str], 
                                       events: List[str], 
                                       query: str) -> List[SeedNode]:
        """Step 2: ç§å­èŠ‚ç‚¹è¯†åˆ«"""
        logger.info("ğŸ¯ Step 2: ç§å­èŠ‚ç‚¹è¯†åˆ«")
        
        seed_nodes = []
        
        try:
            # å®ä½“é“¾æ¥
            for entity in entities:
                linked_nodes = await self._link_entity(entity)
                seed_nodes.extend(linked_nodes)
            
            # äº‹ä»¶é“¾æ¥
            for event in events:
                linked_nodes = await self._link_event(event)
                seed_nodes.extend(linked_nodes)
            
            # å‘é‡æ£€ç´¢å…œåº•
            if not seed_nodes:
                logger.info("  âš ï¸ ä½¿ç”¨å‘é‡æ£€ç´¢å…œåº•")
                return []
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            unique_seeds = self._deduplicate_seed_nodes(seed_nodes)
            result_seeds = unique_seeds[:self.max_seed_nodes]
            
            logger.info(f"  è¯†åˆ«åˆ° {len(result_seeds)} ä¸ªç§å­èŠ‚ç‚¹")
            for node in result_seeds:
                logger.info(f"    - {node.name} ({node.type}, åˆ†æ•°: {node.score:.3f})")
            
            return result_seeds
            
        except Exception as e:
            logger.error(f"ç§å­èŠ‚ç‚¹è¯†åˆ«å¤±è´¥: {e}")
            return []
    
    def _deduplicate_seed_nodes(self, seed_nodes: List[SeedNode]) -> List[SeedNode]:
        """å»é‡ç§å­èŠ‚ç‚¹ï¼Œä¿ç•™æœ€é«˜åˆ†çš„"""
        seen_ids = {}
        for node in seed_nodes:
            if node.id_ not in seen_ids or node.score > seen_ids[node.id_].score:
                seen_ids[node.id_] = node
        return list(seen_ids.values())
    
    
    async def _step3_personalized_pagerank(self, seed_nodes: List[SeedNode]) -> PPRResult:
        """Step 3: ä¸ªæ€§åŒ–PageRank"""
        logger.info("ğŸŒ Step 3: ä¸ªæ€§åŒ–PageRank")
        
        if not seed_nodes:
            return PPRResult({}, {}, [], {"converged": False, "total_nodes": 0})
        
        try:
            # è®¡ç®—ç§å­èŠ‚ç‚¹æƒé‡
            seed_weights = self._compute_seed_weights(seed_nodes)
            
            # åˆå§‹åŒ–æ‰€æœ‰èŠ‚ç‚¹åˆ†æ•°
            all_nodes = set(self.graph_adjacency.keys())
            for neighbors in self.graph_adjacency.values():
                all_nodes.update(neighbors)
            
            # è¿è¡ŒPageRank
            node_scores, convergence_info = self._run_pagerank(all_nodes, seed_weights)
            
            # è®¡ç®—itemå¾—åˆ†
            item_scores = await self._compute_item_scores_from_nodes(node_scores)
            
            # æ„å»ºéå†è·¯å¾„ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            traversal_path = self._build_traversal_path(node_scores, convergence_info)
            
            logger.info(f"  PageRankå®Œæˆï¼Œè®¡ç®—å‡º {len(item_scores)} ä¸ªitemåˆ†æ•°")
            
            return PPRResult(node_scores, item_scores, traversal_path, convergence_info)
            
        except Exception as e:
            logger.error(f"PageRankè®¡ç®—å¤±è´¥: {e}")
            return PPRResult({}, {}, [], {"converged": False, "error": str(e)})
    
    def _compute_seed_weights(self, seed_nodes: List[SeedNode]) -> Dict[str, float]:
        """è®¡ç®—ç§å­èŠ‚ç‚¹æƒé‡"""
        weights = {}
        
        # è®¡ç®—åŠ æƒåˆ†æ•°
        for node in seed_nodes:
            base_weight = node.score
            source_mult = self.SOURCE_WEIGHTS.get(node.source, 0.5)
            type_mult = self.TYPE_WEIGHTS.get(node.type, 0.5)
            
            final_weight = base_weight * source_mult * type_mult
            weights[node.id_] = final_weight
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(weights.values())
        if total_weight > 0:
            weights = {k: v / total_weight for k, v in weights.items()}
        else:
            # å‡åŒ€åˆ†å¸ƒå…œåº•
            uniform_weight = 1.0 / len(seed_nodes)
            weights = {node.id_: uniform_weight for node in seed_nodes}
        
        return weights
    
    def _run_pagerank(self, 
                     all_nodes: Set[str], 
                     seed_weights: Dict[str, float]) -> Tuple[Dict[str, float], Dict[str, Any]]:
        """è¿è¡ŒPageRankç®—æ³•"""
        # åˆå§‹åˆ†æ•°ï¼šåªæœ‰ç§å­èŠ‚ç‚¹æœ‰åˆ†æ•°
        node_scores = {node_id: 0.0 for node_id in all_nodes}
        for seed_id, weight in seed_weights.items():
            if seed_id in node_scores:
                node_scores[seed_id] = weight
        
        prev_scores = None
        converged_at = -1
        
        for iteration in range(self.ppr_iterations):
            new_scores = self._pagerank_iteration(node_scores, seed_weights)
            
            # æ”¶æ•›æ£€æµ‹
            if prev_scores and self._has_converged(new_scores, prev_scores):
                converged_at = iteration
                logger.info(f"  PageRankåœ¨ç¬¬{iteration}è½®æ”¶æ•›")
                break
            
            prev_scores = node_scores.copy()
            node_scores = new_scores
        
        if converged_at == -1:
            logger.info(f"  PageRankæœªæ”¶æ•›ï¼Œå®Œæˆ{self.ppr_iterations}è½®è¿­ä»£")
        
        convergence_info = {
            "converged": converged_at != -1,
            "converged_at": converged_at,
            "final_iteration": self.ppr_iterations if converged_at == -1 else converged_at,
            "total_nodes": len(all_nodes),
            "total_score": sum(node_scores.values())
        }
        
        return node_scores, convergence_info
    
    def _pagerank_iteration(self, 
                          node_scores: Dict[str, float], 
                          seed_weights: Dict[str, float]) -> Dict[str, float]:
        """PageRankå•æ¬¡è¿­ä»£"""
        new_scores = defaultdict(float)
        
        # 1. è®¡ç®—teleportåˆ†æ•°ï¼ˆé‡æ–°åˆ†é…åˆ°ç§å­èŠ‚ç‚¹ï¼‰
        total_score = sum(node_scores.values())
        teleport_mass = total_score * (1 - self.ppr_damping)
        
        for seed_id, seed_weight in seed_weights.items():
            new_scores[seed_id] += teleport_mass * seed_weight
        
        # 2. è®¡ç®—ä¼ æ’­åˆ†æ•°ï¼ˆä»æ¯ä¸ªèŠ‚ç‚¹ä¼ æ’­åˆ°å…¶é‚»å±…ï¼‰
        for node_id, score in node_scores.items():
            if node_id in self.graph_adjacency and score > 0:
                out_neighbors = self.graph_adjacency[node_id]
                if out_neighbors:
                    propagate_score = score * self.ppr_damping / len(out_neighbors)
                    for neighbor_id in out_neighbors:
                        new_scores[neighbor_id] += propagate_score
        
        # 3. ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½å­˜åœ¨äºç»“æœä¸­
        for node_id in node_scores.keys():
            if node_id not in new_scores:
                new_scores[node_id] = 0.0
        
        return dict(new_scores)
    
    def _has_converged(self, 
                      current_scores: Dict[str, float], 
                      prev_scores: Dict[str, float]) -> bool:
        """æ£€æµ‹PageRankæ˜¯å¦æ”¶æ•›"""
        total_diff = sum(
            abs(current_scores[node_id] - prev_scores.get(node_id, 0.0))
            for node_id in current_scores
        )
        return total_diff < self.convergence_tolerance
    
    def _build_traversal_path(self, 
                            node_scores: Dict[str, float], 
                            convergence_info: Dict[str, Any]) -> List[str]:
        """æ„å»ºéå†è·¯å¾„ç”¨äºè°ƒè¯•"""
        top_nodes = sorted(node_scores.items(), key=lambda x: x[1], reverse=True)[:5]
        
        path_info = f"Top nodes: {[(nid[:8], f'{score:.4f}') for nid, score in top_nodes]}"
        convergence_status = "æ”¶æ•›" if convergence_info.get("converged", False) else "æœªæ”¶æ•›"
        
        return [f"{convergence_status} - {path_info}"]
    
    async def _compute_item_scores_from_nodes(self, node_scores: Dict[str, float]) -> Dict[str, float]:
        """ä»èŠ‚ç‚¹åˆ†æ•°è®¡ç®—itemï¼ˆchunkå’Œeventï¼‰åˆ†æ•°"""
        item_scores = defaultdict(float)
        
        # ç›´æ¥åˆ†æ•°ï¼ševentèŠ‚ç‚¹å’ŒchunkèŠ‚ç‚¹
        for node_id, score in node_scores.items():
            node_type = self.node_types.get(node_id)
            if node_type in ["event", "chunk"]:
                item_scores[node_id] = score
        
        # é—´æ¥åˆ†æ•°ï¼šé€šè¿‡å…³ç³»ä¼ æ’­
        await self._propagate_scores_to_items(node_scores, item_scores)
        
        return dict(item_scores)
    
    async def _propagate_scores_to_items(self, 
                                       node_scores: Dict[str, float], 
                                       item_scores: Dict[str, float]):
        """å°†å®ä½“åˆ†æ•°ä¼ æ’­åˆ°ç›¸å…³çš„chunkå’Œevent"""
        
        # è·å–æœ‰åˆ†æ•°çš„å®ä½“ID
        entity_ids = [
            node_id for node_id, score in node_scores.items() 
            if self.node_types.get(node_id) == "entity" and score > 0
        ]
        
        if not entity_ids:
            return
        
        try:
            # å®ä½“åˆ†æ•°ä¼ æ’­åˆ°ç›¸å…³äº‹ä»¶
            await self._propagate_entity_to_events(entity_ids, node_scores, item_scores)
            
            # å®ä½“åˆ†æ•°ä¼ æ’­åˆ°ç›¸å…³chunk
            await self._propagate_entity_to_chunks(entity_ids, node_scores, item_scores)
            
        except Exception as e:
            logger.error(f"åˆ†æ•°ä¼ æ’­å¤±è´¥: {e}")
    
    async def _propagate_entity_to_events(self, 
                                        entity_ids: List[str], 
                                        node_scores: Dict[str, float], 
                                        item_scores: Dict[str, float]):
        """å®ä½“åˆ†æ•°ä¼ æ’­åˆ°äº‹ä»¶"""
        query = """
        MATCH (e:Entity)-[:PARTICIPATES_IN]->(v:Event)
        WHERE e.id_ IN $entity_ids
        RETURN e.id_ as entity_id, v.id_ as event_id
        """
        
        async with self.graph_store._driver.session(database=self.graph_store.database) as session:
            result = await session.run(query, entity_ids=entity_ids)
            records = await result.data()
            
            for record in records:
                entity_id = record["entity_id"]
                event_id = record["event_id"]
                if entity_id in node_scores:
                    # å®ä½“åˆ†æ•°çš„ä¸€éƒ¨åˆ†ä¼ æ’­ç»™äº‹ä»¶
                    item_scores[event_id] += node_scores[entity_id] * 0.5
    
    async def _propagate_entity_to_chunks(self, 
                                        entity_ids: List[str], 
                                        node_scores: Dict[str, float], 
                                        item_scores: Dict[str, float]):
        """å®ä½“åˆ†æ•°ä¼ æ’­åˆ°chunk"""
        query = """
        MATCH (e:Entity)<-[:MENTIONS]-(c:Chunk)
        WHERE e.id_ IN $entity_ids  
        RETURN e.id_ as entity_id, c.id_ as chunk_id
        """
        
        async with self.graph_store._driver.session(database=self.graph_store.database) as session:
            result = await session.run(query, entity_ids=entity_ids)
            records = await result.data()
            
            for record in records:
                entity_id = record["entity_id"]
                chunk_id = record["chunk_id"]
                if entity_id in node_scores:
                    # å®ä½“åˆ†æ•°çš„ä¸€éƒ¨åˆ†ä¼ æ’­ç»™chunk
                    item_scores[chunk_id] += node_scores[entity_id] * 0.3
    
    async def _step4_item_selection(self, 
                                              ppr_result: PPRResult, 
                                              query: str,
                                              preference: QueryPreference) -> List[RetrievalItem]:
        """Step 4: é€‰æ‹©chunkæˆ–event"""
        logger.info("ğŸ¯ Step 4: é€‰æ‹©æ£€ç´¢é¡¹")
        
        item_scores = ppr_result.item_scores
        if not item_scores:
            return []
        
        try:
            # è·å–æ‰€æœ‰å€™é€‰é¡¹
            candidate_items = await self._get_candidate_items(item_scores, preference)
            
            # é€‰æ‹©top-kï¼Œä¿æŒchunk/eventå¹³è¡¡
            selected_items = self._balance_chunk_event_selection(candidate_items)
            
            logger.info(f"  é€‰æ‹©äº† {len(selected_items)} ä¸ªæ£€ç´¢é¡¹:")
            for i, item in enumerate(selected_items):
                logger.info(f"    {i+1}. [{item.type}] åˆ†æ•°: {item.score:.4f} - {item.content[:80]}...")
            
            return selected_items
            
        except Exception as e:
            logger.error(f"é€‰æ‹©æ£€ç´¢é¡¹å¤±è´¥: {e}")
            return []
    
    async def _get_candidate_items(self, 
                                 item_scores: Dict[str, float], 
                                 preference: QueryPreference) -> List[RetrievalItem]:
        """è·å–å€™é€‰æ£€ç´¢é¡¹"""
        candidate_items = []
        
        for item_id, score in item_scores.items():
            item_details = await self._get_item_details(item_id)
            if item_details:
                # æ ¹æ®æŸ¥è¯¢åå¥½è°ƒæ•´åˆ†æ•°
                adjusted_score = self._adjust_score_by_preference(
                    score, item_details["type"], preference
                )
                
                candidate_items.append(RetrievalItem(
                    id_=item_id,
                    content=item_details["content"],
                    type=item_details["type"],
                    score=adjusted_score,
                    source="pagerank",
                    metadata=item_details.get("metadata", {})
                ))
        
        # æŒ‰è°ƒæ•´ååˆ†æ•°æ’åº
        candidate_items.sort(key=lambda x: x.score, reverse=True)
        return candidate_items
    
    def _adjust_score_by_preference(self, 
                                  base_score: float, 
                                  item_type: str, 
                                  preference: QueryPreference) -> float:
        """æ ¹æ®æŸ¥è¯¢åå¥½è°ƒæ•´å¾—åˆ†"""
        if item_type == "chunk":
            return base_score * preference.chunk_weight
        elif item_type == "event":
            return base_score * preference.event_weight
        else:
            return base_score
    
    def _balance_chunk_event_selection(self, candidate_items: List[RetrievalItem]) -> List[RetrievalItem]:
        """å¹³è¡¡é€‰æ‹©chunkå’Œeventï¼Œç¡®ä¿å¤šæ ·æ€§"""
        if not candidate_items:
            return []
        
        # æŒ‰ç±»å‹åˆ†ç»„
        chunks = [item for item in candidate_items if item.type == "chunk"]
        events = [item for item in candidate_items if item.type == "event"]
        
        # è®¡ç®—æ¯ç§ç±»å‹çš„ç›®æ ‡æ•°é‡
        total_chunks = len(chunks)
        total_events = len(events)
        
        if total_chunks == 0:
            return events[:self.top_k_items]
        elif total_events == 0:
            return chunks[:self.top_k_items]
        
        # åŸºäºå¹³è¡¡å› å­åˆ†é…
        target_chunks = max(1, int(self.top_k_items * (1 - self.chunk_event_balance)))
        target_events = self.top_k_items - target_chunks
        
        # ç¡®ä¿ä¸è¶…è¿‡å¯ç”¨æ•°é‡
        actual_chunks = min(target_chunks, total_chunks)
        actual_events = min(target_events, total_events)
        
        # å¦‚æœä¸€ç§ç±»å‹ä¸å¤Ÿï¼Œä»å¦ä¸€ç§ç±»å‹è¡¥å……
        if actual_chunks < target_chunks:
            actual_events = min(self.top_k_items - actual_chunks, total_events)
        if actual_events < target_events:
            actual_chunks = min(self.top_k_items - actual_events, total_chunks)
        
        # é€‰æ‹©æœ€é«˜åˆ†çš„é¡¹
        selected_chunks = chunks[:actual_chunks]
        selected_events = events[:actual_events]
        
        # åˆå¹¶å¹¶æŒ‰åˆ†æ•°é‡æ–°æ’åº
        selected_items = selected_chunks + selected_events
        selected_items.sort(key=lambda x: x.score, reverse=True)
        
        return selected_items
    
    async def _get_item_details(self, item_id: str) -> Optional[Dict[str, Any]]:
        """è·å–itemï¼ˆchunkæˆ–eventï¼‰çš„è¯¦ç»†ä¿¡æ¯"""
        node_type = self.node_types.get(item_id)
        
        if node_type == "chunk":
            for chunk in self.chunk_data:
                if chunk["id_"] == item_id:
                    return {
                        "content": chunk["content"],
                        "type": "chunk",
                        "metadata": {"source": chunk.get("source", "")}
                    }
        elif node_type == "event":
            for event in self.event_data:
                if event["id_"] == item_id:
                    return {
                        "content": event["content"],
                        "type": "event", 
                        "metadata": {
                            "participants": event.get("participants", []),
                            "event_type": event.get("type", "")
                        }
                    }
        
        return None
    
    async def _step5_answer_generation(self, 
                                     query: str, 
                                     ranked_items: List[RetrievalItem], 
                                     seed_nodes: List[SeedNode], 
                                     ppr_result: PPRResult) -> GenerationResult:
        """Step 5: ä¸Šä¸‹æ–‡æ„å»ºä¸ç­”æ¡ˆç”Ÿæˆ"""
        logger.info("ğŸ¤– Step 5: ä¸Šä¸‹æ–‡æ„å»ºä¸ç­”æ¡ˆç”Ÿæˆ")
        
        if not ranked_items:
            return self._create_empty_result(seed_nodes, ppr_result)
        
        try:
            # æ„å»ºä¸Šä¸‹æ–‡
            context_text, citations = self._build_generation_context(ranked_items)
            
            # æ„å»ºç”Ÿæˆprompt
            generation_prompt = self._build_generation_prompt(query, context_text)
            
            # ç”Ÿæˆç­”æ¡ˆ
            response = await self.llm_model.achat([{"role": "user", "content": generation_prompt}])
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_confidence(ranked_items, ppr_result)
            
            logger.info(f"  ç­”æ¡ˆç”Ÿæˆå®Œæˆï¼Œç½®ä¿¡åº¦: {confidence:.3f}")
            
            return GenerationResult(
                answer=response,
                evidence_items=ranked_items,
                citations=citations,
                confidence=confidence,
                retrieval_context=RetrievalContext(ranked_items, seed_nodes, ppr_result)
            )
            
        except Exception as e:
            logger.error(f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return GenerationResult(
                answer="æŠ±æ­‰ï¼Œåœ¨ç”Ÿæˆç­”æ¡ˆæ—¶é‡åˆ°äº†é—®é¢˜ã€‚",
                evidence_items=ranked_items,
                citations=[],
                confidence=0.0,
                retrieval_context=RetrievalContext(ranked_items, seed_nodes, ppr_result)
            )
    
    def _create_empty_result(self, seed_nodes: List[SeedNode], ppr_result: PPRResult) -> GenerationResult:
        """åˆ›å»ºç©ºç»“æœ"""
        return GenerationResult(
            answer="æŠ±æ­‰ï¼Œæˆ‘æ— æ³•åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
            evidence_items=[],
            citations=[],
            confidence=0.0,
            retrieval_context=RetrievalContext([], seed_nodes, ppr_result)
        )
    
    def _build_generation_context(self, ranked_items: List[RetrievalItem]) -> Tuple[str, List[str]]:
        """æ„å»ºç”Ÿæˆä¸Šä¸‹æ–‡"""
        context_text = ""
        citations = []
        
        chunk_items = [item for item in ranked_items if item.type == "chunk"]
        event_items = [item for item in ranked_items if item.type == "event"]
        
        # æ·»åŠ chunkè¯æ®
        if chunk_items:
            context_text += "\nã€æ–‡æ¡£è¯æ®ã€‘\n"
            for i, item in enumerate(chunk_items):
                context_text += f"[æ–‡æ¡£{i+1}]: {item.content}\n\n"
                citations.append(f"æ–‡æ¡£{i+1}")
        
        # æ·»åŠ eventè¯æ®
        if event_items:
            context_text += "\nã€äº‹ä»¶ä¿¡æ¯ã€‘\n"
            for i, item in enumerate(event_items):
                participants = item.metadata.get("participants", [])
                participant_str = f"(å‚ä¸è€…: {', '.join(participants)})" if participants else ""
                context_text += f"[äº‹ä»¶{i+1}]: {item.content} {participant_str}\n\n"
                citations.append(f"äº‹ä»¶{i+1}")
        
        return context_text, citations
    
    def _build_generation_prompt(self, query: str, context_text: str) -> str:
        """æ„å»ºç”Ÿæˆprompt"""
        return f"""è¯·æ ¹æ®ä»¥ä¸‹èƒŒæ™¯ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¯·ç¡®ä¿ï¼š
1. å›ç­”åŸºäºæä¾›çš„è¯æ®
2. åœ¨å›ç­”ä¸­å¼•ç”¨ç›¸å…³è¯æ®ï¼ˆå¦‚[æ–‡æ¡£1]ã€[äº‹ä»¶1]ç­‰ï¼‰
3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜
4. åŒºåˆ†æ–‡æ¡£ä¿¡æ¯å’Œäº‹ä»¶ä¿¡æ¯çš„æ¥æº

{context_text}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€ä½ çš„å›ç­”ã€‘
"""
    
    def _calculate_confidence(self, 
                            items: List[RetrievalItem], 
                            ppr_result: PPRResult) -> float:
        """è®¡ç®—ç­”æ¡ˆç½®ä¿¡åº¦"""
        if not items:
            return 0.0
        
        # åŸºäºå¤šä¸ªå› ç´ è®¡ç®—ç½®ä¿¡åº¦
        factors = {
            "avg_score": np.mean([item.score for item in items]),
            "top_score": max(item.score for item in items),
            "diversity": len(set(item.type for item in items)) / 2.0,
            "convergence": 1.0 if ppr_result.convergence_info.get("converged", False) else 0.8,
            "coverage": min(1.0, len(items) / self.top_k_items)
        }
        
        # åŠ æƒè®¡ç®—ç½®ä¿¡åº¦
        weights = {
            "avg_score": 0.3,
            "top_score": 0.25,
            "diversity": 0.15,
            "convergence": 0.15,
            "coverage": 0.15
        }
        
        confidence = sum(factors[k] * weights[k] for k in factors.keys())
        return min(1.0, max(0.0, confidence))
    
    # ===== å…œåº•æœºåˆ¶ =====
    
    async def _fallback_dense_retrieval(self, query: str) -> GenerationResult:
        """å…œåº•æœºåˆ¶ï¼šç¨ å¯†å‘é‡æ£€ç´¢"""
        logger.info("ğŸ”„ æ‰§è¡Œç¨ å¯†å‘é‡æ£€ç´¢å…œåº•æœºåˆ¶")
        
        if len(self.chunk_embeddings) == 0:
            return self._create_empty_result([], PPRResult({}, {}, [], {}))
        
        try:
            # ç¼–ç æŸ¥è¯¢
            query_embedding = self.embedding_model.embed_documents([query])[0]
            
            # è·å–top items
            top_items = await self._get_dense_retrieval_items(query_embedding)
            
            # ç”Ÿæˆç­”æ¡ˆ
            return await self._step5_answer_generation(
                query, top_items, [], 
                PPRResult({}, {}, ["Dense retrieval fallback"], {"converged": False})
            )
            
        except Exception as e:
            logger.error(f"ç¨ å¯†æ£€ç´¢å…œåº•å¤±è´¥: {e}")
            return self._create_empty_result([], PPRResult({}, {}, [], {}))
    
    async def _get_dense_retrieval_items(self, query_embedding: np.ndarray) -> List[RetrievalItem]:
        """è·å–ç¨ å¯†æ£€ç´¢çš„items"""
        top_items = []
        
        # è®¡ç®—ä¸chunkçš„ç›¸ä¼¼åº¦
        if len(self.chunk_embeddings) > 0:
            chunk_similarities = np.dot(self.chunk_embeddings, query_embedding)
            top_chunk_indices = np.argsort(chunk_similarities)[-(self.top_k_items//2):][::-1]
            
            for idx in top_chunk_indices:
                if idx < len(self.chunk_data):
                    chunk = self.chunk_data[idx]
                    top_items.append(RetrievalItem(
                        id_=chunk["id_"],
                        content=chunk["content"],
                        type="chunk",
                        score=float(chunk_similarities[idx]),
                        source="dense_retrieval",
                        metadata={"source": chunk.get("source", "")}
                    ))
        
        # è®¡ç®—ä¸eventçš„ç›¸ä¼¼åº¦
        if len(self.event_embeddings) > 0:
            event_similarities = np.dot(self.event_embeddings, query_embedding)
            top_event_indices = np.argsort(event_similarities)[-(self.top_k_items//2):][::-1]
            
            for idx in top_event_indices:
                if idx < len(self.event_data):
                    event = self.event_data[idx]
                    top_items.append(RetrievalItem(
                        id_=event["id_"],
                        content=event["content"],
                        type="event",
                        score=float(event_similarities[idx]),
                        source="dense_retrieval",
                        metadata={"participants": event.get("participants", [])}
                    ))
        
        # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
        top_items.sort(key=lambda x: x.score, reverse=True)
        return top_items[:self.top_k_items]
    
    # ===== å®ä½“å’Œäº‹ä»¶é“¾æ¥æ–¹æ³• =====
    
    async def _link_entity(self, entity_name: str) -> List[SeedNode]:
        """é“¾æ¥å®ä½“åˆ°å›¾è°±èŠ‚ç‚¹"""
        seed_nodes = []
        
        try:
            # ç²¾ç¡®åŒ¹é…
            exact_matches = self._find_exact_entity_matches(entity_name)
            seed_nodes.extend(exact_matches)
            
            # å‘é‡ç›¸ä¼¼åº¦åŒ¹é…
            if not seed_nodes and len(self.entity_embeddings) > 0:
                vector_matches = await self._find_vector_entity_matches(entity_name)
                seed_nodes.extend(vector_matches)
                
        except Exception as e:
            logger.error(f"å®ä½“é“¾æ¥å¤±è´¥ {entity_name}: {e}")
        
        return seed_nodes
    
    def _find_exact_entity_matches(self, entity_name: str) -> List[SeedNode]:
        """ç²¾ç¡®åŒ¹é…å®ä½“"""
        matches = []
        for entity in self.entity_data:
            if entity["name"].lower() == entity_name.lower():
                matches.append(SeedNode(
                    id_=entity["id_"],
                    name=entity["name"],
                    type="entity",
                    score=1.0,
                    source="exact_match"
                ))
        return matches
    
    async def _find_vector_entity_matches(self, entity_name: str) -> List[SeedNode]:
        """å‘é‡ç›¸ä¼¼åº¦åŒ¹é…å®ä½“"""
        matches = []
        
        try:
            entity_embedding = self.embedding_model.embed_documents([entity_name])[0]
            similarities = np.dot(self.entity_embeddings, entity_embedding)
            
            for i, sim in enumerate(similarities):
                if sim >= self.similarity_threshold and i < len(self.entity_data):
                    entity = self.entity_data[i]
                    matches.append(SeedNode(
                        id_=entity["id_"],
                        name=entity["name"],
                        type="entity",
                        score=float(sim),
                        source="vector_match"
                    ))
                    
        except Exception as e:
            logger.error(f"å‘é‡åŒ¹é…å®ä½“å¤±è´¥: {e}")
        
        return matches
    
    async def _link_event(self, event_text: str) -> List[SeedNode]:
        """é“¾æ¥äº‹ä»¶åˆ°å›¾è°±èŠ‚ç‚¹"""
        seed_nodes = []
        
        try:
            # å‘é‡ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆäº‹ä»¶é€šå¸¸æ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼‰
            if len(self.event_embeddings) > 0:
                vector_matches = await self._find_vector_event_matches(event_text)
                seed_nodes.extend(vector_matches)
                
        except Exception as e:
            logger.error(f"äº‹ä»¶é“¾æ¥å¤±è´¥ {event_text}: {e}")
        
        return seed_nodes
    
    async def _find_vector_event_matches(self, event_text: str) -> List[SeedNode]:
        """å‘é‡ç›¸ä¼¼åº¦åŒ¹é…äº‹ä»¶"""
        matches = []
        
        try:
            event_embedding = self.embedding_model.embed_documents([event_text])[0]
            similarities = np.dot(self.event_embeddings, event_embedding)
            
            for i, sim in enumerate(similarities):
                if sim >= self.similarity_threshold and i < len(self.event_data):
                    event = self.event_data[i]
                    matches.append(SeedNode(
                        id_=event["id_"],
                        name=event["content"],
                        type="event",
                        score=float(sim),
                        source="vector_match"
                    ))
                    
        except Exception as e:
            logger.error(f"å‘é‡åŒ¹é…äº‹ä»¶å¤±è´¥: {e}")
        
        return matches
    
    # ===== å·¥å…·æ–¹æ³• =====
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        return {
            "initialized": self.is_initialized,
            "entities": len(self.entity_data),
            "events": len(self.event_data),
            "chunks": len(self.chunk_data),
            "graph_nodes": len(self.node_types),
            "graph_edges": sum(len(neighbors) for neighbors in self.graph_adjacency.values()),
            "config": {
                "max_seed_nodes": self.max_seed_nodes,
                "top_k_items": self.top_k_items,
                "similarity_threshold": self.similarity_threshold,
                "chunk_event_balance": self.chunk_event_balance
            }
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            if not self.is_initialized:
                return {"status": "not_initialized"}
            
            # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            checks = {
                "entity_data_loaded": len(self.entity_data) > 0,
                "entity_embeddings_loaded": len(self.entity_embeddings) > 0,
                "event_data_loaded": len(self.event_data) > 0,
                "chunk_data_loaded": len(self.chunk_data) > 0,
                "graph_built": len(self.graph_adjacency) > 0,
                "node_types_mapped": len(self.node_types) > 0
            }
            
            all_passed = all(checks.values())
            
            return {
                "status": "healthy" if all_passed else "unhealthy",
                "checks": checks,
                "stats": self.get_stats()
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }